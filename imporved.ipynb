{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import miceforest as mf\n",
    "from missforest import MissForest\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import MIDASpy as md\n",
    "\n",
    "### Data Preparation Function\n",
    "\n",
    "# def prep(df: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Preprocess the DataFrame by:\n",
    "#     - Dropping rows with missing values and resetting the index.\n",
    "#     - Converting object columns to categorical via LabelEncoder.\n",
    "#     - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "#     - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "#       it is considered categorical and encoded.\n",
    "\n",
    "#     Returns:\n",
    "#         categorical_cols (list): List of columns encoded as categorical.\n",
    "#         discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "#         cont_cols (list): List of remaining continuous numeric columns.\n",
    "#         df_clean (DataFrame): The preprocessed DataFrame.\n",
    "#         encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "#     \"\"\"\n",
    "#     df_clean = df.dropna().reset_index(drop=True)\n",
    "#     categorical_cols = []\n",
    "#     discrete_cols = []\n",
    "#     encoders = {}\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if df_clean[col].dtype == 'object':\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "#         else:\n",
    "#             try:\n",
    "#                 df_clean[col] = df_clean[col].astype(float)\n",
    "#                 if (np.isclose(df_clean[col] % 1, 0).mean() > 0.5):\n",
    "#                     df_clean[col] = df_clean[col].astype(int)\n",
    "#                     discrete_cols.append(col)\n",
    "#             except (ValueError, TypeError):\n",
    "#                 categorical_cols.append(col)\n",
    "#                 le = LabelEncoder()\n",
    "#                 df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#                 encoders[col] = le\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if col not in categorical_cols and df_clean[col].nunique() == 2:\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "\n",
    "#     continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "#     return continuous_cols, discrete_cols, categorical_cols, df_clean, encoders\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "    encoders = {}\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "            le = LabelEncoder()\n",
    "            df_clean[col] = le.fit_transform(df_clean[col])\n",
    "            encoders[col] = le\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                le = LabelEncoder()\n",
    "                df_clean[col] = le.fit_transform(df_clean[col])\n",
    "                encoders[col] = le\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                \n",
    "\n",
    "    # Determine continuous columns as those not flagged as categorical or discrete.\n",
    "    continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "    return continuous_cols, discrete_cols, categorical_cols\n",
    "\n",
    "def reverse_encoding(df: pd.DataFrame, encoders: dict):\n",
    "    \"\"\"\n",
    "    Reverse the LabelEncoder transformation on categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with encoded categorical columns.\n",
    "        encoders (dict): Dictionary mapping column names to their LabelEncoder.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the categorical columns decoded to their original labels.\n",
    "    \"\"\"\n",
    "    df_decoded = df.copy()\n",
    "    for col, le in encoders.items():\n",
    "        df_decoded[col] = le.inverse_transform(df_decoded[col].astype(int))\n",
    "    return df_decoded\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame, DataFrame with missing values, and a mask DataFrame.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "def simulate_missingness(df, show_missingness=False):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=42).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         n_neighbors (int): Number of neighbors for KNN.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     # Optionally scale all numeric columns\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "#     # Apply KNN imputation to the entire dataframe\n",
    "#     imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#     df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "#     # Reverse scale if needed\n",
    "#     if scale:\n",
    "#         df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "#     # Post-process: round discrete and categorical values\n",
    "#     if discrete_cols:\n",
    "#         df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_imputed[categorical_cols] = np.round(df_imputed[categorical_cols]).astype(int)\n",
    "\n",
    "#     return df_imputed\n",
    "\n",
    "def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                # All values missing in this column\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Optionally scale numeric columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "    # Apply KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "    # Reverse scale\n",
    "    if scale:\n",
    "        df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "    # Round discrete and categorical values\n",
    "    if discrete_cols:\n",
    "        df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed[col] = np.round(df_imputed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed[col] = df_imputed[col].map(inv_map)\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_imputed = do_knn(df2, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "#             iters=10, strat='normal', scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): Input DataFrame with missing values.\n",
    "#         continuous_cols (list of str): Names of continuous numeric columns.\n",
    "#         discrete_cols (list of str): Names of discrete numeric columns.\n",
    "#         categorical_cols (list of str): Names of categorical columns.\n",
    "#         iters (int): Number of MICE iterations.\n",
    "#         strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "#     kernel = mf.ImputationKernel(\n",
    "#         df_imputed,\n",
    "#         random_state=0,\n",
    "#         mean_match_strategy=strat,\n",
    "#         variable_schema=None,  # Explicitly set variable_schema to None \n",
    "#         )\n",
    "\n",
    "#     kernel.mice(iterations=iters, verbose=False)  # Disable verbose output\n",
    "#     df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "#     if discrete_cols:\n",
    "#         df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_completed[categorical_cols] = df_completed[categorical_cols].round().astype(int)\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "#     return df_completed\n",
    "\n",
    "def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "            iters=10, strat='normal', scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with missing values.\n",
    "        continuous_cols (list of str): Names of continuous numeric columns.\n",
    "        discrete_cols (list of str): Names of discrete numeric columns.\n",
    "        categorical_cols (list of str): Names of categorical columns.\n",
    "        iters (int): Number of MICE iterations.\n",
    "        strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns if requested\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Run MICE imputation\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df_imputed,\n",
    "        random_state=0,\n",
    "        mean_match_strategy=strat,\n",
    "        variable_schema=None\n",
    "    )\n",
    "\n",
    "    kernel.mice(iterations=iters, verbose=False)\n",
    "    df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "    # Post-process discrete and categorical columns\n",
    "    if discrete_cols:\n",
    "        df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_completed[col] = np.round(df_completed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_completed[col] = df_completed[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "    return df_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mice_imputed = do_mice(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols,\n",
    "                    #    iters=10, strat='normal', scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using MissForest.\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         iters (int): Maximum number of iterations.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "    \n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "    \n",
    "#     imputer = MissForest(max_iter=iters, categorical=categorical_cols)\n",
    "#     df_imputed_result = imputer.fit_transform(df_imputed)\n",
    "    \n",
    "#     if discrete_cols:\n",
    "#         df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "    \n",
    "#     if categorical_cols:\n",
    "#         df_imputed_result[categorical_cols] = df_imputed_result[categorical_cols].round().astype(int)\n",
    "    \n",
    "#     if scale:\n",
    "#         # Reverse scaling for continuous columns\n",
    "#         df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "    \n",
    "#     return df_imputed_result\n",
    "\n",
    "# # mf_imputed = do_mf(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=5, scale=False)\n",
    "\n",
    "def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using MissForest.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        iters (int): Maximum number of iterations.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Impute with MissForest\n",
    "    imputer = MissForest(max_iter=iters)\n",
    "    df_imputed_result = pd.DataFrame(imputer.fit_transform(df_imputed), columns=df.columns)\n",
    "\n",
    "    # Post-process discrete columns\n",
    "    if discrete_cols:\n",
    "        df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "\n",
    "    # Post-process categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed_result[col] = df_imputed_result[col].round().astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed_result[col] = df_imputed_result[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "\n",
    "    return df_imputed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "              layer:list=[256,256], vae:bool=True, samples:int=10, random_seed:float=96 ):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but non-continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      \n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes.\n",
    "    \"\"\"\n",
    "    # 1. Convert categorical columns and get categorical metadata.\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "    \n",
    "    # 2. Define the numeric columns.\n",
    "    num_cols = discrete_cols + continuous_cols  # these are the numeric columns\n",
    "\n",
    "    # 3. Drop original categorical columns and combine with the converted categorical data.\n",
    "    df_copy = df.drop(columns=categorical_cols,axis=1)\n",
    "    constructor_list = [df_copy, md_cat_data]\n",
    "    data_in = pd.concat(constructor_list, axis=1)\n",
    "    \n",
    "    # 4. Scale non-categorical columns BEFORE imputation.\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "    \n",
    "    # 5. Build and train the imputer using the scaled data.\n",
    "    imputer = md.Midas(layer_structure=layer, vae_layer=vae, seed=random_seed, input_drop=0.75)\n",
    "    # Use md_cats as softmax columns for categorical outputs.\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "    \n",
    "    # 6. Generate imputations.\n",
    "    imps = imputer.generate_samples(m=samples).output_list\n",
    "    \n",
    "    # 7. Post-process each imputed DataFrame.\n",
    "    for idx, imp_df in enumerate(imps):\n",
    "        # Reverse transform the numeric columns.\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "        \n",
    "        # Process categorical columns.\n",
    "        # For each softmax group in md_cats, choose the column with the highest probability.\n",
    "        tmp_cat = []\n",
    "        for group in md_cats:\n",
    "            # idxmax returns the column name with maximum value per row for this group.\n",
    "            tmp_cat.append(imp_df[group].idxmax(axis=1))\n",
    "        # Assume the order of md_cats corresponds to categorical_cols.\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        \n",
    "        # Drop the softmax columns.\n",
    "        flat_cats = [col for group in md_cats for col in group]\n",
    "        tmp_cat = [imp_df[x].idxmax(axis=1) for x in md_cats]\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        imp_df = pd.concat([imp_df, cat_df], axis=1).drop(columns=flat_cats, axis=1)\n",
    "        \n",
    "        # Handle discrete data by rounding the values.\n",
    "        imp_df[discrete_cols] = imp_df[discrete_cols].round()\n",
    "        \n",
    "        # Replace the processed DataFrame in the list.\n",
    "        imps[idx] = imp_df\n",
    "\n",
    "        ### make method info\n",
    "        method_info = f'MIDAS, params: samples={samples} ,layer={layer}, vae={vae}'\n",
    "    return imps, method_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midas_imputed = do_midas(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Missingness Creation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Improved Evaluation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameter Optimization Function using Optuna\n",
    "# ------------------------------------------------------------------------------\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    missing_percent, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    "            params['layer'] = trial.suggest_categorical(\"layer\", [[256,256], [128,128], [512,256]])\n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, summary_table, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_midas,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mice,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_excel(r\"C:\\Users\\Matin\\Downloads\\Data for Dr.Matin.xlsx\", 's1')\n",
    "new_df.drop(['n', 'ID','Gen.code'],axis=1,inplace=True)\n",
    "new_df = new_df[:300]\n",
    "# continuous_cols, discrete_cols, categorical_cols, df2, encoders = prep(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df2,missing_percent=30,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=new_df,missing_percent=30, timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_missingness(df, show_missingness=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=random_state).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for cat in categorical_cols:\n",
    "        if cat in best_imputed_df:\n",
    "            best_imputed_df[cat] = best_imputed_df[cat].astype(object)\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = \\\n",
    "                imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    df_missing, \n",
    "                                    mask_df, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    # _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    " # Dynamically define the layer architecture\n",
    "            num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "            params[\"layer\"] = [trial.suggest_categorical(f\"layer_units_{i}\", [64, 128, 256, 512]) for i in range(num_layers)]            \n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, _, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df,\n",
    "             continuous_cols=None,\n",
    "             discrete_cols=None,\n",
    "             categorical_cols=None,\n",
    "             layer: list = [256, 256],\n",
    "             vae: bool = True,\n",
    "             samples: int = 10,\n",
    "             random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe with NaNs in both numeric & categorical.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but not continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "\n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes, with original dtypes restored.\n",
    "      method_info (str): Summary of MIDAS params used.\n",
    "    \"\"\"\n",
    "    # 1. Onehot encode the categoricals\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "\n",
    "    # 2. Build the wide DF: drop raw cats, append onehots\n",
    "    df_num = df.drop(columns=categorical_cols)\n",
    "    data_in = pd.concat([df_num, md_cat_data], axis=1)\n",
    "\n",
    "    # 3. Record & reinsert the NaN locations so MIDAS sees them as missing\n",
    "    na_mask = data_in.isnull()\n",
    "    data_in[na_mask] = np.nan\n",
    "\n",
    "    # 4. Scale only the numeric columns in place\n",
    "    num_cols = discrete_cols + continuous_cols\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "\n",
    "    # 5. Build & train the MIDAS model\n",
    "    imputer = md.Midas(\n",
    "        layer_structure=layer,\n",
    "        vae_layer=vae,\n",
    "        seed=random_seed,\n",
    "        input_drop=0.75\n",
    "    )\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "\n",
    "    # 6. Generate multiple imputations\n",
    "    raw_imps = imputer.generate_samples(m=samples).output_list\n",
    "\n",
    "    # 7. Decode each imputed DF back to original structure\n",
    "    flat_cats = [c for grp in md_cats for c in grp]\n",
    "    imps = []\n",
    "\n",
    "    for imp_df in raw_imps:\n",
    "        # 7a. inversescale numeric cols\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "\n",
    "        # 7b. decode onehots (before dropping them!)\n",
    "        decoded = {}\n",
    "        for i, grp in enumerate(md_cats):\n",
    "            # just in case, only keep those actually present\n",
    "            present = [c for c in grp if c in imp_df.columns]\n",
    "            # idxmax  gives the dummy column name with highest prob\n",
    "            decoded[categorical_cols[i]] = imp_df[present].idxmax(axis=1)\n",
    "\n",
    "        cat_df = pd.DataFrame(decoded, index=imp_df.index)\n",
    "\n",
    "        # 7c. now drop the dummy cols\n",
    "        base = imp_df.drop(columns=flat_cats, errors='ignore')\n",
    "\n",
    "        # 7d. concat in your decoded cat columns\n",
    "        merged = pd.concat([base, cat_df], axis=1)\n",
    "\n",
    "        # 7e. round discrete cols\n",
    "        merged[discrete_cols] = merged[discrete_cols].round().astype(int)\n",
    "\n",
    "        imps.append(merged)\n",
    "\n",
    "    method_info = f\"MIDAS, params: samples={samples}, layer={layer}, vae={vae}\"\n",
    "    return imps, method_info\n",
    "\n",
    "\n",
    "def run_full_pipeline(df: pd.DataFrame, \n",
    "                      simulate:bool=False,               # True for simulated missingness, False for random missingness\n",
    "                      missingness_value: float = 10.0,   # used only for random missingness (percent)\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "\n",
    "    The pipeline performs these steps:\n",
    "    \n",
    "      1. Preprocesses the DataFrame using `prep`, which cleans the data,\n",
    "         encodes categorical variables, and splits features into continuous,\n",
    "         discrete, and categorical lists.\n",
    "      2. Introduces missingness using either simulated missingness (reintroducing missingness \n",
    "         based on the original NaN proportions) or random missingness (dropping values randomly\n",
    "         given a specified missing percentage).\n",
    "      3. Runs hyperparameter optimization (via `optimize_imputation_hyperparams`) for each candidate \n",
    "         imputation method (e.g., do_knn, do_mice, do_mf, do_midas).\n",
    "         \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        missing_type (str): \"simulate\" to simulate missingness using original missing proportions,\n",
    "                            \"random\" to drop values randomly.\n",
    "        missingness_value (float): Percentage of missingness (only used if missing_type == \"random\").\n",
    "        show_missingness (bool): If True, prints missingness comparison when using simulate missingness.\n",
    "        timelimit (int): Time limit (in seconds) for each hyperparameter optimization study.\n",
    "        min_trials (int): Minimum number of trials for each study.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are method names (strings) and the values are the best \n",
    "              hyperparameter dictionaries (from the best Optuna trial) for that method.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Step 1: Preprocess Data\n",
    "    # Note: For simulation, the missing proportions are taken from the original df.\n",
    "    if simulate: \n",
    "        # simulate_missingness returns: (complete_df, df_with_missing, missing_mask)\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(df, \n",
    "                                                                      show_missingness=show_missingness,\n",
    "                                                                      random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(df, \n",
    "                                                                 missingness=missingness_value, \n",
    "                                                                 random_seed=random_seed)\n",
    "        \n",
    "    # Step 2: Preprocess Data, convert categorical cols to encoded values and find the data types.\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "    \n",
    "\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    # Optimize hyperparameters for each imputation method candidate.\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "        # Optimize hyperparameters for each imputation method candidate.\n",
    "    \n",
    "    for key, val in best_hyperparams.items():\n",
    "        if key == 'KNN':\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MICE':\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MissForest':\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], scale=val['scale']) \n",
    "\n",
    "        elif key == 'MIDAS':\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, layer=val['layer'], vae=val['vae'], samples=val['samples'])\n",
    "\n",
    "    # Create a list of imputed DataFrames.  \n",
    "    imputed_dfs = [df_knn, df_mice, df_mf, df_midas] \n",
    "    # decoded_imputed_dfs = []\n",
    "    # for i in imputed_dfs:\n",
    "    #     decoded_df = reverse_encoding(i, encoders)\n",
    "    #     decoded_imputed_dfs.append(decoded_df)\n",
    "          \n",
    "    # Create a list of method names.    \n",
    "    method_names = ['KNN', 'MICE', 'MissForest', 'MIDAS']\n",
    "    \n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        # Determine the data type label.\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        # Only evaluate columns that had artificial missing values.\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "        \n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            # Ensure the original column is numeric.\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                raise ValueError(f\"Column '{col}' is marked as numeric but contains non-numeric values.\")\n",
    "            for df_imp in imputed_dfs:\n",
    "                # Convert values to numeric.\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            \n",
    "            # Compute additional metrics for the best method.\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            \n",
    "            # Compute fraction within 10%.\n",
    "            # For nonzero original values, check error <= 0.1 * |original|.\n",
    "            # For zeros, require the imputed value to be exactly 0.\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "            \n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            # For categorical columns, compute accuracy.\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            # Extra metrics are not applicable for categoricals.\n",
    "            error_sd = np.nan\n",
    "            max_error = np.nan\n",
    "            min_error = np.nan\n",
    "            within_10pct = np.nan\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "        \n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "        \n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "    \n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    # Build best-imputed DataFrame by replacing masked entries with values from the best method.\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline2(df: pd.DataFrame, \n",
    "                      simulate: bool = False,               \n",
    "                      missingness_value: float = 10.0,   \n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "    \"\"\"\n",
    "    # Step 1: Create missingness (simulated or random)\n",
    "    if simulate: \n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed)\n",
    "\n",
    "    # Step 2: Preprocess for column types\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    # Step 3: Optimize hyperparameters per method\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_trial.params} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    # Step 4: Run best imputation for each method and collect valid results\n",
    "    if best_hyperparams.get('KNN'):\n",
    "        try:\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols,\n",
    "                            discrete_cols=discrete_cols,\n",
    "                            categorical_cols=categorical_cols,\n",
    "                            n_neighbors=best_hyperparams['KNN']['n_neighbors'],\n",
    "                            scale=best_hyperparams['KNN']['scale'])\n",
    "            imputed_dfs.append(df_knn)\n",
    "            method_names.append('KNN')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with KNN: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MICE'):\n",
    "        try:\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols,\n",
    "                              discrete_cols=discrete_cols,\n",
    "                              categorical_cols=categorical_cols,\n",
    "                              iters=best_hyperparams['MICE']['iters'],\n",
    "                              strat=best_hyperparams['MICE']['strat'],\n",
    "                              scale=best_hyperparams['MICE']['scale'])\n",
    "            imputed_dfs.append(df_mice)\n",
    "            method_names.append('MICE')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MICE: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MissForest'):\n",
    "        try:\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols,\n",
    "                          discrete_cols=discrete_cols,\n",
    "                          categorical_cols=categorical_cols,\n",
    "                          iters=best_hyperparams['MissForest']['iters'],\n",
    "                          scale=best_hyperparams['MissForest']['scale'])\n",
    "            imputed_dfs.append(df_mf)\n",
    "            method_names.append('MissForest')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MissForest: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MIDAS'):\n",
    "        try:\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                   discrete_cols=discrete_cols,\n",
    "                                   categorical_cols=categorical_cols,\n",
    "                                   layer=best_hyperparams['MIDAS']['layer'],\n",
    "                                   vae=best_hyperparams['MIDAS']['vae'],\n",
    "                                   samples=best_hyperparams['MIDAS']['samples'])\n",
    "            imputed_dfs.append(df_midas)\n",
    "            method_names.append('MIDAS')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MIDAS: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    # Step 5: Evaluate and select best method per column\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            summary_list.append({\n",
    "                'Column': col, 'Data Type': col_data_type, 'Best Method': None,\n",
    "                'Metric': np.nan, 'Error_SD': np.nan, 'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan, 'Within_10pct': np.nan\n",
    "            })\n",
    "            best_method_per_col[col] = None\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = max_error = min_error = within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    # Step 6: Final best-imputed DataFrame\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline3(df: pd.DataFrame, \n",
    "                      simulate: bool = False,\n",
    "                      missingness_value: float = 10.0,\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600,\n",
    "                      min_trials: int = 20,\n",
    "                      random_seed: int = 96):\n",
    "    if simulate:\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed\n",
    "        )\n",
    "    else:\n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "\n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    for method in ['KNN', 'MICE', 'MissForest', 'MIDAS']:\n",
    "        val = best_hyperparams.get(method)\n",
    "        if not val:\n",
    "            continue\n",
    "        try:\n",
    "            if method == 'KNN':\n",
    "                df_knn = do_knn(df_missing, continuous_cols=continuous_cols, \n",
    "                                discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_knn)\n",
    "                method_names.append('KNN')\n",
    "\n",
    "            elif method == 'MICE':\n",
    "                df_mice = do_mice(df_missing, continuous_cols=continuous_cols, \n",
    "                                  discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                  iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mice)\n",
    "                method_names.append('MICE')\n",
    "\n",
    "            elif method == 'MissForest':\n",
    "                df_mf = do_mf(df_missing, continuous_cols=continuous_cols, \n",
    "                              discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                              iters=val['iters'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mf)\n",
    "                method_names.append('MissForest')\n",
    "\n",
    "            elif method == 'MIDAS':\n",
    "                df_midas_list, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                            discrete_cols=discrete_cols,\n",
    "                                            categorical_cols=categorical_cols,\n",
    "                                            layer=val['layer'], vae=val['vae'], \n",
    "                                            samples=val['samples'])\n",
    "                imputed_dfs.extend(df_midas_list)\n",
    "                method_names.extend([f'MIDAS_{i+1}' for i in range(len(df_midas_list))])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with {method}: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = run_full_pipeline(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:36:22,159] A new study created in memory with name: no-name-ff91a4c6-940e-4944-8056-566f1e7df573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing hyperparameters for KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:36:22,387] Trial 0 finished with value: 134005.4336048941 and parameters: {'n_neighbors': 3, 'scale': True}. Best is trial 0 with value: 134005.4336048941.\n",
      "[I 2025-04-17 04:36:22,555] Trial 1 finished with value: 134005.4336048941 and parameters: {'n_neighbors': 3, 'scale': True}. Best is trial 0 with value: 134005.4336048941.\n",
      "[I 2025-04-17 04:36:22,709] Trial 2 finished with value: 155476.810668925 and parameters: {'n_neighbors': 3, 'scale': False}. Best is trial 0 with value: 134005.4336048941.\n",
      "[I 2025-04-17 04:36:22,895] Trial 3 finished with value: 134005.4336048941 and parameters: {'n_neighbors': 3, 'scale': True}. Best is trial 0 with value: 134005.4336048941.\n",
      "[I 2025-04-17 04:36:23,061] Trial 4 finished with value: 131029.31363559558 and parameters: {'n_neighbors': 9, 'scale': False}. Best is trial 4 with value: 131029.31363559558.\n",
      "[I 2025-04-17 04:36:23,234] Trial 5 finished with value: 131226.66063021446 and parameters: {'n_neighbors': 11, 'scale': False}. Best is trial 4 with value: 131029.31363559558.\n",
      "[I 2025-04-17 04:36:23,412] Trial 6 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:23,584] Trial 7 finished with value: 128910.987206718 and parameters: {'n_neighbors': 5, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:23,739] Trial 8 finished with value: 131741.00288182148 and parameters: {'n_neighbors': 5, 'scale': False}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:23,908] Trial 9 finished with value: 129171.32967365228 and parameters: {'n_neighbors': 9, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:24,092] Trial 10 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:24,267] Trial 11 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:24,444] Trial 12 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:24,616] Trial 13 finished with value: 126868.45825273624 and parameters: {'n_neighbors': 13, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:24,838] Trial 14 finished with value: 127738.59110562567 and parameters: {'n_neighbors': 12, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,030] Trial 15 finished with value: 126270.22596605406 and parameters: {'n_neighbors': 14, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,205] Trial 16 finished with value: 129037.69250588538 and parameters: {'n_neighbors': 11, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,381] Trial 17 finished with value: 126868.45825273624 and parameters: {'n_neighbors': 13, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,541] Trial 18 finished with value: 131090.14980959828 and parameters: {'n_neighbors': 7, 'scale': False}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,717] Trial 19 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 6 with value: 125837.15715446904.\n",
      "[I 2025-04-17 04:36:25,719] A new study created in memory with name: no-name-aad2454a-7d2e-4dc7-beab-ba5b00133403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  n_neighbors: 15\n",
      "  scale: True\n",
      "Best Objective Value (aggregated error): 125837.15715446904\n",
      "Best hyperparameters for KNN: {'n_neighbors': 15, 'scale': True} with best agg error of 125837.15715446904\n",
      "\n",
      "Optimizing hyperparameters for MICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:38:26,069] Trial 0 finished with value: 105014.7291872032 and parameters: {'iters': 16, 'strat': 'fast', 'scale': True}. Best is trial 0 with value: 105014.7291872032.\n",
      "[I 2025-04-17 04:38:26,072] A new study created in memory with name: no-name-9743503d-44b6-4436-a341-cad67d19ad26\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  iters: 16\n",
      "  strat: fast\n",
      "  scale: True\n",
      "Best Objective Value (aggregated error): 105014.7291872032\n",
      "Best hyperparameters for MICE: {'iters': 16, 'strat': 'fast', 'scale': True} with best agg error of 105014.7291872032\n",
      "\n",
      "Optimizing hyperparameters for MissForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      " 33%|      | 2/6 [00:07<00:15,  3.97s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 33%|      | 2/6 [00:12<00:24,  6.05s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.08it/s]\n",
      "[I 2025-04-17 04:38:39,191] Trial 0 finished with value: 133132.05827693842 and parameters: {'iters': 6, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 40%|      | 2/5 [00:07<00:11,  3.92s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 40%|      | 2/5 [00:12<00:18,  6.06s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.01it/s]\n",
      "[I 2025-04-17 04:38:52,319] Trial 1 finished with value: 133145.73115992727 and parameters: {'iters': 5, 'scale': True}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 20%|        | 2/10 [00:08<00:32,  4.06s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 20%|        | 2/10 [00:12<00:48,  6.05s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.91it/s]\n",
      "[I 2025-04-17 04:39:05,440] Trial 2 finished with value: 133132.05827693842 and parameters: {'iters': 10, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 20%|        | 2/10 [00:08<00:33,  4.24s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 20%|        | 2/10 [00:12<00:49,  6.18s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.17it/s]\n",
      "[I 2025-04-17 04:39:18,806] Trial 3 finished with value: 133145.73115992727 and parameters: {'iters': 10, 'scale': True}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 29%|       | 2/7 [00:08<00:21,  4.22s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 29%|       | 2/7 [00:12<00:30,  6.19s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.07it/s]\n",
      "[I 2025-04-17 04:39:32,183] Trial 4 finished with value: 133132.05827693842 and parameters: {'iters': 7, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "[I 2025-04-17 04:39:32,184] A new study created in memory with name: no-name-5fec2fe8-c0b3-40f9-85c8-a73f5a8813f1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  iters: 6\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 133132.05827693842\n",
      "Best hyperparameters for MissForest: {'iters': 6, 'scale': False} with best agg error of 133132.05827693842\n",
      "\n",
      "Optimizing hyperparameters for MIDAS...\n",
      "Size index: [41, 2]\n",
      "WARNING:tensorflow:From c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\MIDASpy\\midas_base.py:511: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.6290709177653\n",
      "Epoch: 1 , loss: 19.083565288119846\n",
      "Epoch: 2 , loss: 18.552426232231987\n",
      "Epoch: 3 , loss: 17.745455106099445\n",
      "Epoch: 4 , loss: 16.940537134806316\n",
      "Epoch: 5 , loss: 15.741025394863552\n",
      "Epoch: 6 , loss: 14.810981644524468\n",
      "Epoch: 7 , loss: 13.859081798129612\n",
      "Epoch: 8 , loss: 12.561987929873997\n",
      "Epoch: 9 , loss: 11.22615385055542\n",
      "Epoch: 10 , loss: 9.999006748199463\n",
      "Epoch: 11 , loss: 8.838940938313803\n",
      "Epoch: 12 , loss: 8.478200594584147\n",
      "Epoch: 13 , loss: 8.038259082370335\n",
      "Epoch: 14 , loss: 7.917967028088039\n",
      "Epoch: 15 , loss: 7.7704689502716064\n",
      "Epoch: 16 , loss: 7.736167112986247\n",
      "Epoch: 17 , loss: 7.5620419714185925\n",
      "Epoch: 18 , loss: 7.706060462527805\n",
      "Epoch: 19 , loss: 7.6246354315016\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:39:48,482] Trial 0 finished with value: 152228.43086586503 and parameters: {'num_layers': 3, 'layer_units_0': 512, 'layer_units_1': 64, 'layer_units_2': 256, 'vae': True, 'samples': 12}. Best is trial 0 with value: 152228.43086586503.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 16.595743656158447\n",
      "Epoch: 1 , loss: 16.185080263349747\n",
      "Epoch: 2 , loss: 15.86205350028144\n",
      "Epoch: 3 , loss: 15.304164409637451\n",
      "Epoch: 4 , loss: 14.613676230112711\n",
      "Epoch: 5 , loss: 13.242398526933458\n",
      "Epoch: 6 , loss: 11.778789944118923\n",
      "Epoch: 7 , loss: 10.726841396755642\n",
      "Epoch: 8 , loss: 9.964822027418348\n",
      "Epoch: 9 , loss: 9.513539685143364\n",
      "Epoch: 10 , loss: 9.271725177764893\n",
      "Epoch: 11 , loss: 9.054997815026177\n",
      "Epoch: 12 , loss: 8.802911228603786\n",
      "Epoch: 13 , loss: 8.667386399375069\n",
      "Epoch: 14 , loss: 8.612505224015978\n",
      "Epoch: 15 , loss: 8.374741262859768\n",
      "Epoch: 16 , loss: 8.32166838645935\n",
      "Epoch: 17 , loss: 8.134780221515232\n",
      "Epoch: 18 , loss: 7.954272588094075\n",
      "Epoch: 19 , loss: 8.046590169270834\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:40:03,997] Trial 1 finished with value: 145623.93825168363 and parameters: {'num_layers': 3, 'layer_units_0': 64, 'layer_units_1': 256, 'layer_units_2': 64, 'vae': False, 'samples': 18}. Best is trial 1 with value: 145623.93825168363.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.495594024658203\n",
      "Epoch: 1 , loss: 18.502465142144096\n",
      "Epoch: 2 , loss: 17.598949962192112\n",
      "Epoch: 3 , loss: 16.297986189524334\n",
      "Epoch: 4 , loss: 14.668400446573893\n",
      "Epoch: 5 , loss: 12.126959376864964\n",
      "Epoch: 6 , loss: 10.294663906097412\n",
      "Epoch: 7 , loss: 9.103092140621609\n",
      "Epoch: 8 , loss: 8.62883636686537\n",
      "Epoch: 9 , loss: 8.59159451060825\n",
      "Epoch: 10 , loss: 8.239004426532322\n",
      "Epoch: 11 , loss: 8.16954411400689\n",
      "Epoch: 12 , loss: 8.170255978902182\n",
      "Epoch: 13 , loss: 7.897458844714695\n",
      "Epoch: 14 , loss: 8.02757019466824\n",
      "Epoch: 15 , loss: 7.657258166207208\n",
      "Epoch: 16 , loss: 7.872721248202854\n",
      "Epoch: 17 , loss: 7.530998229980469\n",
      "Epoch: 18 , loss: 7.542338874604967\n",
      "Epoch: 19 , loss: 7.634667846891615\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:40:18,193] Trial 2 finished with value: 144586.52535897872 and parameters: {'num_layers': 2, 'layer_units_0': 512, 'layer_units_1': 256, 'vae': True, 'samples': 16}. Best is trial 2 with value: 144586.52535897872.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.851636356777615\n",
      "Epoch: 1 , loss: 18.99972629547119\n",
      "Epoch: 2 , loss: 18.37714778052436\n",
      "Epoch: 3 , loss: 17.671007050408257\n",
      "Epoch: 4 , loss: 17.042907343970406\n",
      "Epoch: 5 , loss: 16.06382926305135\n",
      "Epoch: 6 , loss: 15.160788165198433\n",
      "Epoch: 7 , loss: 14.502736250559488\n",
      "Epoch: 8 , loss: 13.457603030734592\n",
      "Epoch: 9 , loss: 12.378278679317898\n",
      "Epoch: 10 , loss: 10.640864743126762\n",
      "Epoch: 11 , loss: 9.207866138882107\n",
      "Epoch: 12 , loss: 8.733767774369982\n",
      "Epoch: 13 , loss: 8.492260138193766\n",
      "Epoch: 14 , loss: 8.237030506134033\n",
      "Epoch: 15 , loss: 8.211301565170288\n",
      "Epoch: 16 , loss: 7.834245522816976\n",
      "Epoch: 17 , loss: 7.900117079416911\n",
      "Epoch: 18 , loss: 7.806340959337023\n",
      "Epoch: 19 , loss: 7.8308643500010175\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:40:32,886] Trial 3 finished with value: 154193.70547185797 and parameters: {'num_layers': 3, 'layer_units_0': 256, 'layer_units_1': 256, 'layer_units_2': 128, 'vae': True, 'samples': 12}. Best is trial 2 with value: 144586.52535897872.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  num_layers: 2\n",
      "  layer_units_0: 512\n",
      "  layer_units_1: 256\n",
      "  vae: True\n",
      "  samples: 16\n",
      "Best Objective Value (aggregated error): 144586.52535897872\n",
      "Best hyperparameters for MIDAS: {'num_layers': 2, 'layer_units_0': 512, 'layer_units_1': 256, 'vae': True, 'samples': 16} with best agg error of 144586.52535897872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 33%|      | 2/6 [00:09<00:18,  4.54s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 33%|      | 2/6 [00:13<00:26,  6.61s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to impute with MIDAS: 'layer'\n"
     ]
    }
   ],
   "source": [
    "impsss, table = run_full_pipeline3(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Data Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Best Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Metric",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Error_SD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Max_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Min_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Within_10pct",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "52ac94cd-fadc-4998-a82c-4045f4e36530",
       "rows": [
        [
         "0",
         "Dm2",
         "Categorical",
         "MICE",
         "0.9629629629629629",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Dm4",
         "Discrete",
         "MICE",
         "8.029411764705882",
         "7.275885009660116",
         "32.0",
         "0.0",
         "0.47058823529411764"
        ],
        [
         "2",
         "E11",
         "Discrete",
         "MICE",
         "4.551724137931035",
         "4.664349647643064",
         "23.0",
         "0.0",
         "0.8275862068965517"
        ],
        [
         "3",
         "E12",
         "Discrete",
         "MICE",
         "3.652173913043478",
         "3.575277429186744",
         "13.0",
         "0.0",
         "0.8695652173913043"
        ],
        [
         "4",
         "E21",
         "Discrete",
         "MICE",
         "1.2857142857142858",
         "2.008385781013735",
         "8.0",
         "0.0",
         "0.7142857142857143"
        ],
        [
         "5",
         "E22",
         "Discrete",
         "MICE",
         "4.357142857142857",
         "13.928958073591613",
         "74.0",
         "0.0",
         "0.75"
        ],
        [
         "6",
         "E31",
         "Discrete",
         "MICE",
         "5.357142857142857",
         "5.800474346594651",
         "19.0",
         "0.0",
         "0.8928571428571429"
        ],
        [
         "7",
         "E32",
         "Discrete",
         "MICE",
         "3.8333333333333335",
         "2.7516228977511745",
         "10.0",
         "0.0",
         "1.0"
        ],
        [
         "8",
         "E41",
         "Discrete",
         "MICE",
         "2.6808510638297873",
         "2.9862466115781334",
         "15.0",
         "0.0",
         "0.9361702127659575"
        ],
        [
         "9",
         "E42",
         "Discrete",
         "MICE",
         "3.1538461538461537",
         "2.5407448938027244",
         "8.0",
         "0.0",
         "0.9230769230769231"
        ],
        [
         "10",
         "E23",
         "Continuous",
         "MICE",
         "5.500000000000001",
         "4.822343828471793",
         "15.200000000000003",
         "0.20000000000000284",
         "0.6206896551724138"
        ],
        [
         "11",
         "E24",
         "Discrete",
         "MICE",
         "5.586666666666667",
         "5.0834914239596465",
         "24.0",
         "0.0",
         "0.9333333333333333"
        ],
        [
         "12",
         "E25",
         "Discrete",
         "MICE",
         "5.8921052631578945",
         "5.470776494070988",
         "26.0",
         "1.0",
         "0.8421052631578947"
        ],
        [
         "13",
         "E26",
         "Discrete",
         "MICE",
         "2.9411764705882355",
         "2.607578424638789",
         "10.0",
         "0.0",
         "1.0"
        ],
        [
         "14",
         "E27",
         "Discrete",
         "MICE",
         "1.6857142857142857",
         "1.2549063375273817",
         "5.0",
         "0.0",
         "0.5428571428571428"
        ],
        [
         "15",
         "FastingBloodSugar",
         "Discrete",
         "MissForest",
         "6.36",
         "5.964338465692011",
         "23.0",
         "1.0",
         "0.76"
        ],
        [
         "16",
         "Glucose2hpp",
         "Discrete",
         "MICE",
         "22.62162162162162",
         "20.09747268709199",
         "89.0",
         "0.0",
         "0.35135135135135137"
        ],
        [
         "17",
         "Cholestrol",
         "Discrete",
         "MICE",
         "18.703703703703702",
         "18.089127237556934",
         "68.0",
         "1.0",
         "0.7037037037037037"
        ],
        [
         "18",
         "Triglycerides",
         "Discrete",
         "KNN",
         "47.44",
         "56.36789866581865",
         "286.0",
         "2.0",
         "0.2"
        ],
        [
         "19",
         "HDL",
         "Discrete",
         "KNN",
         "7.84",
         "8.658714300248816",
         "30.0",
         "0.0",
         "0.56"
        ],
        [
         "20",
         "LDL",
         "Discrete",
         "MICE",
         "12.428571428571429",
         "8.534655009691592",
         "32.0",
         "0.0",
         "0.45714285714285713"
        ],
        [
         "21",
         "Hb.A1C",
         "Continuous",
         "MICE",
         "0.4987500000000001",
         "0.43072550019907024",
         "1.46",
         "0.05999999999999961",
         "0.6666666666666666"
        ],
        [
         "22",
         "CreatininUrine",
         "Continuous",
         "MissForest",
         "79.7687721475582",
         "57.94243295504932",
         "226.3339901652958",
         "1.0592536031653594",
         "0.27586206896551724"
        ],
        [
         "23",
         "PotassiumUrineRandom",
         "Continuous",
         "MICE",
         "54.835806451612896",
         "70.65128495053214",
         "346.76",
         "0.4000000000000057",
         "0.06451612903225806"
        ],
        [
         "24",
         "SodiumUrineRandom",
         "Continuous",
         "MICE",
         "39.047999999999995",
         "32.77627597312829",
         "105.00000000000001",
         "0.4000000000000057",
         "0.44"
        ],
        [
         "25",
         "W.B.C",
         "Discrete",
         "KNN",
         "1090.0",
         "886.5796721961965",
         "3460.0",
         "0.0",
         "0.38235294117647056"
        ],
        [
         "26",
         "R.B.C",
         "Continuous",
         "MICE",
         "0.09388888888888894",
         "0.10749824657115581",
         "0.54",
         "0.0",
         "0.9722222222222222"
        ],
        [
         "27",
         "Hemoglobin",
         "Continuous",
         "MICE",
         "0.2758620689655172",
         "0.2278081467466385",
         "0.8000000000000007",
         "0.0",
         "1.0"
        ],
        [
         "28",
         "Hematocrit",
         "Continuous",
         "MICE",
         "0.9484848484848482",
         "0.7150704563717878",
         "3.1000000000000014",
         "0.0",
         "1.0"
        ],
        [
         "29",
         "MCV",
         "Continuous",
         "MICE",
         "1.6971428571428564",
         "1.2103683722894663",
         "4.799999999999997",
         "0.0",
         "1.0"
        ],
        [
         "30",
         "MCH",
         "Continuous",
         "MICE",
         "0.38749999999999973",
         "0.3579075664008191",
         "1.4000000000000021",
         "0.0",
         "1.0"
        ],
        [
         "31",
         "MCHC",
         "Continuous",
         "MICE",
         "0.6333333333333343",
         "0.6106690320187093",
         "2.5",
         "0.0",
         "1.0"
        ],
        [
         "32",
         "Neutrophils",
         "Continuous",
         "MICE",
         "1.784000000000001",
         "2.4392075215801823",
         "10.600000000000001",
         "0.0",
         "0.92"
        ],
        [
         "33",
         "Lymphocyte",
         "Continuous",
         "MICE",
         "2.217142857142858",
         "2.2224106829516197",
         "8.900000000000006",
         "0.0",
         "0.8571428571428571"
        ],
        [
         "34",
         "Mixed",
         "Continuous",
         "MICE",
         "2.51578947368421",
         "1.5833379501317728",
         "5.899999999999999",
         "0.7999999999999998",
         "0.21052631578947367"
        ],
        [
         "35",
         "Platelets",
         "Discrete",
         "MissForest",
         "45.964285714285715",
         "37.411111582540144",
         "157.0",
         "4.0",
         "0.35714285714285715"
        ],
        [
         "36",
         "DBP",
         "Discrete",
         "MICE",
         "1.25",
         "1.3228756555322954",
         "5.0",
         "0.0",
         "1.0"
        ],
        [
         "37",
         "SBP",
         "Discrete",
         "MICE",
         "2.1",
         "3.710190997964837",
         "18.5",
         "0.0",
         "0.9333333333333333"
        ],
        [
         "38",
         "gdi",
         "Discrete",
         "KNN",
         "1.103448275862069",
         "0.9763206289141645",
         "3.0",
         "0.0",
         "0.3103448275862069"
        ],
        [
         "39",
         "work_activity",
         "Discrete",
         "MICE",
         "1839473.6923076923",
         "2061028.8511300907",
         "7174132.0",
         "0.0",
         "0.34615384615384615"
        ],
        [
         "40",
         "transport",
         "Discrete",
         "MissForest",
         "1063492.9189189188",
         "1126686.2427863001",
         "2477200.0",
         "462.0",
         "0.0"
        ],
        [
         "41",
         "lesiretime",
         "Discrete",
         "MICE",
         "1147944.64",
         "1707333.6402303562",
         "4783128.0",
         "0.0",
         "0.56"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 42
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Best Method</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Error_SD</th>\n",
       "      <th>Max_Error</th>\n",
       "      <th>Min_Error</th>\n",
       "      <th>Within_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dm2</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>MICE</td>\n",
       "      <td>9.629630e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dm4</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>8.029412e+00</td>\n",
       "      <td>7.275885e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E11</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>4.551724e+00</td>\n",
       "      <td>4.664350e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E12</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>3.652174e+00</td>\n",
       "      <td>3.575277e+00</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E21</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.285714e+00</td>\n",
       "      <td>2.008386e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E22</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>4.357143e+00</td>\n",
       "      <td>1.392896e+01</td>\n",
       "      <td>7.400000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E31</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>5.357143e+00</td>\n",
       "      <td>5.800474e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E32</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>3.833333e+00</td>\n",
       "      <td>2.751623e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E41</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.680851e+00</td>\n",
       "      <td>2.986247e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E42</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>3.153846e+00</td>\n",
       "      <td>2.540745e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E23</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>5.500000e+00</td>\n",
       "      <td>4.822344e+00</td>\n",
       "      <td>1.520000e+01</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E24</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>5.586667e+00</td>\n",
       "      <td>5.083491e+00</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E25</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>5.892105e+00</td>\n",
       "      <td>5.470776e+00</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E26</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.941176e+00</td>\n",
       "      <td>2.607578e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E27</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.685714e+00</td>\n",
       "      <td>1.254906e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FastingBloodSugar</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>6.360000e+00</td>\n",
       "      <td>5.964338e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Glucose2hpp</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.262162e+01</td>\n",
       "      <td>2.009747e+01</td>\n",
       "      <td>8.900000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cholestrol</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.870370e+01</td>\n",
       "      <td>1.808913e+01</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Triglycerides</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.744000e+01</td>\n",
       "      <td>5.636790e+01</td>\n",
       "      <td>2.860000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.840000e+00</td>\n",
       "      <td>8.658714e+00</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.242857e+01</td>\n",
       "      <td>8.534655e+00</td>\n",
       "      <td>3.200000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hb.A1C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>4.987500e-01</td>\n",
       "      <td>4.307255e-01</td>\n",
       "      <td>1.460000e+00</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CreatininUrine</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>7.976877e+01</td>\n",
       "      <td>5.794243e+01</td>\n",
       "      <td>2.263340e+02</td>\n",
       "      <td>1.059254</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PotassiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>5.483581e+01</td>\n",
       "      <td>7.065128e+01</td>\n",
       "      <td>3.467600e+02</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SodiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>3.904800e+01</td>\n",
       "      <td>3.277628e+01</td>\n",
       "      <td>1.050000e+02</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>W.B.C</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.090000e+03</td>\n",
       "      <td>8.865797e+02</td>\n",
       "      <td>3.460000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>R.B.C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>9.388889e-02</td>\n",
       "      <td>1.074982e-01</td>\n",
       "      <td>5.400000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hemoglobin</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.758621e-01</td>\n",
       "      <td>2.278081e-01</td>\n",
       "      <td>8.000000e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>9.484848e-01</td>\n",
       "      <td>7.150705e-01</td>\n",
       "      <td>3.100000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MCV</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.697143e+00</td>\n",
       "      <td>1.210368e+00</td>\n",
       "      <td>4.800000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MCH</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>3.875000e-01</td>\n",
       "      <td>3.579076e-01</td>\n",
       "      <td>1.400000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MCHC</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>6.333333e-01</td>\n",
       "      <td>6.106690e-01</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Neutrophils</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.784000e+00</td>\n",
       "      <td>2.439208e+00</td>\n",
       "      <td>1.060000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lymphocyte</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.217143e+00</td>\n",
       "      <td>2.222411e+00</td>\n",
       "      <td>8.900000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mixed</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.515789e+00</td>\n",
       "      <td>1.583338e+00</td>\n",
       "      <td>5.900000e+00</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>4.596429e+01</td>\n",
       "      <td>3.741111e+01</td>\n",
       "      <td>1.570000e+02</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>DBP</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.250000e+00</td>\n",
       "      <td>1.322876e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SBP</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>2.100000e+00</td>\n",
       "      <td>3.710191e+00</td>\n",
       "      <td>1.850000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gdi</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.103448e+00</td>\n",
       "      <td>9.763206e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>work_activity</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.839474e+06</td>\n",
       "      <td>2.061029e+06</td>\n",
       "      <td>7.174132e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>transport</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.063493e+06</td>\n",
       "      <td>1.126686e+06</td>\n",
       "      <td>2.477200e+06</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lesiretime</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MICE</td>\n",
       "      <td>1.147945e+06</td>\n",
       "      <td>1.707334e+06</td>\n",
       "      <td>4.783128e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column    Data Type Best Method        Metric      Error_SD  \\\n",
       "0                    Dm2  Categorical        MICE  9.629630e-01           NaN   \n",
       "1                    Dm4     Discrete        MICE  8.029412e+00  7.275885e+00   \n",
       "2                    E11     Discrete        MICE  4.551724e+00  4.664350e+00   \n",
       "3                    E12     Discrete        MICE  3.652174e+00  3.575277e+00   \n",
       "4                    E21     Discrete        MICE  1.285714e+00  2.008386e+00   \n",
       "5                    E22     Discrete        MICE  4.357143e+00  1.392896e+01   \n",
       "6                    E31     Discrete        MICE  5.357143e+00  5.800474e+00   \n",
       "7                    E32     Discrete        MICE  3.833333e+00  2.751623e+00   \n",
       "8                    E41     Discrete        MICE  2.680851e+00  2.986247e+00   \n",
       "9                    E42     Discrete        MICE  3.153846e+00  2.540745e+00   \n",
       "10                   E23   Continuous        MICE  5.500000e+00  4.822344e+00   \n",
       "11                   E24     Discrete        MICE  5.586667e+00  5.083491e+00   \n",
       "12                   E25     Discrete        MICE  5.892105e+00  5.470776e+00   \n",
       "13                   E26     Discrete        MICE  2.941176e+00  2.607578e+00   \n",
       "14                   E27     Discrete        MICE  1.685714e+00  1.254906e+00   \n",
       "15     FastingBloodSugar     Discrete  MissForest  6.360000e+00  5.964338e+00   \n",
       "16           Glucose2hpp     Discrete        MICE  2.262162e+01  2.009747e+01   \n",
       "17            Cholestrol     Discrete        MICE  1.870370e+01  1.808913e+01   \n",
       "18         Triglycerides     Discrete         KNN  4.744000e+01  5.636790e+01   \n",
       "19                   HDL     Discrete         KNN  7.840000e+00  8.658714e+00   \n",
       "20                   LDL     Discrete        MICE  1.242857e+01  8.534655e+00   \n",
       "21                Hb.A1C   Continuous        MICE  4.987500e-01  4.307255e-01   \n",
       "22        CreatininUrine   Continuous  MissForest  7.976877e+01  5.794243e+01   \n",
       "23  PotassiumUrineRandom   Continuous        MICE  5.483581e+01  7.065128e+01   \n",
       "24     SodiumUrineRandom   Continuous        MICE  3.904800e+01  3.277628e+01   \n",
       "25                 W.B.C     Discrete         KNN  1.090000e+03  8.865797e+02   \n",
       "26                 R.B.C   Continuous        MICE  9.388889e-02  1.074982e-01   \n",
       "27            Hemoglobin   Continuous        MICE  2.758621e-01  2.278081e-01   \n",
       "28            Hematocrit   Continuous        MICE  9.484848e-01  7.150705e-01   \n",
       "29                   MCV   Continuous        MICE  1.697143e+00  1.210368e+00   \n",
       "30                   MCH   Continuous        MICE  3.875000e-01  3.579076e-01   \n",
       "31                  MCHC   Continuous        MICE  6.333333e-01  6.106690e-01   \n",
       "32           Neutrophils   Continuous        MICE  1.784000e+00  2.439208e+00   \n",
       "33            Lymphocyte   Continuous        MICE  2.217143e+00  2.222411e+00   \n",
       "34                 Mixed   Continuous        MICE  2.515789e+00  1.583338e+00   \n",
       "35             Platelets     Discrete  MissForest  4.596429e+01  3.741111e+01   \n",
       "36                   DBP     Discrete        MICE  1.250000e+00  1.322876e+00   \n",
       "37                   SBP     Discrete        MICE  2.100000e+00  3.710191e+00   \n",
       "38                   gdi     Discrete         KNN  1.103448e+00  9.763206e-01   \n",
       "39         work_activity     Discrete        MICE  1.839474e+06  2.061029e+06   \n",
       "40             transport     Discrete  MissForest  1.063493e+06  1.126686e+06   \n",
       "41            lesiretime     Discrete        MICE  1.147945e+06  1.707334e+06   \n",
       "\n",
       "       Max_Error   Min_Error  Within_10pct  \n",
       "0            NaN         NaN           NaN  \n",
       "1   3.200000e+01    0.000000      0.470588  \n",
       "2   2.300000e+01    0.000000      0.827586  \n",
       "3   1.300000e+01    0.000000      0.869565  \n",
       "4   8.000000e+00    0.000000      0.714286  \n",
       "5   7.400000e+01    0.000000      0.750000  \n",
       "6   1.900000e+01    0.000000      0.892857  \n",
       "7   1.000000e+01    0.000000      1.000000  \n",
       "8   1.500000e+01    0.000000      0.936170  \n",
       "9   8.000000e+00    0.000000      0.923077  \n",
       "10  1.520000e+01    0.200000      0.620690  \n",
       "11  2.400000e+01    0.000000      0.933333  \n",
       "12  2.600000e+01    1.000000      0.842105  \n",
       "13  1.000000e+01    0.000000      1.000000  \n",
       "14  5.000000e+00    0.000000      0.542857  \n",
       "15  2.300000e+01    1.000000      0.760000  \n",
       "16  8.900000e+01    0.000000      0.351351  \n",
       "17  6.800000e+01    1.000000      0.703704  \n",
       "18  2.860000e+02    2.000000      0.200000  \n",
       "19  3.000000e+01    0.000000      0.560000  \n",
       "20  3.200000e+01    0.000000      0.457143  \n",
       "21  1.460000e+00    0.060000      0.666667  \n",
       "22  2.263340e+02    1.059254      0.275862  \n",
       "23  3.467600e+02    0.400000      0.064516  \n",
       "24  1.050000e+02    0.400000      0.440000  \n",
       "25  3.460000e+03    0.000000      0.382353  \n",
       "26  5.400000e-01    0.000000      0.972222  \n",
       "27  8.000000e-01    0.000000      1.000000  \n",
       "28  3.100000e+00    0.000000      1.000000  \n",
       "29  4.800000e+00    0.000000      1.000000  \n",
       "30  1.400000e+00    0.000000      1.000000  \n",
       "31  2.500000e+00    0.000000      1.000000  \n",
       "32  1.060000e+01    0.000000      0.920000  \n",
       "33  8.900000e+00    0.000000      0.857143  \n",
       "34  5.900000e+00    0.800000      0.210526  \n",
       "35  1.570000e+02    4.000000      0.357143  \n",
       "36  5.000000e+00    0.000000      1.000000  \n",
       "37  1.850000e+01    0.000000      0.933333  \n",
       "38  3.000000e+00    0.000000      0.310345  \n",
       "39  7.174132e+06    0.000000      0.346154  \n",
       "40  2.477200e+06  462.000000      0.000000  \n",
       "41  4.783128e+06    0.000000      0.560000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

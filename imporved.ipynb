{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import miceforest as mf\n",
    "from missforest import MissForest\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import MIDASpy as md\n",
    "\n",
    "### Data Preparation Function\n",
    "\n",
    "# def prep(df: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Preprocess the DataFrame by:\n",
    "#     - Dropping rows with missing values and resetting the index.\n",
    "#     - Converting object columns to categorical via LabelEncoder.\n",
    "#     - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "#     - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "#       it is considered categorical and encoded.\n",
    "\n",
    "#     Returns:\n",
    "#         categorical_cols (list): List of columns encoded as categorical.\n",
    "#         discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "#         cont_cols (list): List of remaining continuous numeric columns.\n",
    "#         df_clean (DataFrame): The preprocessed DataFrame.\n",
    "#         encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "#     \"\"\"\n",
    "#     df_clean = df.dropna().reset_index(drop=True)\n",
    "#     categorical_cols = []\n",
    "#     discrete_cols = []\n",
    "#     encoders = {}\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if df_clean[col].dtype == 'object':\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "#         else:\n",
    "#             try:\n",
    "#                 df_clean[col] = df_clean[col].astype(float)\n",
    "#                 if (np.isclose(df_clean[col] % 1, 0).mean() > 0.5):\n",
    "#                     df_clean[col] = df_clean[col].astype(int)\n",
    "#                     discrete_cols.append(col)\n",
    "#             except (ValueError, TypeError):\n",
    "#                 categorical_cols.append(col)\n",
    "#                 le = LabelEncoder()\n",
    "#                 df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#                 encoders[col] = le\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if col not in categorical_cols and df_clean[col].nunique() == 2:\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "\n",
    "#     continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "#     return continuous_cols, discrete_cols, categorical_cols, df_clean, encoders\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "    encoders = {}\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "            le = LabelEncoder()\n",
    "            df_clean[col] = le.fit_transform(df_clean[col])\n",
    "            encoders[col] = le\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                le = LabelEncoder()\n",
    "                df_clean[col] = le.fit_transform(df_clean[col])\n",
    "                encoders[col] = le\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                \n",
    "\n",
    "    # Determine continuous columns as those not flagged as categorical or discrete.\n",
    "    continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "    return continuous_cols, discrete_cols, categorical_cols\n",
    "\n",
    "def reverse_encoding(df: pd.DataFrame, encoders: dict):\n",
    "    \"\"\"\n",
    "    Reverse the LabelEncoder transformation on categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with encoded categorical columns.\n",
    "        encoders (dict): Dictionary mapping column names to their LabelEncoder.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the categorical columns decoded to their original labels.\n",
    "    \"\"\"\n",
    "    df_decoded = df.copy()\n",
    "    for col, le in encoders.items():\n",
    "        df_decoded[col] = le.inverse_transform(df_decoded[col].astype(int))\n",
    "    return df_decoded\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame, DataFrame with missing values, and a mask DataFrame.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "def simulate_missingness(df, show_missingness=False):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=42).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         n_neighbors (int): Number of neighbors for KNN.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     # Optionally scale all numeric columns\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "#     # Apply KNN imputation to the entire dataframe\n",
    "#     imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#     df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "#     # Reverse scale if needed\n",
    "#     if scale:\n",
    "#         df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "#     # Post-process: round discrete and categorical values\n",
    "#     if discrete_cols:\n",
    "#         df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_imputed[categorical_cols] = np.round(df_imputed[categorical_cols]).astype(int)\n",
    "\n",
    "#     return df_imputed\n",
    "\n",
    "def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                # All values missing in this column\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Optionally scale numeric columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "    # Apply KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "    # Reverse scale\n",
    "    if scale:\n",
    "        df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "    # Round discrete and categorical values\n",
    "    if discrete_cols:\n",
    "        df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed[col] = np.round(df_imputed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed[col] = df_imputed[col].map(inv_map)\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_imputed = do_knn(df2, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "#             iters=10, strat='normal', scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): Input DataFrame with missing values.\n",
    "#         continuous_cols (list of str): Names of continuous numeric columns.\n",
    "#         discrete_cols (list of str): Names of discrete numeric columns.\n",
    "#         categorical_cols (list of str): Names of categorical columns.\n",
    "#         iters (int): Number of MICE iterations.\n",
    "#         strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "#     kernel = mf.ImputationKernel(\n",
    "#         df_imputed,\n",
    "#         random_state=0,\n",
    "#         mean_match_strategy=strat,\n",
    "#         variable_schema=None,  # Explicitly set variable_schema to None \n",
    "#         )\n",
    "\n",
    "#     kernel.mice(iterations=iters, verbose=False)  # Disable verbose output\n",
    "#     df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "#     if discrete_cols:\n",
    "#         df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_completed[categorical_cols] = df_completed[categorical_cols].round().astype(int)\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "#     return df_completed\n",
    "\n",
    "def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "            iters=10, strat='normal', scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with missing values.\n",
    "        continuous_cols (list of str): Names of continuous numeric columns.\n",
    "        discrete_cols (list of str): Names of discrete numeric columns.\n",
    "        categorical_cols (list of str): Names of categorical columns.\n",
    "        iters (int): Number of MICE iterations.\n",
    "        strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns if requested\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Run MICE imputation\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df_imputed,\n",
    "        random_state=0,\n",
    "        mean_match_strategy=strat,\n",
    "        variable_schema=None\n",
    "    )\n",
    "\n",
    "    kernel.mice(iterations=iters, verbose=False)\n",
    "    df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "    # Post-process discrete and categorical columns\n",
    "    if discrete_cols:\n",
    "        df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_completed[col] = np.round(df_completed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_completed[col] = df_completed[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "    return df_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mice_imputed = do_mice(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols,\n",
    "                    #    iters=10, strat='normal', scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using MissForest.\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         iters (int): Maximum number of iterations.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "    \n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "    \n",
    "#     imputer = MissForest(max_iter=iters, categorical=categorical_cols)\n",
    "#     df_imputed_result = imputer.fit_transform(df_imputed)\n",
    "    \n",
    "#     if discrete_cols:\n",
    "#         df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "    \n",
    "#     if categorical_cols:\n",
    "#         df_imputed_result[categorical_cols] = df_imputed_result[categorical_cols].round().astype(int)\n",
    "    \n",
    "#     if scale:\n",
    "#         # Reverse scaling for continuous columns\n",
    "#         df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "    \n",
    "#     return df_imputed_result\n",
    "\n",
    "# # mf_imputed = do_mf(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=5, scale=False)\n",
    "\n",
    "def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using MissForest.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        iters (int): Maximum number of iterations.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Impute with MissForest\n",
    "    imputer = MissForest(max_iter=iters)\n",
    "    df_imputed_result = pd.DataFrame(imputer.fit_transform(df_imputed), columns=df.columns)\n",
    "\n",
    "    # Post-process discrete columns\n",
    "    if discrete_cols:\n",
    "        df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "\n",
    "    # Post-process categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed_result[col] = df_imputed_result[col].round().astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed_result[col] = df_imputed_result[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "\n",
    "    return df_imputed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "              layer:list=[256,256], vae:bool=True, samples:int=10, random_seed:float=96 ):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but non-continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      \n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes.\n",
    "    \"\"\"\n",
    "    # 1. Convert categorical columns and get categorical metadata.\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "    \n",
    "    # 2. Define the numeric columns.\n",
    "    num_cols = discrete_cols + continuous_cols  # these are the numeric columns\n",
    "\n",
    "    # 3. Drop original categorical columns and combine with the converted categorical data.\n",
    "    df_copy = df.drop(columns=categorical_cols,axis=1)\n",
    "    constructor_list = [df_copy, md_cat_data]\n",
    "    data_in = pd.concat(constructor_list, axis=1)\n",
    "    \n",
    "    # 4. Scale non-categorical columns BEFORE imputation.\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "    \n",
    "    # 5. Build and train the imputer using the scaled data.\n",
    "    imputer = md.Midas(layer_structure=layer, vae_layer=vae, seed=random_seed, input_drop=0.75)\n",
    "    # Use md_cats as softmax columns for categorical outputs.\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "    \n",
    "    # 6. Generate imputations.\n",
    "    imps = imputer.generate_samples(m=samples).output_list\n",
    "    \n",
    "    # 7. Post-process each imputed DataFrame.\n",
    "    for idx, imp_df in enumerate(imps):\n",
    "        # Reverse transform the numeric columns.\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "        \n",
    "        # Process categorical columns.\n",
    "        # For each softmax group in md_cats, choose the column with the highest probability.\n",
    "        tmp_cat = []\n",
    "        for group in md_cats:\n",
    "            # idxmax returns the column name with maximum value per row for this group.\n",
    "            tmp_cat.append(imp_df[group].idxmax(axis=1))\n",
    "        # Assume the order of md_cats corresponds to categorical_cols.\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        \n",
    "        # Drop the softmax columns.\n",
    "        flat_cats = [col for group in md_cats for col in group]\n",
    "        tmp_cat = [imp_df[x].idxmax(axis=1) for x in md_cats]\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        imp_df = pd.concat([imp_df, cat_df], axis=1).drop(columns=flat_cats, axis=1)\n",
    "        \n",
    "        # Handle discrete data by rounding the values.\n",
    "        imp_df[discrete_cols] = imp_df[discrete_cols].round()\n",
    "        \n",
    "        # Replace the processed DataFrame in the list.\n",
    "        imps[idx] = imp_df\n",
    "\n",
    "        ### make method info\n",
    "        method_info = f'MIDAS, params: samples={samples} ,layer={layer}, vae={vae}'\n",
    "    return imps, method_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midas_imputed = do_midas(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Missingness Creation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Improved Evaluation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameter Optimization Function using Optuna\n",
    "# ------------------------------------------------------------------------------\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    missing_percent, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    "            params['layer'] = trial.suggest_categorical(\"layer\", [[256,256], [128,128], [512,256]])\n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, summary_table, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_midas,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mice,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_excel(r\"C:\\Users\\Matin\\Downloads\\Data for Dr.Matin.xlsx\", 's1')\n",
    "new_df.drop(['n', 'ID','Gen.code'],axis=1,inplace=True)\n",
    "new_df = new_df[:1000]\n",
    "# continuous_cols, discrete_cols, categorical_cols, df2, encoders = prep(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df2,missing_percent=30,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=new_df,missing_percent=30, timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_missingness(df, show_missingness=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=random_state).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for cat in categorical_cols:\n",
    "        if cat in best_imputed_df:\n",
    "            best_imputed_df[cat] = best_imputed_df[cat].astype(object)\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = \\\n",
    "                imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    df_missing, \n",
    "                                    mask_df, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    # _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    " # Dynamically define the layer architecture\n",
    "            num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "            params[\"layer\"] = [trial.suggest_categorical(f\"layer_units_{i}\", [64, 128, 256, 512]) for i in range(num_layers)]            \n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, _, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df,\n",
    "             continuous_cols=None,\n",
    "             discrete_cols=None,\n",
    "             categorical_cols=None,\n",
    "             layer: list = [256, 256],\n",
    "             vae: bool = True,\n",
    "             samples: int = 10,\n",
    "             random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe with NaNs in both numeric & categorical.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but not continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "\n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes, with original dtypes restored.\n",
    "      method_info (str): Summary of MIDAS params used.\n",
    "    \"\"\"\n",
    "    # 1. Onehot encode the categoricals\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "\n",
    "    # 2. Build the wide DF: drop raw cats, append onehots\n",
    "    df_num = df.drop(columns=categorical_cols)\n",
    "    data_in = pd.concat([df_num, md_cat_data], axis=1)\n",
    "\n",
    "    # 3. Record & reinsert the NaN locations so MIDAS sees them as missing\n",
    "    na_mask = data_in.isnull()\n",
    "    data_in[na_mask] = np.nan\n",
    "\n",
    "    # 4. Scale only the numeric columns in place\n",
    "    num_cols = discrete_cols + continuous_cols\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "\n",
    "    # 5. Build & train the MIDAS model\n",
    "    imputer = md.Midas(\n",
    "        layer_structure=layer,\n",
    "        vae_layer=vae,\n",
    "        seed=random_seed,\n",
    "        input_drop=0.75\n",
    "    )\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "\n",
    "    # 6. Generate multiple imputations\n",
    "    raw_imps = imputer.generate_samples(m=samples).output_list\n",
    "\n",
    "    # 7. Decode each imputed DF back to original structure\n",
    "    flat_cats = [c for grp in md_cats for c in grp]\n",
    "    imps = []\n",
    "\n",
    "    for imp_df in raw_imps:\n",
    "        # 7a. inversescale numeric cols\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "\n",
    "        # 7b. decode onehots (before dropping them!)\n",
    "        decoded = {}\n",
    "        for i, grp in enumerate(md_cats):\n",
    "            # just in case, only keep those actually present\n",
    "            present = [c for c in grp if c in imp_df.columns]\n",
    "            # idxmax  gives the dummy column name with highest prob\n",
    "            decoded[categorical_cols[i]] = imp_df[present].idxmax(axis=1)\n",
    "\n",
    "        cat_df = pd.DataFrame(decoded, index=imp_df.index)\n",
    "\n",
    "        # 7c. now drop the dummy cols\n",
    "        base = imp_df.drop(columns=flat_cats, errors='ignore')\n",
    "\n",
    "        # 7d. concat in your decoded cat columns\n",
    "        merged = pd.concat([base, cat_df], axis=1)\n",
    "\n",
    "        # 7e. round discrete cols\n",
    "        merged[discrete_cols] = merged[discrete_cols].round().astype(int)\n",
    "\n",
    "        imps.append(merged)\n",
    "\n",
    "    method_info = f\"MIDAS, params: samples={samples}, layer={layer}, vae={vae}\"\n",
    "    return imps, method_info\n",
    "\n",
    "\n",
    "def run_full_pipeline(df: pd.DataFrame, \n",
    "                      simulate:bool=False,               # True for simulated missingness, False for random missingness\n",
    "                      missingness_value: float = 10.0,   # used only for random missingness (percent)\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "\n",
    "    The pipeline performs these steps:\n",
    "    \n",
    "      1. Preprocesses the DataFrame using `prep`, which cleans the data,\n",
    "         encodes categorical variables, and splits features into continuous,\n",
    "         discrete, and categorical lists.\n",
    "      2. Introduces missingness using either simulated missingness (reintroducing missingness \n",
    "         based on the original NaN proportions) or random missingness (dropping values randomly\n",
    "         given a specified missing percentage).\n",
    "      3. Runs hyperparameter optimization (via `optimize_imputation_hyperparams`) for each candidate \n",
    "         imputation method (e.g., do_knn, do_mice, do_mf, do_midas).\n",
    "         \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        missing_type (str): \"simulate\" to simulate missingness using original missing proportions,\n",
    "                            \"random\" to drop values randomly.\n",
    "        missingness_value (float): Percentage of missingness (only used if missing_type == \"random\").\n",
    "        show_missingness (bool): If True, prints missingness comparison when using simulate missingness.\n",
    "        timelimit (int): Time limit (in seconds) for each hyperparameter optimization study.\n",
    "        min_trials (int): Minimum number of trials for each study.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are method names (strings) and the values are the best \n",
    "              hyperparameter dictionaries (from the best Optuna trial) for that method.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Step 1: Preprocess Data\n",
    "    # Note: For simulation, the missing proportions are taken from the original df.\n",
    "    if simulate: \n",
    "        # simulate_missingness returns: (complete_df, df_with_missing, missing_mask)\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(df, \n",
    "                                                                      show_missingness=show_missingness,\n",
    "                                                                      random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(df, \n",
    "                                                                 missingness=missingness_value, \n",
    "                                                                 random_seed=random_seed)\n",
    "        \n",
    "    # Step 2: Preprocess Data, convert categorical cols to encoded values and find the data types.\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "    \n",
    "\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    # Optimize hyperparameters for each imputation method candidate.\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "        # Optimize hyperparameters for each imputation method candidate.\n",
    "    \n",
    "    for key, val in best_hyperparams.items():\n",
    "        if key == 'KNN':\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MICE':\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MissForest':\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], scale=val['scale']) \n",
    "\n",
    "        elif key == 'MIDAS':\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, layer=val['layer'], vae=val['vae'], samples=val['samples'])\n",
    "\n",
    "    # Create a list of imputed DataFrames.  \n",
    "    imputed_dfs = [df_knn, df_mice, df_mf, df_midas] \n",
    "    # decoded_imputed_dfs = []\n",
    "    # for i in imputed_dfs:\n",
    "    #     decoded_df = reverse_encoding(i, encoders)\n",
    "    #     decoded_imputed_dfs.append(decoded_df)\n",
    "          \n",
    "    # Create a list of method names.    \n",
    "    method_names = ['KNN', 'MICE', 'MissForest', 'MIDAS']\n",
    "    \n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        # Determine the data type label.\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        # Only evaluate columns that had artificial missing values.\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "        \n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            # Ensure the original column is numeric.\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                raise ValueError(f\"Column '{col}' is marked as numeric but contains non-numeric values.\")\n",
    "            for df_imp in imputed_dfs:\n",
    "                # Convert values to numeric.\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            \n",
    "            # Compute additional metrics for the best method.\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            \n",
    "            # Compute fraction within 10%.\n",
    "            # For nonzero original values, check error <= 0.1 * |original|.\n",
    "            # For zeros, require the imputed value to be exactly 0.\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "            \n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            # For categorical columns, compute accuracy.\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            # Extra metrics are not applicable for categoricals.\n",
    "            error_sd = np.nan\n",
    "            max_error = np.nan\n",
    "            min_error = np.nan\n",
    "            within_10pct = np.nan\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "        \n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "        \n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "    \n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    # Build best-imputed DataFrame by replacing masked entries with values from the best method.\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline2(df: pd.DataFrame, \n",
    "                      simulate: bool = False,               \n",
    "                      missingness_value: float = 10.0,   \n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "    \"\"\"\n",
    "    # Step 1: Create missingness (simulated or random)\n",
    "    if simulate: \n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed)\n",
    "\n",
    "    # Step 2: Preprocess for column types\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    # Step 3: Optimize hyperparameters per method\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_trial.params} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    # Step 4: Run best imputation for each method and collect valid results\n",
    "    if best_hyperparams.get('KNN'):\n",
    "        try:\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols,\n",
    "                            discrete_cols=discrete_cols,\n",
    "                            categorical_cols=categorical_cols,\n",
    "                            n_neighbors=best_hyperparams['KNN']['n_neighbors'],\n",
    "                            scale=best_hyperparams['KNN']['scale'])\n",
    "            imputed_dfs.append(df_knn)\n",
    "            method_names.append('KNN')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with KNN: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MICE'):\n",
    "        try:\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols,\n",
    "                              discrete_cols=discrete_cols,\n",
    "                              categorical_cols=categorical_cols,\n",
    "                              iters=best_hyperparams['MICE']['iters'],\n",
    "                              strat=best_hyperparams['MICE']['strat'],\n",
    "                              scale=best_hyperparams['MICE']['scale'])\n",
    "            imputed_dfs.append(df_mice)\n",
    "            method_names.append('MICE')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MICE: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MissForest'):\n",
    "        try:\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols,\n",
    "                          discrete_cols=discrete_cols,\n",
    "                          categorical_cols=categorical_cols,\n",
    "                          iters=best_hyperparams['MissForest']['iters'],\n",
    "                          scale=best_hyperparams['MissForest']['scale'])\n",
    "            imputed_dfs.append(df_mf)\n",
    "            method_names.append('MissForest')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MissForest: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MIDAS'):\n",
    "        try:\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                   discrete_cols=discrete_cols,\n",
    "                                   categorical_cols=categorical_cols,\n",
    "                                   layer=best_hyperparams['MIDAS']['layer'],\n",
    "                                   vae=best_hyperparams['MIDAS']['vae'],\n",
    "                                   samples=best_hyperparams['MIDAS']['samples'])\n",
    "            imputed_dfs.append(df_midas)\n",
    "            method_names.append('MIDAS')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MIDAS: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    # Step 5: Evaluate and select best method per column\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            summary_list.append({\n",
    "                'Column': col, 'Data Type': col_data_type, 'Best Method': None,\n",
    "                'Metric': np.nan, 'Error_SD': np.nan, 'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan, 'Within_10pct': np.nan\n",
    "            })\n",
    "            best_method_per_col[col] = None\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = max_error = min_error = within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    # Step 6: Final best-imputed DataFrame\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline3(df: pd.DataFrame, \n",
    "                      simulate: bool = False,\n",
    "                      missingness_value: float = 10.0,\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600,\n",
    "                      min_trials: int = 20,\n",
    "                      random_seed: int = 96):\n",
    "    if simulate:\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed\n",
    "        )\n",
    "    else:\n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "\n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    for method in ['KNN', 'MICE', 'MissForest', 'MIDAS']:\n",
    "        val = best_hyperparams.get(method)\n",
    "        if not val:\n",
    "            continue\n",
    "        try:\n",
    "            if method == 'KNN':\n",
    "                df_knn = do_knn(df_missing, continuous_cols=continuous_cols, \n",
    "                                discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_knn)\n",
    "                method_names.append('KNN')\n",
    "\n",
    "            elif method == 'MICE':\n",
    "                df_mice = do_mice(df_missing, continuous_cols=continuous_cols, \n",
    "                                  discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                  iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mice)\n",
    "                method_names.append('MICE')\n",
    "\n",
    "            elif method == 'MissForest':\n",
    "                df_mf = do_mf(df_missing, continuous_cols=continuous_cols, \n",
    "                              discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                              iters=val['iters'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mf)\n",
    "                method_names.append('MissForest')\n",
    "\n",
    "            elif method == 'MIDAS':\n",
    "                df_midas_list, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                            discrete_cols=discrete_cols,\n",
    "                                            categorical_cols=categorical_cols,\n",
    "                                            layer=val['layer'], vae=val['vae'], \n",
    "                                            samples=val['samples'])\n",
    "                imputed_dfs.extend(df_midas_list)\n",
    "                method_names.extend([f'MIDAS_{i+1}' for i in range(len(df_midas_list))])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with {method}: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = run_full_pipeline(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impsss, table = run_full_pipeline3(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline4(df: pd.DataFrame, \n",
    "                      simulate: bool = False,\n",
    "                      build: bool = False,\n",
    "                      missingness_value: float = 10.0,\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600,\n",
    "                      min_trials: int = 20,\n",
    "                      random_seed: int = 96):\n",
    "    if simulate:\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed\n",
    "        )\n",
    "    else:\n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "\n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    for method in ['KNN', 'MICE', 'MissForest', 'MIDAS']:\n",
    "        val = best_hyperparams.get(method)\n",
    "        if not val:\n",
    "            continue\n",
    "        try:\n",
    "            if method == 'KNN':\n",
    "                df_knn = do_knn(df_missing, continuous_cols=continuous_cols, \n",
    "                                discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_knn)\n",
    "                method_names.append('KNN')\n",
    "\n",
    "            elif method == 'MICE':\n",
    "                df_mice = do_mice(df_missing, continuous_cols=continuous_cols, \n",
    "                                  discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                  iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mice)\n",
    "                method_names.append('MICE')\n",
    "\n",
    "            elif method == 'MissForest':\n",
    "                df_mf = do_mf(df_missing, continuous_cols=continuous_cols, \n",
    "                              discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                              iters=val['iters'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mf)\n",
    "                method_names.append('MissForest')\n",
    "\n",
    "            elif method == 'MIDAS':\n",
    "                df_midas_list, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                            discrete_cols=discrete_cols,\n",
    "                                            categorical_cols=categorical_cols,\n",
    "                                            layer=val['layer'], vae=val['vae'], \n",
    "                                            samples=val['samples'])\n",
    "                imputed_dfs.extend(df_midas_list)\n",
    "                method_names.extend([f'MIDAS_{i+1}' for i in range(len(df_midas_list))])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with {method}: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    # Build the final imputed DataFrame if both build and simulate are True\n",
    "    if build and simulate:\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"Original DataFrame has no missing values. Build is not needed.\")\n",
    "            final_imputed_df = df.copy()\n",
    "        else:\n",
    "            imputed_dfs_build = []\n",
    "            method_names_build = []\n",
    "\n",
    "            for method in ['KNN', 'MICE', 'MissForest', 'MIDAS']:\n",
    "                val = best_hyperparams.get(method)\n",
    "                if not val:\n",
    "                    continue\n",
    "                try:\n",
    "                    if method == 'KNN':\n",
    "                        df_knn_build = do_knn(df, continuous_cols=continuous_cols, \n",
    "                                            discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                            n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "                        imputed_dfs_build.append(df_knn_build)\n",
    "                        method_names_build.append('KNN')\n",
    "\n",
    "                    elif method == 'MICE':\n",
    "                        df_mice_build = do_mice(df, continuous_cols=continuous_cols, \n",
    "                                              discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                              iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "                        imputed_dfs_build.append(df_mice_build)\n",
    "                        method_names_build.append('MICE')\n",
    "\n",
    "                    elif method == 'MissForest':\n",
    "                        df_mf_build = do_mf(df, continuous_cols=continuous_cols, \n",
    "                                          discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                          iters=val['iters'], scale=val['scale'])\n",
    "                        imputed_dfs_build.append(df_mf_build)\n",
    "                        method_names_build.append('MissForest')\n",
    "\n",
    "                    elif method == 'MIDAS':\n",
    "                        df_midas_list_build, _ = do_midas(df, continuous_cols=continuous_cols,\n",
    "                                                        discrete_cols=discrete_cols,\n",
    "                                                        categorical_cols=categorical_cols,\n",
    "                                                        layer=val['layer'], vae=val['vae'], \n",
    "                                                        samples=val['samples'],\n",
    "                                                        random_seed=random_seed)\n",
    "                        imputed_dfs_build.extend(df_midas_list_build)\n",
    "                        method_names_build.extend([f'MIDAS_{i+1}' for i in range(len(df_midas_list_build))])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to impute with {method} during build: {e}\")\n",
    "\n",
    "            # Map method names to their imputed DataFrames\n",
    "            method_to_imp_df = dict(zip(method_names_build, imputed_dfs_build))\n",
    "            final_imputed_df = df.copy()\n",
    "            for col in df.columns:\n",
    "                if df[col].isnull().sum() > 0:\n",
    "                    col_summary = summary_table[summary_table['Column'] == col]\n",
    "                    if col_summary.empty:\n",
    "                        continue\n",
    "                    best_method = col_summary['Best Method'].iloc[0]\n",
    "                    if best_method and best_method in method_to_imp_df:\n",
    "                        imp_df = method_to_imp_df[best_method]\n",
    "                        missing_mask = df[col].isnull()\n",
    "                        final_imputed_df.loc[missing_mask, col] = imp_df.loc[missing_mask, col]\n",
    "\n",
    "            best_imputed_df = final_imputed_df\n",
    "\n",
    "    return best_imputed_df, summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 10:49:26,860] A new study created in memory with name: no-name-5fd8dbde-0864-4e85-b5b4-4b2e834d1a56\n",
      "[I 2025-04-17 10:49:26,965] Trial 0 finished with value: 148864.08792925824 and parameters: {'n_neighbors': 8, 'scale': False}. Best is trial 0 with value: 148864.08792925824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing hyperparameters for KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 10:49:27,082] Trial 1 finished with value: 148864.14124771062 and parameters: {'n_neighbors': 9, 'scale': True}. Best is trial 0 with value: 148864.08792925824.\n",
      "[I 2025-04-17 10:49:27,211] Trial 2 finished with value: 148864.2995161089 and parameters: {'n_neighbors': 11, 'scale': False}. Best is trial 0 with value: 148864.08792925824.\n",
      "[I 2025-04-17 10:49:27,320] Trial 3 finished with value: 148864.2995161089 and parameters: {'n_neighbors': 11, 'scale': True}. Best is trial 0 with value: 148864.08792925824.\n",
      "[I 2025-04-17 10:49:27,427] Trial 4 finished with value: 148864.0746643773 and parameters: {'n_neighbors': 10, 'scale': True}. Best is trial 4 with value: 148864.0746643773.\n",
      "[I 2025-04-17 10:49:27,533] Trial 5 finished with value: 148864.2995161089 and parameters: {'n_neighbors': 11, 'scale': True}. Best is trial 4 with value: 148864.0746643773.\n",
      "[I 2025-04-17 10:49:27,630] Trial 6 finished with value: 148864.33548051486 and parameters: {'n_neighbors': 9, 'scale': False}. Best is trial 4 with value: 148864.0746643773.\n",
      "[I 2025-04-17 10:49:27,727] Trial 7 finished with value: 148863.85539453602 and parameters: {'n_neighbors': 6, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:27,839] Trial 8 finished with value: 148864.65656517097 and parameters: {'n_neighbors': 6, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:27,944] Trial 9 finished with value: 148864.08792925824 and parameters: {'n_neighbors': 8, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,050] Trial 10 finished with value: 148864.93255723445 and parameters: {'n_neighbors': 3, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,158] Trial 11 finished with value: 148864.2066346154 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,257] Trial 12 finished with value: 148864.36279532968 and parameters: {'n_neighbors': 5, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,370] Trial 13 finished with value: 148864.2066346154 and parameters: {'n_neighbors': 14, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,483] Trial 14 finished with value: 148864.65656517097 and parameters: {'n_neighbors': 6, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,586] Trial 15 finished with value: 148864.93255723445 and parameters: {'n_neighbors': 3, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,706] Trial 16 finished with value: 148864.2066346154 and parameters: {'n_neighbors': 13, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,814] Trial 17 finished with value: 148864.36279532968 and parameters: {'n_neighbors': 5, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:28,927] Trial 18 finished with value: 148864.5043259419 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:29,027] Trial 19 finished with value: 148864.2066346154 and parameters: {'n_neighbors': 12, 'scale': False}. Best is trial 7 with value: 148863.85539453602.\n",
      "[I 2025-04-17 10:49:29,029] A new study created in memory with name: no-name-509799df-cdd6-4916-8834-f9a93e030ef3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  n_neighbors: 6\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 148863.85539453602\n",
      "Best hyperparameters for KNN: {'n_neighbors': 6, 'scale': False} with best agg error of 148863.85539453602\n",
      "\n",
      "Optimizing hyperparameters for MICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:831: RuntimeWarning: overflow encountered in cast\n",
      "  bachelor_preds = bachelor_preds.astype(_PRE_LINK_DATATYPE)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:776: RuntimeWarning: overflow encountered in cast\n",
      "  candidate_preds = candidate_preds.astype(_PRE_LINK_DATATYPE)  # type: ignore\n",
      "[W 2025-04-17 10:49:30,223] Trial 0 failed with parameters: {'iters': 15, 'strat': 'shap', 'scale': True} because of the following error: ValueError('data must be finite, check for nan or inf values').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_39768\\937945787.py\", line 216, in objective\n",
      "    imputed_df = imputation_func(df_missing,\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_39768\\3092344904.py\", line 90, in do_mice\n",
      "    kernel.mice(iterations=iters, verbose=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 1186, in mice\n",
      "    imputation_values = self._mean_match_mice(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 971, in _mean_match_mice\n",
      "    imputation_values = self._mean_match_nearest_neighbors(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 602, in _mean_match_nearest_neighbors\n",
      "    kd_tree = KDTree(candidate_preds, leafsize=16, balanced_tree=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\_kdtree.py\", line 360, in __init__\n",
      "    super().__init__(data, leafsize, compact_nodes, copy_data,\n",
      "  File \"_ckdtree.pyx\", line 561, in scipy.spatial._ckdtree.cKDTree.__init__\n",
      "ValueError: data must be finite, check for nan or inf values\n",
      "[W 2025-04-17 10:49:30,226] Trial 0 failed with value None.\n",
      "[I 2025-04-17 10:49:30,228] A new study created in memory with name: no-name-b1a86e64-66aa-4cfd-ae7f-3b15d04b0710\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while optimizing MICE: data must be finite, check for nan or inf values\n",
      "\n",
      "Optimizing hyperparameters for MissForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:05<00:00,  1.20it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 7/7 [00:02<00:00,  3.25it/s]\n",
      "[I 2025-04-17 10:49:38,464] Trial 0 finished with value: 148864.2066345438 and parameters: {'iters': 7, 'scale': False}. Best is trial 0 with value: 148864.2066345438.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 7/7 [00:05<00:00,  1.26it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 7/7 [00:02<00:00,  3.34it/s]\n",
      "[I 2025-04-17 10:49:46,348] Trial 1 finished with value: 148864.206634553 and parameters: {'iters': 7, 'scale': True}. Best is trial 0 with value: 148864.2066345438.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 6/6 [00:04<00:00,  1.28it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 6/6 [00:01<00:00,  3.32it/s]\n",
      "[I 2025-04-17 10:49:53,034] Trial 2 finished with value: 148864.20663419348 and parameters: {'iters': 6, 'scale': False}. Best is trial 2 with value: 148864.20663419348.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 6/6 [00:04<00:00,  1.24it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 6/6 [00:01<00:00,  3.40it/s]\n",
      "[I 2025-04-17 10:49:59,844] Trial 3 finished with value: 148864.20663419348 and parameters: {'iters': 6, 'scale': False}. Best is trial 2 with value: 148864.20663419348.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 4/4 [00:03<00:00,  1.28it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 4/4 [00:01<00:00,  3.40it/s]\n",
      "[I 2025-04-17 10:50:04,364] Trial 4 finished with value: 148864.77804999144 and parameters: {'iters': 4, 'scale': True}. Best is trial 2 with value: 148864.20663419348.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 15/15 [00:12<00:00,  1.22it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 15/15 [00:04<00:00,  3.30it/s]\n",
      "[I 2025-04-17 10:50:21,421] Trial 5 finished with value: 148864.20663459407 and parameters: {'iters': 15, 'scale': False}. Best is trial 2 with value: 148864.20663419348.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 3/3 [00:02<00:00,  1.27it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 3/3 [00:00<00:00,  3.39it/s]\n",
      "[I 2025-04-17 10:50:24,887] Trial 6 finished with value: 148872.18280968122 and parameters: {'iters': 3, 'scale': True}. Best is trial 2 with value: 148864.20663419348.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 12/12 [00:09<00:00,  1.27it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 12/12 [00:03<00:00,  3.33it/s]\n",
      "[I 2025-04-17 10:50:38,176] Trial 7 finished with value: 148864.20663459407 and parameters: {'iters': 12, 'scale': False}. Best is trial 2 with value: 148864.20663419348.\n",
      "[I 2025-04-17 10:50:38,178] A new study created in memory with name: no-name-6ade4b29-c14f-4ecf-a75b-1abda055bfc8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  iters: 6\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 148864.20663419348\n",
      "Best hyperparameters for MissForest: {'iters': 6, 'scale': False} with best agg error of 148864.20663419348\n",
      "\n",
      "Optimizing hyperparameters for MIDAS...\n",
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-17 10:50:43,521] Trial 0 failed with parameters: {'num_layers': 1, 'layer_units_0': 256, 'vae': False, 'samples': 14} because of the following error: ZeroDivisionError('division by zero').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_39768\\937945787.py\", line 239, in objective\n",
      "    imputed_dfs, method_info = imputation_func(df_missing,\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_39768\\436080122.py\", line 46, in do_midas\n",
      "    imputer.train_model(training_epochs=20)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\MIDASpy\\midas_base.py\", line 818, in train_model\n",
      "    print('Epoch:', epoch, \", loss:\", str(run_loss / count), flush=True)\n",
      "ZeroDivisionError: division by zero\n",
      "[W 2025-04-17 10:50:43,523] Trial 0 failed with value None.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while optimizing MIDAS: division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:04<00:00,  1.30it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 6/6 [00:02<00:00,  2.91it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 83%| | 5/6 [00:45<00:09,  9.23s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 83%| | 5/6 [00:54<00:10, 10.98s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 5/5 [00:01<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df2 = create_missings(new_df, missingness=10)[1]\n",
    "\n",
    "imp_df2, res_table = run_full_pipeline4(new_df2, timelimit=60, simulate=True, random_seed=96, build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_df2.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:30:50,503] A new study created in memory with name: no-name-ffe7b871-66a0-4fdd-8401-3f254ca889e1\n",
      "[I 2025-04-17 11:30:50,610] Trial 0 finished with value: 151928.04991514524 and parameters: {'n_neighbors': 7, 'scale': False}. Best is trial 0 with value: 151928.04991514524.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing hyperparameters for KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:30:50,710] Trial 1 finished with value: 151588.46830800237 and parameters: {'n_neighbors': 8, 'scale': False}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:50,820] Trial 2 finished with value: 156330.99018300237 and parameters: {'n_neighbors': 8, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,153] Trial 3 finished with value: 153012.67953419287 and parameters: {'n_neighbors': 5, 'scale': False}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,248] Trial 4 finished with value: 153012.67953419287 and parameters: {'n_neighbors': 5, 'scale': False}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,362] Trial 5 finished with value: 156805.7843754627 and parameters: {'n_neighbors': 5, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,473] Trial 6 finished with value: 155540.24196981895 and parameters: {'n_neighbors': 9, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,585] Trial 7 finished with value: 154907.84721673254 and parameters: {'n_neighbors': 10, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,701] Trial 8 finished with value: 154637.01357727675 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,814] Trial 9 finished with value: 153959.6571175262 and parameters: {'n_neighbors': 12, 'scale': True}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:51,917] Trial 10 finished with value: 154275.29294160026 and parameters: {'n_neighbors': 15, 'scale': False}. Best is trial 1 with value: 151588.46830800237.\n",
      "[I 2025-04-17 11:30:52,047] Trial 11 finished with value: 150940.85114891146 and parameters: {'n_neighbors': 11, 'scale': False}. Best is trial 11 with value: 150940.85114891146.\n",
      "[I 2025-04-17 11:30:52,153] Trial 12 finished with value: 150796.56299715582 and parameters: {'n_neighbors': 12, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,272] Trial 13 finished with value: 150796.56299715582 and parameters: {'n_neighbors': 12, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,381] Trial 14 finished with value: 153281.3807541475 and parameters: {'n_neighbors': 14, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,488] Trial 15 finished with value: 150796.56299715582 and parameters: {'n_neighbors': 12, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,595] Trial 16 finished with value: 152134.4302692356 and parameters: {'n_neighbors': 13, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,701] Trial 17 finished with value: 155543.59644953677 and parameters: {'n_neighbors': 3, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,806] Trial 18 finished with value: 154275.29294160026 and parameters: {'n_neighbors': 15, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,918] Trial 19 finished with value: 152134.4302692356 and parameters: {'n_neighbors': 13, 'scale': False}. Best is trial 12 with value: 150796.56299715582.\n",
      "[I 2025-04-17 11:30:52,920] A new study created in memory with name: no-name-2eaac788-fdcd-4a64-9cda-ed1d513b6ebf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  n_neighbors: 12\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 150796.56299715582\n",
      "Best hyperparameters for KNN: {'n_neighbors': 12, 'scale': False} with best agg error of 150796.56299715582\n",
      "\n",
      "Optimizing hyperparameters for MICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:31:21,578] Trial 0 finished with value: 142458.48845238093 and parameters: {'iters': 14, 'strat': 'normal', 'scale': True}. Best is trial 0 with value: 142458.48845238093.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:831: RuntimeWarning: overflow encountered in cast\n",
      "  bachelor_preds = bachelor_preds.astype(_PRE_LINK_DATATYPE)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:776: RuntimeWarning: overflow encountered in cast\n",
      "  candidate_preds = candidate_preds.astype(_PRE_LINK_DATATYPE)  # type: ignore\n",
      "[W 2025-04-17 11:31:22,650] Trial 1 failed with parameters: {'iters': 12, 'strat': 'shap', 'scale': False} because of the following error: ValueError('data must be finite, check for nan or inf values').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"d:\\Prog\\AutoStats\\impute.py\", line 535, in objective\n",
      "    imputed_df = imputation_func(df_missing,\n",
      "  File \"d:\\Prog\\AutoStats\\impute.py\", line 212, in do_mice\n",
      "    kernel.mice(iterations=iters, verbose=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 1186, in mice\n",
      "    imputation_values = self._mean_match_mice(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 971, in _mean_match_mice\n",
      "    imputation_values = self._mean_match_nearest_neighbors(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 602, in _mean_match_nearest_neighbors\n",
      "    kd_tree = KDTree(candidate_preds, leafsize=16, balanced_tree=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\_kdtree.py\", line 360, in __init__\n",
      "    super().__init__(data, leafsize, compact_nodes, copy_data,\n",
      "  File \"_ckdtree.pyx\", line 561, in scipy.spatial._ckdtree.cKDTree.__init__\n",
      "ValueError: data must be finite, check for nan or inf values\n",
      "[W 2025-04-17 11:31:22,652] Trial 1 failed with value None.\n",
      "[I 2025-04-17 11:31:22,654] A new study created in memory with name: no-name-63798fbf-1d38-43d5-a797-694a7084292c\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while optimizing MICE: data must be finite, check for nan or inf values\n",
      "\n",
      "Optimizing hyperparameters for MissForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:10<00:00,  1.11it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 12/12 [00:03<00:00,  3.15it/s]\n",
      "[I 2025-04-17 11:31:37,547] Trial 0 finished with value: 152665.0804310027 and parameters: {'iters': 12, 'scale': False}. Best is trial 0 with value: 152665.0804310027.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 5/5 [00:04<00:00,  1.24it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 5/5 [00:01<00:00,  3.16it/s]\n",
      "[I 2025-04-17 11:31:43,360] Trial 1 finished with value: 152664.81852253052 and parameters: {'iters': 5, 'scale': False}. Best is trial 1 with value: 152664.81852253052.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 5/5 [00:04<00:00,  1.23it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 5/5 [00:01<00:00,  3.34it/s]\n",
      "[I 2025-04-17 11:31:49,131] Trial 2 finished with value: 152664.81852253052 and parameters: {'iters': 5, 'scale': False}. Best is trial 1 with value: 152664.81852253052.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 8/8 [00:06<00:00,  1.22it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 8/8 [00:02<00:00,  2.97it/s]\n",
      "[I 2025-04-17 11:31:58,601] Trial 3 finished with value: 152665.08043097943 and parameters: {'iters': 8, 'scale': True}. Best is trial 1 with value: 152664.81852253052.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 14/14 [00:11<00:00,  1.23it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 14/14 [00:04<00:00,  2.93it/s]\n",
      "[I 2025-04-17 11:32:14,989] Trial 4 finished with value: 152665.0804310027 and parameters: {'iters': 14, 'scale': False}. Best is trial 1 with value: 152664.81852253052.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 14/14 [00:12<00:00,  1.10it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 14/14 [00:05<00:00,  2.77it/s]\n",
      "[I 2025-04-17 11:32:32,977] Trial 5 finished with value: 152665.0804310027 and parameters: {'iters': 14, 'scale': False}. Best is trial 1 with value: 152664.81852253052.\n",
      "[I 2025-04-17 11:32:32,979] A new study created in memory with name: no-name-a8491f27-6978-4fda-bb0d-014647767ecb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  iters: 5\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 152664.81852253052\n",
      "Best hyperparameters for MissForest: {'iters': 5, 'scale': False} with best agg error of 152664.81852253052\n",
      "\n",
      "Optimizing hyperparameters for MIDAS...\n",
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.586223602294922\n",
      "Epoch: 1 , loss: 19.801607131958008\n",
      "Epoch: 2 , loss: 20.72382926940918\n",
      "Epoch: 3 , loss: 19.1903133392334\n",
      "Epoch: 4 , loss: 18.736347198486328\n",
      "Epoch: 5 , loss: 21.04656982421875\n",
      "Epoch: 6 , loss: 21.109683990478516\n",
      "Epoch: 7 , loss: 19.2098445892334\n",
      "Epoch: 8 , loss: 20.16907501220703\n",
      "Epoch: 9 , loss: 17.956172943115234\n",
      "Epoch: 10 , loss: 19.853092193603516\n",
      "Epoch: 11 , loss: 21.23710060119629\n",
      "Epoch: 12 , loss: 17.979694366455078\n",
      "Epoch: 13 , loss: 20.591459274291992\n",
      "Epoch: 14 , loss: 21.486764907836914\n",
      "Epoch: 15 , loss: 18.844812393188477\n",
      "Epoch: 16 , loss: 20.497222900390625\n",
      "Epoch: 17 , loss: 19.607391357421875\n",
      "Epoch: 18 , loss: 19.53896713256836\n",
      "Epoch: 19 , loss: 19.823070526123047\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:32:43,746] Trial 0 finished with value: 255797.870547739 and parameters: {'num_layers': 3, 'layer_units_0': 256, 'layer_units_1': 512, 'layer_units_2': 512, 'vae': False, 'samples': 16}. Best is trial 0 with value: 255797.870547739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.628690719604492\n",
      "Epoch: 1 , loss: 19.993013381958008\n",
      "Epoch: 2 , loss: 20.69632339477539\n",
      "Epoch: 3 , loss: 18.977214813232422\n",
      "Epoch: 4 , loss: 18.393728256225586\n",
      "Epoch: 5 , loss: 20.60384750366211\n",
      "Epoch: 6 , loss: 20.73811912536621\n",
      "Epoch: 7 , loss: 18.747814178466797\n",
      "Epoch: 8 , loss: 19.675630569458008\n",
      "Epoch: 9 , loss: 17.467988967895508\n",
      "Epoch: 10 , loss: 19.293947219848633\n",
      "Epoch: 11 , loss: 20.590545654296875\n",
      "Epoch: 12 , loss: 17.471939086914062\n",
      "Epoch: 13 , loss: 20.073326110839844\n",
      "Epoch: 14 , loss: 20.89478302001953\n",
      "Epoch: 15 , loss: 18.29037094116211\n",
      "Epoch: 16 , loss: 19.921588897705078\n",
      "Epoch: 17 , loss: 19.110151290893555\n",
      "Epoch: 18 , loss: 19.066476821899414\n",
      "Epoch: 19 , loss: 19.35572052001953\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:32:53,515] Trial 1 finished with value: 255653.69071206026 and parameters: {'num_layers': 2, 'layer_units_0': 512, 'layer_units_1': 512, 'vae': False, 'samples': 8}. Best is trial 1 with value: 255653.69071206026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "WARNING:tensorflow:From c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\MIDASpy\\midas_base.py:511: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 22.593008041381836\n",
      "Epoch: 1 , loss: 22.979093551635742\n",
      "Epoch: 2 , loss: 24.530229568481445\n",
      "Epoch: 3 , loss: 22.533607482910156\n",
      "Epoch: 4 , loss: 22.201587677001953\n",
      "Epoch: 5 , loss: 24.265193939208984\n",
      "Epoch: 6 , loss: 24.616653442382812\n",
      "Epoch: 7 , loss: 22.463951110839844\n",
      "Epoch: 8 , loss: 23.315757751464844\n",
      "Epoch: 9 , loss: 21.314189910888672\n",
      "Epoch: 10 , loss: 23.237518310546875\n",
      "Epoch: 11 , loss: 24.26202392578125\n",
      "Epoch: 12 , loss: 21.052888870239258\n",
      "Epoch: 13 , loss: 23.54488754272461\n",
      "Epoch: 14 , loss: 24.347368240356445\n",
      "Epoch: 15 , loss: 21.891460418701172\n",
      "Epoch: 16 , loss: 23.469860076904297\n",
      "Epoch: 17 , loss: 22.564062118530273\n",
      "Epoch: 18 , loss: 22.542987823486328\n",
      "Epoch: 19 , loss: 22.77960968017578\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:33:03,929] Trial 2 finished with value: 259291.6758534335 and parameters: {'num_layers': 1, 'layer_units_0': 256, 'vae': True, 'samples': 18}. Best is trial 1 with value: 255653.69071206026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 22.69588851928711\n",
      "Epoch: 1 , loss: 23.068042755126953\n",
      "Epoch: 2 , loss: 24.239410400390625\n",
      "Epoch: 3 , loss: 22.387981414794922\n",
      "Epoch: 4 , loss: 21.972150802612305\n",
      "Epoch: 5 , loss: 24.4119873046875\n",
      "Epoch: 6 , loss: 24.571474075317383\n",
      "Epoch: 7 , loss: 22.54391098022461\n",
      "Epoch: 8 , loss: 23.440031051635742\n",
      "Epoch: 9 , loss: 21.296707153320312\n",
      "Epoch: 10 , loss: 23.229040145874023\n",
      "Epoch: 11 , loss: 24.579153060913086\n",
      "Epoch: 12 , loss: 21.24481201171875\n",
      "Epoch: 13 , loss: 23.89200210571289\n",
      "Epoch: 14 , loss: 24.797134399414062\n",
      "Epoch: 15 , loss: 22.116498947143555\n",
      "Epoch: 16 , loss: 23.75806427001953\n",
      "Epoch: 17 , loss: 22.84708023071289\n",
      "Epoch: 18 , loss: 22.78594398498535\n",
      "Epoch: 19 , loss: 23.038122177124023\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:33:14,480] Trial 3 finished with value: 256213.35992938254 and parameters: {'num_layers': 3, 'layer_units_0': 128, 'layer_units_1': 64, 'layer_units_2': 256, 'vae': True, 'samples': 7}. Best is trial 1 with value: 255653.69071206026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.81810188293457\n",
      "Epoch: 1 , loss: 20.1691837310791\n",
      "Epoch: 2 , loss: 21.105831146240234\n",
      "Epoch: 3 , loss: 19.573457717895508\n",
      "Epoch: 4 , loss: 19.259733200073242\n",
      "Epoch: 5 , loss: 21.01310157775879\n",
      "Epoch: 6 , loss: 21.477588653564453\n",
      "Epoch: 7 , loss: 19.25461769104004\n",
      "Epoch: 8 , loss: 20.258249282836914\n",
      "Epoch: 9 , loss: 17.95290184020996\n",
      "Epoch: 10 , loss: 19.82832145690918\n",
      "Epoch: 11 , loss: 21.261781692504883\n",
      "Epoch: 12 , loss: 17.95758819580078\n",
      "Epoch: 13 , loss: 20.56989288330078\n",
      "Epoch: 14 , loss: 21.481164932250977\n",
      "Epoch: 15 , loss: 18.878849029541016\n",
      "Epoch: 16 , loss: 20.45262908935547\n",
      "Epoch: 17 , loss: 19.591800689697266\n",
      "Epoch: 18 , loss: 19.503931045532227\n",
      "Epoch: 19 , loss: 19.8103084564209\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:33:24,056] Trial 4 finished with value: 256270.71707915416 and parameters: {'num_layers': 2, 'layer_units_0': 128, 'layer_units_1': 64, 'vae': False, 'samples': 13}. Best is trial 1 with value: 255653.69071206026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 21.50151824951172\n",
      "Epoch: 1 , loss: 22.531946182250977\n",
      "Epoch: 2 , loss: 23.03473663330078\n",
      "Epoch: 3 , loss: 21.33109474182129\n",
      "Epoch: 4 , loss: 20.007078170776367\n",
      "Epoch: 5 , loss: 22.88729476928711\n",
      "Epoch: 6 , loss: 22.365447998046875\n",
      "Epoch: 7 , loss: 20.329326629638672\n",
      "Epoch: 8 , loss: 21.146982192993164\n",
      "Epoch: 9 , loss: 18.408266067504883\n",
      "Epoch: 10 , loss: 20.32744789123535\n",
      "Epoch: 11 , loss: 21.94100570678711\n",
      "Epoch: 12 , loss: 18.357044219970703\n",
      "Epoch: 13 , loss: 20.59676742553711\n",
      "Epoch: 14 , loss: 21.83759307861328\n",
      "Epoch: 15 , loss: 18.915298461914062\n",
      "Epoch: 16 , loss: 20.631980895996094\n",
      "Epoch: 17 , loss: 19.7132568359375\n",
      "Epoch: 18 , loss: 19.3873348236084\n",
      "Epoch: 19 , loss: 19.576072692871094\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 11:33:33,754] Trial 5 finished with value: 256589.3033855924 and parameters: {'num_layers': 1, 'layer_units_0': 64, 'vae': False, 'samples': 15}. Best is trial 1 with value: 255653.69071206026.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  num_layers: 2\n",
      "  layer_units_0: 512\n",
      "  layer_units_1: 512\n",
      "  vae: False\n",
      "  samples: 8\n",
      "Best Objective Value (aggregated error): 255653.69071206026\n",
      "Best hyperparameters for MIDAS: {'num_layers': 2, 'layer_units_0': 512, 'layer_units_1': 512, 'vae': False, 'samples': 8} with best agg error of 255653.69071206026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:04<00:00,  1.16it/s]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 5/5 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to impute with MIDAS: 'layer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "100%|| 5/5 [00:52<00:00, 10.40s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 5/5 [00:01<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to impute with MIDAS during build: 'layer'\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Data Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Best Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Metric",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Error_SD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Max_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Min_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Within_10pct",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "140f01b4-0189-40c2-93b3-c7755c71346b",
       "rows": [
        [
         "0",
         "Dm2",
         "Categorical",
         "KNN",
         "0.5",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Dm4",
         "Discrete",
         "KNN",
         "12.0",
         "7.0710678118654755",
         "17.0",
         "7.0",
         "0.0"
        ],
        [
         "2",
         "E11",
         "Discrete",
         "MissForest",
         "5.333333333333333",
         "2.8867513459481287",
         "7.0",
         "2.0",
         "1.0"
        ],
        [
         "3",
         "E12",
         "Discrete",
         "KNN",
         "12.0",
         "11.313708498984761",
         "20.0",
         "4.0",
         "0.5"
        ],
        [
         "4",
         "E21",
         "Discrete",
         "KNN",
         "1.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "5",
         "E22",
         "Discrete",
         "KNN",
         "1.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0"
        ],
        [
         "6",
         "E31",
         "Discrete",
         "KNN",
         "6.5",
         "3.5355339059327378",
         "9.0",
         "4.0",
         "1.0"
        ],
        [
         "7",
         "E32",
         "Discrete",
         "KNN",
         "10.5",
         "0.7071067811865476",
         "11.0",
         "10.0",
         "1.0"
        ],
        [
         "8",
         "E41",
         "Discrete",
         "KNN",
         "11.0",
         "5.196152422706632",
         "14.0",
         "5.0",
         "0.3333333333333333"
        ],
        [
         "9",
         "E42",
         "Discrete",
         "KNN",
         "9.5",
         "0.7071067811865476",
         "10.0",
         "9.0",
         "0.0"
        ],
        [
         "10",
         "E23",
         "Continuous",
         "KNN",
         "12.25",
         "6.914274569080641",
         "17.139130434782615",
         "7.360869565217385",
         "0.0"
        ],
        [
         "11",
         "E24",
         "Continuous",
         "KNN",
         "13.75",
         "0.9069413063044937",
         "14.391304347826093",
         "13.108695652173907",
         "1.0"
        ],
        [
         "12",
         "E25",
         "Discrete",
         "KNN",
         "6.666666666666667",
         "6.6583281184793925",
         "14.0",
         "1.0",
         "0.6666666666666666"
        ],
        [
         "13",
         "E26",
         "Discrete",
         "MissForest",
         "1.3333333333333333",
         "0.5773502691896257",
         "2.0",
         "1.0",
         "1.0"
        ],
        [
         "14",
         "E27",
         "Discrete",
         "KNN",
         "1.5",
         "0.7071067811865476",
         "2.0",
         "1.0",
         "0.5"
        ],
        [
         "15",
         "FastingBloodSugar",
         "Discrete",
         "KNN",
         "16.5",
         "10.606601717798213",
         "24.0",
         "9.0",
         "0.0"
        ],
        [
         "16",
         "Glucose2hpp",
         "Discrete",
         "KNN",
         "34.0",
         "28.0",
         "66.0",
         "14.0",
         "0.0"
        ],
        [
         "17",
         "Cholestrol",
         "Discrete",
         "KNN",
         "10.5",
         "0.7071067811865476",
         "11.0",
         "10.0",
         "1.0"
        ],
        [
         "18",
         "Triglycerides",
         "Discrete",
         "MissForest",
         "27.333333333333332",
         "17.21433511156714",
         "41.0",
         "8.0",
         "0.3333333333333333"
        ],
        [
         "19",
         "HDL",
         "Discrete",
         "MissForest",
         "4.333333333333333",
         "3.7859388972001824",
         "7.0",
         "0.0",
         "0.3333333333333333"
        ],
        [
         "20",
         "LDL",
         "Discrete",
         "KNN",
         "3.3333333333333335",
         "3.0550504633038935",
         "6.0",
         "0.0",
         "1.0"
        ],
        [
         "21",
         "Hb.A1C",
         "Continuous",
         "KNN",
         "0.5549999999999997",
         "0.313893923300636",
         "0.7769565217391294",
         "0.33304347826087",
         "0.5"
        ],
        [
         "22",
         "CreatininUrine",
         "Continuous",
         "KNN",
         "76.9254797979798",
         "78.74579658554303",
         "163.80863636363637",
         "10.259166666666658",
         "0.3333333333333333"
        ],
        [
         "23",
         "PotassiumUrineRandom",
         "Continuous",
         "KNN",
         "18.43826086956523",
         "23.419376592898452",
         "34.99826086956523",
         "1.8782608695652314",
         "0.5"
        ],
        [
         "24",
         "SodiumUrineRandom",
         "Continuous",
         "KNN",
         "42.038888888888884",
         "7.734589574115256",
         "46.845454545454544",
         "33.116666666666646",
         "0.0"
        ],
        [
         "25",
         "W.B.C",
         "Discrete",
         "KNN",
         "1800.0",
         "141.4213562373095",
         "1900.0",
         "1700.0",
         "0.0"
        ],
        [
         "26",
         "R.B.C",
         "Continuous",
         "MissForest",
         "0.35666753133138024",
         "0.21220846847146108",
         "0.5299974060058599",
         "0.12000259399414048",
         "1.0"
        ],
        [
         "27",
         "Hemoglobin",
         "Continuous",
         "KNN",
         "1.0999999999999996",
         "0.6640654988534532",
         "1.569565217391304",
         "0.6304347826086953",
         "0.5"
        ],
        [
         "28",
         "Hematocrit",
         "Continuous",
         "MissForest",
         "2.8712049357096348",
         "2.539499561533081",
         "5.113614807128904",
         "0.11361480712890426",
         "0.6666666666666666"
        ],
        [
         "29",
         "MCV",
         "Continuous",
         "KNN",
         "1.077272727272728",
         "0.8329103609761813",
         "1.9499999999999886",
         "0.29090909090909634",
         "1.0"
        ],
        [
         "30",
         "MCH",
         "Continuous",
         "MissForest",
         "1.2021729278564468",
         "0.7778174593052007",
         "1.7521729278564457",
         "0.6521729278564479",
         "1.0"
        ],
        [
         "31",
         "MCHC",
         "Continuous",
         "KNN",
         "1.6565217391304294",
         "1.1313708498984771",
         "2.45652173913043",
         "0.8565217391304287",
         "1.0"
        ],
        [
         "32",
         "Neutrophils",
         "Continuous",
         "MissForest",
         "1.9545617167154934",
         "1.7815841803242",
         "3.8636851501464804",
         "0.33631484985351534",
         "1.0"
        ],
        [
         "33",
         "Lymphocyte",
         "Continuous",
         "KNN",
         "2.2999999999999994",
         "1.4470474356398053",
         "3.8590909090909093",
         "1.0",
         "0.6666666666666666"
        ],
        [
         "34",
         "Mixed",
         "Continuous",
         "KNN",
         "1.9500000000000002",
         "2.0690559292980275",
         "3.413043478260869",
         "0.4869565217391312",
         "0.5"
        ],
        [
         "35",
         "Platelets",
         "Discrete",
         "KNN",
         "55.0",
         "2.8284271247461903",
         "57.0",
         "53.0",
         "0.0"
        ],
        [
         "36",
         "DBP",
         "Continuous",
         "KNN",
         "9.0",
         "2.8284271247461903",
         "11.0",
         "7.0",
         "0.5"
        ],
        [
         "37",
         "SBP",
         "Continuous",
         "KNN",
         "8.434782608695656",
         "2.1213203435596424",
         "9.934782608695656",
         "6.934782608695656",
         "1.0"
        ],
        [
         "38",
         "gdi",
         "Discrete",
         "KNN",
         "0.6666666666666666",
         "0.5773502691896258",
         "1.0",
         "0.0",
         "0.3333333333333333"
        ],
        [
         "39",
         "work_activity",
         "Discrete",
         "KNN",
         "3585366.0",
         "221966.47546870675",
         "3742320.0",
         "3428412.0",
         "0.0"
        ],
        [
         "40",
         "transport",
         "Discrete",
         "KNN",
         "355328.3333333333",
         "137526.31603199925",
         "435029.0",
         "196527.0",
         "0.0"
        ],
        [
         "41",
         "lesiretime",
         "Discrete",
         "KNN",
         "2390527.0",
         "0.0",
         "2390527.0",
         "2390527.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 42
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Best Method</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Error_SD</th>\n",
       "      <th>Max_Error</th>\n",
       "      <th>Min_Error</th>\n",
       "      <th>Within_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dm2</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>KNN</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dm4</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>7.071068</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E11</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>5.333333e+00</td>\n",
       "      <td>2.886751</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E12</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>11.313708</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E21</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E22</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E31</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.500000e+00</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E32</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.050000e+01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E41</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>5.196152</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E42</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>9.500000e+00</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E23</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.225000e+01</td>\n",
       "      <td>6.914275</td>\n",
       "      <td>1.713913e+01</td>\n",
       "      <td>7.360870e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E24</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.375000e+01</td>\n",
       "      <td>0.906941</td>\n",
       "      <td>1.439130e+01</td>\n",
       "      <td>1.310870e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E25</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.666667e+00</td>\n",
       "      <td>6.658328</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E26</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.333333e+00</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E27</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.500000e+00</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FastingBloodSugar</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.650000e+01</td>\n",
       "      <td>10.606602</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Glucose2hpp</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.600000e+01</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cholestrol</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.050000e+01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Triglycerides</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>2.733333e+01</td>\n",
       "      <td>17.214335</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>4.333333e+00</td>\n",
       "      <td>3.785939</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>3.333333e+00</td>\n",
       "      <td>3.055050</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hb.A1C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>5.550000e-01</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>7.769565e-01</td>\n",
       "      <td>3.330435e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CreatininUrine</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.692548e+01</td>\n",
       "      <td>78.745797</td>\n",
       "      <td>1.638086e+02</td>\n",
       "      <td>1.025917e+01</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PotassiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.843826e+01</td>\n",
       "      <td>23.419377</td>\n",
       "      <td>3.499826e+01</td>\n",
       "      <td>1.878261e+00</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SodiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.203889e+01</td>\n",
       "      <td>7.734590</td>\n",
       "      <td>4.684545e+01</td>\n",
       "      <td>3.311667e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>W.B.C</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.800000e+03</td>\n",
       "      <td>141.421356</td>\n",
       "      <td>1.900000e+03</td>\n",
       "      <td>1.700000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>R.B.C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>3.566675e-01</td>\n",
       "      <td>0.212208</td>\n",
       "      <td>5.299974e-01</td>\n",
       "      <td>1.200026e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hemoglobin</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.100000e+00</td>\n",
       "      <td>0.664065</td>\n",
       "      <td>1.569565e+00</td>\n",
       "      <td>6.304348e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>2.871205e+00</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>5.113615e+00</td>\n",
       "      <td>1.136148e-01</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MCV</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.077273e+00</td>\n",
       "      <td>0.832910</td>\n",
       "      <td>1.950000e+00</td>\n",
       "      <td>2.909091e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MCH</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.202173e+00</td>\n",
       "      <td>0.777817</td>\n",
       "      <td>1.752173e+00</td>\n",
       "      <td>6.521729e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MCHC</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.656522e+00</td>\n",
       "      <td>1.131371</td>\n",
       "      <td>2.456522e+00</td>\n",
       "      <td>8.565217e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Neutrophils</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.954562e+00</td>\n",
       "      <td>1.781584</td>\n",
       "      <td>3.863685e+00</td>\n",
       "      <td>3.363148e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lymphocyte</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>2.300000e+00</td>\n",
       "      <td>1.447047</td>\n",
       "      <td>3.859091e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mixed</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.950000e+00</td>\n",
       "      <td>2.069056</td>\n",
       "      <td>3.413043e+00</td>\n",
       "      <td>4.869565e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>5.500000e+01</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>DBP</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SBP</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>8.434783e+00</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>9.934783e+00</td>\n",
       "      <td>6.934783e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gdi</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.666667e-01</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>work_activity</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>3.585366e+06</td>\n",
       "      <td>221966.475469</td>\n",
       "      <td>3.742320e+06</td>\n",
       "      <td>3.428412e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>transport</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>3.553283e+05</td>\n",
       "      <td>137526.316032</td>\n",
       "      <td>4.350290e+05</td>\n",
       "      <td>1.965270e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lesiretime</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>2.390527e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.390527e+06</td>\n",
       "      <td>2.390527e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column    Data Type  ...     Min_Error  Within_10pct\n",
       "0                    Dm2  Categorical  ...           NaN           NaN\n",
       "1                    Dm4     Discrete  ...  7.000000e+00      0.000000\n",
       "2                    E11     Discrete  ...  2.000000e+00      1.000000\n",
       "3                    E12     Discrete  ...  4.000000e+00      0.500000\n",
       "4                    E21     Discrete  ...  1.000000e+00      1.000000\n",
       "5                    E22     Discrete  ...  1.000000e+00      1.000000\n",
       "6                    E31     Discrete  ...  4.000000e+00      1.000000\n",
       "7                    E32     Discrete  ...  1.000000e+01      1.000000\n",
       "8                    E41     Discrete  ...  5.000000e+00      0.333333\n",
       "9                    E42     Discrete  ...  9.000000e+00      0.000000\n",
       "10                   E23   Continuous  ...  7.360870e+00      0.000000\n",
       "11                   E24   Continuous  ...  1.310870e+01      1.000000\n",
       "12                   E25     Discrete  ...  1.000000e+00      0.666667\n",
       "13                   E26     Discrete  ...  1.000000e+00      1.000000\n",
       "14                   E27     Discrete  ...  1.000000e+00      0.500000\n",
       "15     FastingBloodSugar     Discrete  ...  9.000000e+00      0.000000\n",
       "16           Glucose2hpp     Discrete  ...  1.400000e+01      0.000000\n",
       "17            Cholestrol     Discrete  ...  1.000000e+01      1.000000\n",
       "18         Triglycerides     Discrete  ...  8.000000e+00      0.333333\n",
       "19                   HDL     Discrete  ...  0.000000e+00      0.333333\n",
       "20                   LDL     Discrete  ...  0.000000e+00      1.000000\n",
       "21                Hb.A1C   Continuous  ...  3.330435e-01      0.500000\n",
       "22        CreatininUrine   Continuous  ...  1.025917e+01      0.333333\n",
       "23  PotassiumUrineRandom   Continuous  ...  1.878261e+00      0.500000\n",
       "24     SodiumUrineRandom   Continuous  ...  3.311667e+01      0.000000\n",
       "25                 W.B.C     Discrete  ...  1.700000e+03      0.000000\n",
       "26                 R.B.C   Continuous  ...  1.200026e-01      1.000000\n",
       "27            Hemoglobin   Continuous  ...  6.304348e-01      0.500000\n",
       "28            Hematocrit   Continuous  ...  1.136148e-01      0.666667\n",
       "29                   MCV   Continuous  ...  2.909091e-01      1.000000\n",
       "30                   MCH   Continuous  ...  6.521729e-01      1.000000\n",
       "31                  MCHC   Continuous  ...  8.565217e-01      1.000000\n",
       "32           Neutrophils   Continuous  ...  3.363148e-01      1.000000\n",
       "33            Lymphocyte   Continuous  ...  1.000000e+00      0.666667\n",
       "34                 Mixed   Continuous  ...  4.869565e-01      0.500000\n",
       "35             Platelets     Discrete  ...  5.300000e+01      0.000000\n",
       "36                   DBP   Continuous  ...  7.000000e+00      0.500000\n",
       "37                   SBP   Continuous  ...  6.934783e+00      1.000000\n",
       "38                   gdi     Discrete  ...  0.000000e+00      0.333333\n",
       "39         work_activity     Discrete  ...  3.428412e+06      0.000000\n",
       "40             transport     Discrete  ...  1.965270e+05      0.000000\n",
       "41            lesiretime     Discrete  ...  2.390527e+06      0.000000\n",
       "\n",
       "[42 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from impute import run_full_pipeline, create_missings\n",
    "\n",
    "\n",
    "new_df = pd.read_excel(r\"C:\\Users\\Matin\\Downloads\\Data for Dr.Matin.xlsx\", 's1')\n",
    "\n",
    "\n",
    "new_df.drop(['n', 'ID','Gen.code'],axis=1,inplace=True)\n",
    "_, new_df2, _ = create_missings(new_df, missingness=10) \n",
    "\n",
    "\n",
    "imputed_df, summary = run_full_pipeline(new_df2, simulate=True, timelimit=60, random_seed=96, build=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.to_excel('test.xlsx', index=False)\n",
    "summary.to_excel('summary.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

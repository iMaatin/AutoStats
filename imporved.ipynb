{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import miceforest as mf\n",
    "from missforest import MissForest\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import MIDASpy as md\n",
    "\n",
    "### Data Preparation Function\n",
    "\n",
    "# def prep(df: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Preprocess the DataFrame by:\n",
    "#     - Dropping rows with missing values and resetting the index.\n",
    "#     - Converting object columns to categorical via LabelEncoder.\n",
    "#     - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "#     - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "#       it is considered categorical and encoded.\n",
    "\n",
    "#     Returns:\n",
    "#         categorical_cols (list): List of columns encoded as categorical.\n",
    "#         discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "#         cont_cols (list): List of remaining continuous numeric columns.\n",
    "#         df_clean (DataFrame): The preprocessed DataFrame.\n",
    "#         encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "#     \"\"\"\n",
    "#     df_clean = df.dropna().reset_index(drop=True)\n",
    "#     categorical_cols = []\n",
    "#     discrete_cols = []\n",
    "#     encoders = {}\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if df_clean[col].dtype == 'object':\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "#         else:\n",
    "#             try:\n",
    "#                 df_clean[col] = df_clean[col].astype(float)\n",
    "#                 if (np.isclose(df_clean[col] % 1, 0).mean() > 0.5):\n",
    "#                     df_clean[col] = df_clean[col].astype(int)\n",
    "#                     discrete_cols.append(col)\n",
    "#             except (ValueError, TypeError):\n",
    "#                 categorical_cols.append(col)\n",
    "#                 le = LabelEncoder()\n",
    "#                 df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#                 encoders[col] = le\n",
    "\n",
    "#     for col in df_clean.columns:\n",
    "#         if col not in categorical_cols and df_clean[col].nunique() == 2:\n",
    "#             categorical_cols.append(col)\n",
    "#             le = LabelEncoder()\n",
    "#             df_clean[col] = le.fit_transform(df_clean[col])\n",
    "#             encoders[col] = le\n",
    "\n",
    "#     continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "#     return continuous_cols, discrete_cols, categorical_cols, df_clean, encoders\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "    encoders = {}\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "            le = LabelEncoder()\n",
    "            df_clean[col] = le.fit_transform(df_clean[col])\n",
    "            encoders[col] = le\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                le = LabelEncoder()\n",
    "                df_clean[col] = le.fit_transform(df_clean[col])\n",
    "                encoders[col] = le\n",
    "def prep(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by:\n",
    "    - Dropping rows with missing values and resetting the index.\n",
    "    - Converting object columns to categorical via LabelEncoder.\n",
    "    - Converting other columns to float (and then to int if >50% of values are integer-like).\n",
    "    - If any numeric column (not already marked as categorical) has only 2 unique values,\n",
    "      it is considered categorical and encoded.\n",
    "\n",
    "    Returns:\n",
    "        continuous_cols (list): List of remaining continuous numeric columns.\n",
    "        discrete_cols (list): List of columns that are numeric and integer-like.\n",
    "        categorical_cols (list): List of columns encoded as categorical.\n",
    "        df_clean (DataFrame): The preprocessed DataFrame.\n",
    "        encoders (dict): Mapping from categorical column name to its LabelEncoder.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values.\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "    categorical_cols = []\n",
    "    discrete_cols = []\n",
    "\n",
    "    # Loop over each column to check its type and convert accordingly.\n",
    "    for col in df_clean.columns:\n",
    "        # If the column type is object, encode it as a categorical variable.\n",
    "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() == 2:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            try:\n",
    "                # Convert column to float first.\n",
    "                df_clean[col] = df_clean[col].astype(float)\n",
    "                # Check if most of the values are integer-like using np.isclose.\n",
    "                # This computes the proportion of values where the modulus with 1 is nearly 0.\n",
    "                if (np.isclose(df_clean[col] % 1, 0)).mean() > 0.5:\n",
    "                    df_clean[col] = df_clean[col].astype(int)\n",
    "                    discrete_cols.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                # If conversion to float fails, treat the column as categorical.\n",
    "                categorical_cols.append(col)\n",
    "                \n",
    "\n",
    "    # Determine continuous columns as those not flagged as categorical or discrete.\n",
    "    continuous_cols = [col for col in df_clean.columns if col not in categorical_cols + discrete_cols]\n",
    "\n",
    "    return continuous_cols, discrete_cols, categorical_cols\n",
    "\n",
    "def reverse_encoding(df: pd.DataFrame, encoders: dict):\n",
    "    \"\"\"\n",
    "    Reverse the LabelEncoder transformation on categorical columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with encoded categorical columns.\n",
    "        encoders (dict): Dictionary mapping column names to their LabelEncoder.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the categorical columns decoded to their original labels.\n",
    "    \"\"\"\n",
    "    df_decoded = df.copy()\n",
    "    for col, le in encoders.items():\n",
    "        df_decoded[col] = le.inverse_transform(df_decoded[col].astype(int))\n",
    "    return df_decoded\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame, DataFrame with missing values, and a mask DataFrame.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "def simulate_missingness(df, show_missingness=False):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=42).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         n_neighbors (int): Number of neighbors for KNN.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     # Optionally scale all numeric columns\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "#     # Apply KNN imputation to the entire dataframe\n",
    "#     imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "#     df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "#     # Reverse scale if needed\n",
    "#     if scale:\n",
    "#         df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "#     # Post-process: round discrete and categorical values\n",
    "#     if discrete_cols:\n",
    "#         df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_imputed[categorical_cols] = np.round(df_imputed[categorical_cols]).astype(int)\n",
    "\n",
    "#     return df_imputed\n",
    "\n",
    "def do_knn(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using KNN imputation over all columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        n_neighbors (int): Number of neighbors for KNN.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                # All values missing in this column\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Optionally scale numeric columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[df.columns] = scaler.fit_transform(df_imputed)\n",
    "\n",
    "    # Apply KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed[df.columns] = imputer.fit_transform(df_imputed)\n",
    "\n",
    "    # Reverse scale\n",
    "    if scale:\n",
    "        df_imputed[df.columns] = scaler.inverse_transform(df_imputed)\n",
    "\n",
    "    # Round discrete and categorical values\n",
    "    if discrete_cols:\n",
    "        df_imputed[discrete_cols] = np.round(df_imputed[discrete_cols]).astype(int)\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed[col] = np.round(df_imputed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed[col] = df_imputed[col].map(inv_map)\n",
    "\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_imputed = do_knn(df2, continuous_cols=None, discrete_cols=None, categorical_cols=None, n_neighbors=5, scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "#             iters=10, strat='normal', scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): Input DataFrame with missing values.\n",
    "#         continuous_cols (list of str): Names of continuous numeric columns.\n",
    "#         discrete_cols (list of str): Names of discrete numeric columns.\n",
    "#         categorical_cols (list of str): Names of categorical columns.\n",
    "#         iters (int): Number of MICE iterations.\n",
    "#         strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "#     kernel = mf.ImputationKernel(\n",
    "#         df_imputed,\n",
    "#         random_state=0,\n",
    "#         mean_match_strategy=strat,\n",
    "#         variable_schema=None,  # Explicitly set variable_schema to None \n",
    "#         )\n",
    "\n",
    "#     kernel.mice(iterations=iters, verbose=False)  # Disable verbose output\n",
    "#     df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "#     if discrete_cols:\n",
    "#         df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "#     if categorical_cols:\n",
    "#         df_completed[categorical_cols] = df_completed[categorical_cols].round().astype(int)\n",
    "\n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "#     return df_completed\n",
    "\n",
    "def do_mice(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "            iters=10, strat='normal', scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using the MICE forest method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with missing values.\n",
    "        continuous_cols (list of str): Names of continuous numeric columns.\n",
    "        discrete_cols (list of str): Names of discrete numeric columns.\n",
    "        categorical_cols (list of str): Names of categorical columns.\n",
    "        iters (int): Number of MICE iterations.\n",
    "        strat: ['normal', 'shap', 'fast'] or a dictionary specifying the mean matching strategy.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns if requested\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Run MICE imputation\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df_imputed,\n",
    "        random_state=0,\n",
    "        mean_match_strategy=strat,\n",
    "        variable_schema=None\n",
    "    )\n",
    "\n",
    "    kernel.mice(iterations=iters, verbose=False)\n",
    "    df_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "    # Post-process discrete and categorical columns\n",
    "    if discrete_cols:\n",
    "        df_completed[discrete_cols] = df_completed[discrete_cols].round().astype(int)\n",
    "\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_completed[col] = np.round(df_completed[col]).astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_completed[col] = df_completed[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_completed[continuous_cols] = scaler.inverse_transform(df_completed[continuous_cols])\n",
    "\n",
    "    return df_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mice_imputed = do_mice(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols,\n",
    "                    #    iters=10, strat='normal', scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "#     \"\"\"\n",
    "#     Impute missing values using MissForest.\n",
    "    \n",
    "#     Parameters:\n",
    "#         df (pd.DataFrame): DataFrame with missing values.\n",
    "#         continuous_cols (list): Names of continuous numeric columns.\n",
    "#         discrete_cols (list): Names of discrete numeric columns.\n",
    "#         categorical_cols (list): Names of categorical columns.\n",
    "#         iters (int): Maximum number of iterations.\n",
    "#         scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Imputed DataFrame.\n",
    "#     \"\"\"\n",
    "#     df_imputed = df.copy()\n",
    "    \n",
    "#     if scale:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "    \n",
    "#     imputer = MissForest(max_iter=iters, categorical=categorical_cols)\n",
    "#     df_imputed_result = imputer.fit_transform(df_imputed)\n",
    "    \n",
    "#     if discrete_cols:\n",
    "#         df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "    \n",
    "#     if categorical_cols:\n",
    "#         df_imputed_result[categorical_cols] = df_imputed_result[categorical_cols].round().astype(int)\n",
    "    \n",
    "#     if scale:\n",
    "#         # Reverse scaling for continuous columns\n",
    "#         df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "    \n",
    "#     return df_imputed_result\n",
    "\n",
    "# # mf_imputed = do_mf(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=5, scale=False)\n",
    "\n",
    "def do_mf(df, continuous_cols=None, discrete_cols=None, categorical_cols=None, iters=5, scale=False):\n",
    "    \"\"\"\n",
    "    Impute missing values using MissForest.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with missing values.\n",
    "        continuous_cols (list): Names of continuous numeric columns.\n",
    "        discrete_cols (list): Names of discrete numeric columns.\n",
    "        categorical_cols (list): Names of categorical columns.\n",
    "        iters (int): Maximum number of iterations.\n",
    "        scale (bool): Whether to apply MinMaxScaler before imputation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Imputed DataFrame.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Encode categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null = df_imputed[col].dropna()\n",
    "            if not not_null.empty:\n",
    "                le.fit(not_null)\n",
    "                df_imputed[col] = df_imputed[col].map(lambda x: le.transform([x])[0] if pd.notnull(x) else np.nan)\n",
    "                encoders[col] = le\n",
    "            else:\n",
    "                encoders[col] = None\n",
    "\n",
    "    # Scale continuous columns\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        df_imputed[continuous_cols] = scaler.fit_transform(df_imputed[continuous_cols])\n",
    "\n",
    "    # Impute with MissForest\n",
    "    imputer = MissForest(max_iter=iters)\n",
    "    df_imputed_result = pd.DataFrame(imputer.fit_transform(df_imputed), columns=df.columns)\n",
    "\n",
    "    # Post-process discrete columns\n",
    "    if discrete_cols:\n",
    "        df_imputed_result[discrete_cols] = df_imputed_result[discrete_cols].round().astype(int)\n",
    "\n",
    "    # Post-process categorical columns\n",
    "    if categorical_cols:\n",
    "        for col in categorical_cols:\n",
    "            df_imputed_result[col] = df_imputed_result[col].round().astype(int)\n",
    "            if encoders[col] is not None:\n",
    "                inv_map = dict(enumerate(encoders[col].classes_))\n",
    "                df_imputed_result[col] = df_imputed_result[col].map(inv_map)\n",
    "\n",
    "    # Reverse scaling\n",
    "    if scale:\n",
    "        df_imputed_result[continuous_cols] = scaler.inverse_transform(df_imputed_result[continuous_cols])\n",
    "\n",
    "    return df_imputed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df, continuous_cols=None, discrete_cols=None, categorical_cols=None,\n",
    "              layer:list=[256,256], vae:bool=True, samples:int=10, random_seed:float=96 ):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but non-continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      \n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes.\n",
    "    \"\"\"\n",
    "    # 1. Convert categorical columns and get categorical metadata.\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "    \n",
    "    # 2. Define the numeric columns.\n",
    "    num_cols = discrete_cols + continuous_cols  # these are the numeric columns\n",
    "\n",
    "    # 3. Drop original categorical columns and combine with the converted categorical data.\n",
    "    df_copy = df.drop(columns=categorical_cols,axis=1)\n",
    "    constructor_list = [df_copy, md_cat_data]\n",
    "    data_in = pd.concat(constructor_list, axis=1)\n",
    "    \n",
    "    # 4. Scale non-categorical columns BEFORE imputation.\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "    \n",
    "    # 5. Build and train the imputer using the scaled data.\n",
    "    imputer = md.Midas(layer_structure=layer, vae_layer=vae, seed=random_seed, input_drop=0.75)\n",
    "    # Use md_cats as softmax columns for categorical outputs.\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "    \n",
    "    # 6. Generate imputations.\n",
    "    imps = imputer.generate_samples(m=samples).output_list\n",
    "    \n",
    "    # 7. Post-process each imputed DataFrame.\n",
    "    for idx, imp_df in enumerate(imps):\n",
    "        # Reverse transform the numeric columns.\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "        \n",
    "        # Process categorical columns.\n",
    "        # For each softmax group in md_cats, choose the column with the highest probability.\n",
    "        tmp_cat = []\n",
    "        for group in md_cats:\n",
    "            # idxmax returns the column name with maximum value per row for this group.\n",
    "            tmp_cat.append(imp_df[group].idxmax(axis=1))\n",
    "        # Assume the order of md_cats corresponds to categorical_cols.\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        \n",
    "        # Drop the softmax columns.\n",
    "        flat_cats = [col for group in md_cats for col in group]\n",
    "        tmp_cat = [imp_df[x].idxmax(axis=1) for x in md_cats]\n",
    "        cat_df = pd.DataFrame({categorical_cols[j]: tmp_cat[j] for j in range(len(categorical_cols))})\n",
    "        imp_df = pd.concat([imp_df, cat_df], axis=1).drop(columns=flat_cats, axis=1)\n",
    "        \n",
    "        # Handle discrete data by rounding the values.\n",
    "        imp_df[discrete_cols] = imp_df[discrete_cols].round()\n",
    "        \n",
    "        # Replace the processed DataFrame in the list.\n",
    "        imps[idx] = imp_df\n",
    "\n",
    "        ### make method info\n",
    "        method_info = f'MIDAS, params: samples={samples} ,layer={layer}, vae={vae}'\n",
    "    return imps, method_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midas_imputed = do_midas(df2, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Missingness Creation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Improved Evaluation Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameter Optimization Function using Optuna\n",
    "# ------------------------------------------------------------------------------\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    missing_percent, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    "            params['layer'] = trial.suggest_categorical(\"layer\", [[256,256], [128,128], [512,256]])\n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, summary_table, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=df,missing_percent=20,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_midas,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mice,original_df=df,missing_percent=20,timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_excel(r\"C:\\Users\\Matin\\Downloads\\Data for Dr.Matin.xlsx\", 's1')\n",
    "new_df.drop(['n', 'ID','Gen.code'],axis=1,inplace=True)\n",
    "new_df = new_df[:300]\n",
    "# continuous_cols, discrete_cols, categorical_cols, df2, encoders = prep(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_knn,original_df=df2,missing_percent=30,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_method, best_val = optimize_imputation_hyperparams(imputation_func=do_mf,original_df=new_df,missing_percent=30, timelimit=300,\n",
    "#                                                         continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_missingness(df, show_missingness=False, random_state=42):\n",
    "    \"\"\"\n",
    "    Simulate missingness by dropping rows with missing values and reintroducing them.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        show_missingness (bool): If True, prints missingness percentages.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Original DataFrame without missing values, simulated DataFrame with missingness, and a mask.\n",
    "    \"\"\"\n",
    "    missing_original = df.isna().mean()\n",
    "    df2 = df.dropna().reset_index(drop=True)\n",
    "    df3 = df2.copy()\n",
    "    missing_mask = pd.DataFrame(False, index=df3.index, columns=df3.columns)\n",
    "\n",
    "    for col in df3.columns:\n",
    "        n_missing = int(round(missing_original[col] * len(df3)))\n",
    "        if n_missing > 0:\n",
    "            missing_indices = df3.sample(n=n_missing, random_state=random_state).index\n",
    "            df3.loc[missing_indices, col] = np.nan\n",
    "            missing_mask.loc[missing_indices, col] = True\n",
    "\n",
    "    if show_missingness:\n",
    "        missing_df3 = df3.isna().mean()\n",
    "        print(\"Missingness Comparison:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"Column '{col}': Original: {missing_original[col]*100:.2f}% \\t -> \\t df3: {missing_df3[col]*100:.2f}%\")\n",
    "\n",
    "    return df2, df3, missing_mask\n",
    "\n",
    "def create_missings(df: pd.DataFrame, missingness: float, random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Create random missingness in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        missingness (float): Percentage of missing values to introduce.\n",
    "        random_seed (float): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original DataFrame, DataFrame with missing values, mask DataFrame)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    mask = np.random.rand(*df.shape) < (missingness / 100)\n",
    "    mask_df = pd.DataFrame(mask, columns=df.columns)\n",
    "    df_missing = df.mask(mask)\n",
    "    return df, df_missing, mask_df\n",
    "\n",
    "\n",
    "def select_best_imputations(imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols, method_info=None, method_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate one or several imputed DataFrames and determine an aggregated error.\n",
    "\n",
    "    For each column with simulated missing data (per mask_df), numeric columns\n",
    "    are scored using Mean Absolute Error (MAE) while categorical columns are scored\n",
    "    by misclassification rate (1 - accuracy). An overall aggregated error is returned,\n",
    "    which is the mean error over all evaluated columns.\n",
    "\n",
    "    Parameters:\n",
    "      imputed_dfs (list of pd.DataFrame): A list of imputed DataFrames.\n",
    "      original_df (pd.DataFrame): The original (complete) DataFrame.\n",
    "      mask_df (pd.DataFrame): Boolean DataFrame with True at positions where values are masked.\n",
    "      continuous_cols (list): List of continuous numeric column names.\n",
    "      discrete_cols (list): List of discrete numeric column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "      method_info (str, optional): Text description of the method and its hyperparameters.\n",
    "      method_names (list, optional): List of names for each imputation method candidate.\n",
    "\n",
    "    Returns:\n",
    "      best_imputed_df (pd.DataFrame): A DataFrame where, for each column with missing values,\n",
    "                                     the candidate with the lowest error is chosen.\n",
    "      summary_table (pd.DataFrame): A summary table with metrics for each column.\n",
    "      aggregated_error (float): The average error across columns (lower is better).\n",
    "    \"\"\"\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    if method_info is not None:\n",
    "        parts = method_info.split(',')\n",
    "        base_name = parts[0].strip()\n",
    "        params = ','.join(parts[1:]).strip() if len(parts) > 1 else \"\"\n",
    "        method_names = [f\"{base_name} ({params})\"] * n_methods\n",
    "    elif method_names is None:\n",
    "        method_names = [f\"Method {i+1}\" for i in range(n_methods)]\n",
    "    \n",
    "    summary_list = []\n",
    "    best_method_per_col = {}\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_type = \"Categorical\"\n",
    "        else:\n",
    "            col_type = str(original_df[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,  \n",
    "            })\n",
    "            continue\n",
    "\n",
    "        col_errors = []\n",
    "        for df_imp in imputed_dfs:\n",
    "            if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "                try:\n",
    "                    imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                    orig_vals = pd.to_numeric(original_df[col][mask_df[col]], errors='coerce')\n",
    "                except Exception as e:\n",
    "                    imp_vals = df_imp[col][mask_df[col]]\n",
    "                    orig_vals = original_df[col][mask_df[col]]\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean()\n",
    "                col_errors.append(mae)\n",
    "            else:\n",
    "                correct = (df_imp[col][mask_df[col]] == original_df[col][mask_df[col]])\n",
    "                accuracy = correct.mean()\n",
    "                col_errors.append(1 - accuracy)\n",
    "\n",
    "        if col_type in [\"Continuous\", \"Discrete\"]:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        else:\n",
    "            best_idx = int(np.nanargmin(col_errors))\n",
    "        best_method = method_names[best_idx]\n",
    "        best_metric = col_errors[best_idx]\n",
    "\n",
    "        best_method_per_col[col] = best_idx\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    best_imputed_df = original_df.copy()\n",
    "    for cat in categorical_cols:\n",
    "        if cat in best_imputed_df:\n",
    "            best_imputed_df[cat] = best_imputed_df[cat].astype(object)\n",
    "\n",
    "    for col in original_df.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = \\\n",
    "                imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    errors = summary_table['Metric'].dropna().values\n",
    "    aggregated_error = np.mean(errors) if len(errors) > 0 else np.nan\n",
    "\n",
    "    return best_imputed_df, summary_table, aggregated_error\n",
    "\n",
    "\n",
    "def optimize_imputation_hyperparams(imputation_func, \n",
    "                                    original_df, \n",
    "                                    df_missing, \n",
    "                                    mask_df, \n",
    "                                    continuous_cols, \n",
    "                                    discrete_cols, \n",
    "                                    categorical_cols, \n",
    "                                    timelimit=600,    # in seconds\n",
    "                                    min_trials=20,\n",
    "                                    random_seed=96):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for an imputation function using Optuna.\n",
    "\n",
    "    This function takes the complete (original) DataFrame and a missing percentage.\n",
    "    It uses `create_missings` to generate a DataFrame with simulated missing values and\n",
    "    a corresponding mask. Then it runs the candidate imputation method on the incomplete\n",
    "    DataFrame, evaluates the imputed results against the original DataFrame using the mask,\n",
    "    and guides the hyperparameter search based on an aggregated error (lower is better).\n",
    "\n",
    "    Parameters:\n",
    "        imputation_func (callable): An imputation function (do_knn, do_mice, do_mf, or do_midas).\n",
    "        original_df (pd.DataFrame): The complete ground-truth DataFrame.\n",
    "        missing_percent (float): Percentage of missing values to simulate.\n",
    "        continuous_cols (list): List of continuous numeric column names.\n",
    "        discrete_cols (list): List of discrete numeric column names.\n",
    "        categorical_cols (list): List of categorical column names.\n",
    "        timelimit (int): Maximum time in seconds to run the optimization.\n",
    "        min_trials (int): Minimum number of Optuna trials to run.\n",
    "        random_seed (int): Seed for generating missingness (passed to create_missings).\n",
    "\n",
    "    Returns:\n",
    "        best_trial: The best trial object from the study.\n",
    "        best_value: The best (lowest) aggregated objective value.\n",
    "    \"\"\"\n",
    "    # Generate missing values and mask using the provided function.\n",
    "    # _, df_missing, mask_df = create_missings(original_df, missingness=missing_percent, random_seed=random_seed)\n",
    "\n",
    "    def objective(trial):\n",
    "        func_name = imputation_func.__name__\n",
    "        params = {}\n",
    "\n",
    "        if func_name == \"do_knn\":\n",
    "            params['n_neighbors'] = trial.suggest_int(\"n_neighbors\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            # Run imputation on df_missing, not the original complete data.\n",
    "            imputed_df = imputation_func(df_missing, \n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols, \n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"KNN, n_neighbors={params['n_neighbors']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mice\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 5, 20)\n",
    "            params['strat'] = trial.suggest_categorical(\"strat\", ['normal', 'shap', 'fast'])\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MICE, iters={params['iters']}, strat={params['strat']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_mf\":\n",
    "            params['iters'] = trial.suggest_int(\"iters\", 3, 15)\n",
    "            params['scale'] = trial.suggest_categorical(\"scale\", [True, False])\n",
    "            imputed_df = imputation_func(df_missing,\n",
    "                                         continuous_cols=continuous_cols, \n",
    "                                         discrete_cols=discrete_cols, \n",
    "                                         categorical_cols=categorical_cols,\n",
    "                                         **params)\n",
    "            imputed_dfs = [imputed_df]\n",
    "            method_info = f\"MissForest, iters={params['iters']}, scale={params['scale']}\"\n",
    "        elif func_name == \"do_midas\":\n",
    "            params['layer'] = trial.suggest_categorical(\"layer\", [[256,256], [128,128], [512,256]])\n",
    "            params['vae'] = trial.suggest_categorical(\"vae\", [True, False])\n",
    "            params['samples'] = trial.suggest_int(\"samples\", 5, 20)\n",
    "            imputed_dfs, method_info = imputation_func(df_missing,\n",
    "                                                       continuous_cols=continuous_cols, \n",
    "                                                       discrete_cols=discrete_cols, \n",
    "                                                       categorical_cols=categorical_cols,\n",
    "                                                       **params)\n",
    "            imputed_dfs = [imputed_dfs[0]]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported imputation function: {func_name}\")\n",
    "\n",
    "        # Evaluate the imputed result by comparing against the original complete DataFrame.\n",
    "        _, _, aggregated_error = select_best_imputations(\n",
    "            imputed_dfs, original_df, mask_df, continuous_cols, discrete_cols, categorical_cols,\n",
    "            method_info=method_info\n",
    "        )\n",
    "\n",
    "        if np.isnan(aggregated_error):\n",
    "            aggregated_error = 1e6\n",
    "\n",
    "        return aggregated_error\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, timeout=timelimit, n_trials=min_trials)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_value = best_trial.value\n",
    "\n",
    "    print(\"Optimization completed!\")\n",
    "    print(\"Best Trial Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Objective Value (aggregated error): {best_value}\")\n",
    "\n",
    "    return best_trial, best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_midas(df,\n",
    "             continuous_cols=None,\n",
    "             discrete_cols=None,\n",
    "             categorical_cols=None,\n",
    "             layer: list = [256, 256],\n",
    "             vae: bool = True,\n",
    "             samples: int = 10,\n",
    "             random_seed: float = 96):\n",
    "    \"\"\"\n",
    "    Imputes missing values using the MIDAS model.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input dataframe with NaNs in both numeric & categorical.\n",
    "      continuous_cols (list): List of continuous column names.\n",
    "      discrete_cols (list): List of discrete (numeric but not continuous) column names.\n",
    "      categorical_cols (list): List of categorical column names.\n",
    "\n",
    "    Returns:\n",
    "      imps (list): A list of imputed dataframes, with original dtypes restored.\n",
    "      method_info (str): Summary of MIDAS params used.\n",
    "    \"\"\"\n",
    "    # 1. Onehot encode the categoricals\n",
    "    md_cat_data, md_cats = md.cat_conv(df[categorical_cols])\n",
    "\n",
    "    # 2. Build the wide DF: drop raw cats, append onehots\n",
    "    df_num = df.drop(columns=categorical_cols)\n",
    "    data_in = pd.concat([df_num, md_cat_data], axis=1)\n",
    "\n",
    "    # 3. Record & reinsert the NaN locations so MIDAS sees them as missing\n",
    "    na_mask = data_in.isnull()\n",
    "    data_in[na_mask] = np.nan\n",
    "\n",
    "    # 4. Scale only the numeric columns in place\n",
    "    num_cols = discrete_cols + continuous_cols\n",
    "    scaler = MinMaxScaler()\n",
    "    data_in[num_cols] = scaler.fit_transform(data_in[num_cols])\n",
    "\n",
    "    # 5. Build & train the MIDAS model\n",
    "    imputer = md.Midas(\n",
    "        layer_structure=layer,\n",
    "        vae_layer=vae,\n",
    "        seed=random_seed,\n",
    "        input_drop=0.75\n",
    "    )\n",
    "    imputer.build_model(data_in, softmax_columns=md_cats)\n",
    "    imputer.train_model(training_epochs=20)\n",
    "\n",
    "    # 6. Generate multiple imputations\n",
    "    raw_imps = imputer.generate_samples(m=samples).output_list\n",
    "\n",
    "    # 7. Decode each imputed DF back to original structure\n",
    "    flat_cats = [c for grp in md_cats for c in grp]\n",
    "    imps = []\n",
    "\n",
    "    for imp_df in raw_imps:\n",
    "        # 7a. inversescale numeric cols\n",
    "        imp_df[num_cols] = scaler.inverse_transform(imp_df[num_cols])\n",
    "\n",
    "        # 7b. decode onehots (before dropping them!)\n",
    "        decoded = {}\n",
    "        for i, grp in enumerate(md_cats):\n",
    "            # just in case, only keep those actually present\n",
    "            present = [c for c in grp if c in imp_df.columns]\n",
    "            # idxmax  gives the dummy column name with highest prob\n",
    "            decoded[categorical_cols[i]] = imp_df[present].idxmax(axis=1)\n",
    "\n",
    "        cat_df = pd.DataFrame(decoded, index=imp_df.index)\n",
    "\n",
    "        # 7c. now drop the dummy cols\n",
    "        base = imp_df.drop(columns=flat_cats, errors='ignore')\n",
    "\n",
    "        # 7d. concat in your decoded cat columns\n",
    "        merged = pd.concat([base, cat_df], axis=1)\n",
    "\n",
    "        # 7e. round discrete cols\n",
    "        merged[discrete_cols] = merged[discrete_cols].round().astype(int)\n",
    "\n",
    "        imps.append(merged)\n",
    "\n",
    "    method_info = f\"MIDAS, params: samples={samples}, layer={layer}, vae={vae}\"\n",
    "    return imps, method_info\n",
    "\n",
    "\n",
    "def run_full_pipeline(df: pd.DataFrame, \n",
    "                      simulate:bool=False,               # True for simulated missingness, False for random missingness\n",
    "                      missingness_value: float = 10.0,   # used only for random missingness (percent)\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "\n",
    "    The pipeline performs these steps:\n",
    "    \n",
    "      1. Preprocesses the DataFrame using `prep`, which cleans the data,\n",
    "         encodes categorical variables, and splits features into continuous,\n",
    "         discrete, and categorical lists.\n",
    "      2. Introduces missingness using either simulated missingness (reintroducing missingness \n",
    "         based on the original NaN proportions) or random missingness (dropping values randomly\n",
    "         given a specified missing percentage).\n",
    "      3. Runs hyperparameter optimization (via `optimize_imputation_hyperparams`) for each candidate \n",
    "         imputation method (e.g., do_knn, do_mice, do_mf, do_midas).\n",
    "         \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        missing_type (str): \"simulate\" to simulate missingness using original missing proportions,\n",
    "                            \"random\" to drop values randomly.\n",
    "        missingness_value (float): Percentage of missingness (only used if missing_type == \"random\").\n",
    "        show_missingness (bool): If True, prints missingness comparison when using simulate missingness.\n",
    "        timelimit (int): Time limit (in seconds) for each hyperparameter optimization study.\n",
    "        min_trials (int): Minimum number of trials for each study.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are method names (strings) and the values are the best \n",
    "              hyperparameter dictionaries (from the best Optuna trial) for that method.\n",
    "    \"\"\"\n",
    "\n",
    "    # # Step 1: Preprocess Data\n",
    "    # Note: For simulation, the missing proportions are taken from the original df.\n",
    "    if simulate: \n",
    "        # simulate_missingness returns: (complete_df, df_with_missing, missing_mask)\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(df, \n",
    "                                                                      show_missingness=show_missingness,\n",
    "                                                                      random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(df, \n",
    "                                                                 missingness=missingness_value, \n",
    "                                                                 random_seed=random_seed)\n",
    "        \n",
    "    # Step 2: Preprocess Data, convert categorical cols to encoded values and find the data types.\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "    \n",
    "\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    # Optimize hyperparameters for each imputation method candidate.\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "        # Optimize hyperparameters for each imputation method candidate.\n",
    "    \n",
    "    for key, val in best_hyperparams.items():\n",
    "        if key == 'KNN':\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MICE':\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "\n",
    "        elif key == 'MissForest':\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, iters=val['iters'], scale=val['scale']) \n",
    "\n",
    "        elif key == 'MIDAS':\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols, discrete_cols=discrete_cols, categorical_cols=categorical_cols, layer=val['layer'], vae=val['vae'], samples=val['samples'])\n",
    "\n",
    "    # Create a list of imputed DataFrames.  \n",
    "    imputed_dfs = [df_knn, df_mice, df_mf, df_midas] \n",
    "    # decoded_imputed_dfs = []\n",
    "    # for i in imputed_dfs:\n",
    "    #     decoded_df = reverse_encoding(i, encoders)\n",
    "    #     decoded_imputed_dfs.append(decoded_df)\n",
    "          \n",
    "    # Create a list of method names.    \n",
    "    method_names = ['KNN', 'MICE', 'MissForest', 'MIDAS']\n",
    "    \n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        # Determine the data type label.\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        # Only evaluate columns that had artificial missing values.\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "        \n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            # Ensure the original column is numeric.\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                raise ValueError(f\"Column '{col}' is marked as numeric but contains non-numeric values.\")\n",
    "            for df_imp in imputed_dfs:\n",
    "                # Convert values to numeric.\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            \n",
    "            # Compute additional metrics for the best method.\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            \n",
    "            # Compute fraction within 10%.\n",
    "            # For nonzero original values, check error <= 0.1 * |original|.\n",
    "            # For zeros, require the imputed value to be exactly 0.\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "            \n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            # For categorical columns, compute accuracy.\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "            # Extra metrics are not applicable for categoricals.\n",
    "            error_sd = np.nan\n",
    "            max_error = np.nan\n",
    "            min_error = np.nan\n",
    "            within_10pct = np.nan\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "        \n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "        \n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "    \n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "    \n",
    "    # Build best-imputed DataFrame by replacing masked entries with values from the best method.\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline2(df: pd.DataFrame, \n",
    "                      simulate: bool = False,               \n",
    "                      missingness_value: float = 10.0,   \n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600, \n",
    "                      min_trials: int = 20, \n",
    "                      random_seed: int = 96):\n",
    "    \"\"\"\n",
    "    Run the full pipeline to find the best hyperparameters for each imputation method.\n",
    "    \"\"\"\n",
    "    # Step 1: Create missingness (simulated or random)\n",
    "    if simulate: \n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed)\n",
    "    else:   \n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed)\n",
    "\n",
    "    # Step 2: Preprocess for column types\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "  \n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    # Step 3: Optimize hyperparameters per method\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_trial.params} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    # Step 4: Run best imputation for each method and collect valid results\n",
    "    if best_hyperparams.get('KNN'):\n",
    "        try:\n",
    "            df_knn = do_knn(df_missing, continuous_cols=continuous_cols,\n",
    "                            discrete_cols=discrete_cols,\n",
    "                            categorical_cols=categorical_cols,\n",
    "                            n_neighbors=best_hyperparams['KNN']['n_neighbors'],\n",
    "                            scale=best_hyperparams['KNN']['scale'])\n",
    "            imputed_dfs.append(df_knn)\n",
    "            method_names.append('KNN')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with KNN: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MICE'):\n",
    "        try:\n",
    "            df_mice = do_mice(df_missing, continuous_cols=continuous_cols,\n",
    "                              discrete_cols=discrete_cols,\n",
    "                              categorical_cols=categorical_cols,\n",
    "                              iters=best_hyperparams['MICE']['iters'],\n",
    "                              strat=best_hyperparams['MICE']['strat'],\n",
    "                              scale=best_hyperparams['MICE']['scale'])\n",
    "            imputed_dfs.append(df_mice)\n",
    "            method_names.append('MICE')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MICE: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MissForest'):\n",
    "        try:\n",
    "            df_mf = do_mf(df_missing, continuous_cols=continuous_cols,\n",
    "                          discrete_cols=discrete_cols,\n",
    "                          categorical_cols=categorical_cols,\n",
    "                          iters=best_hyperparams['MissForest']['iters'],\n",
    "                          scale=best_hyperparams['MissForest']['scale'])\n",
    "            imputed_dfs.append(df_mf)\n",
    "            method_names.append('MissForest')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MissForest: {e}\")\n",
    "\n",
    "    if best_hyperparams.get('MIDAS'):\n",
    "        try:\n",
    "            df_midas, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                   discrete_cols=discrete_cols,\n",
    "                                   categorical_cols=categorical_cols,\n",
    "                                   layer=best_hyperparams['MIDAS']['layer'],\n",
    "                                   vae=best_hyperparams['MIDAS']['vae'],\n",
    "                                   samples=best_hyperparams['MIDAS']['samples'])\n",
    "            imputed_dfs.append(df_midas)\n",
    "            method_names.append('MIDAS')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with MIDAS: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    # Step 5: Evaluate and select best method per column\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            summary_list.append({\n",
    "                'Column': col, 'Data Type': col_data_type, 'Best Method': None,\n",
    "                'Metric': np.nan, 'Error_SD': np.nan, 'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan, 'Within_10pct': np.nan\n",
    "            })\n",
    "            best_method_per_col[col] = None\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = max_error = min_error = within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            print([type(df_imp) for df_imp in imputed_dfs])\n",
    "\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    # Step 6: Final best-imputed DataFrame\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline3(df: pd.DataFrame, \n",
    "                      simulate: bool = False,\n",
    "                      missingness_value: float = 10.0,\n",
    "                      show_missingness: bool = False,\n",
    "                      timelimit: int = 600,\n",
    "                      min_trials: int = 20,\n",
    "                      random_seed: int = 96):\n",
    "    if simulate:\n",
    "        df_complete, df_missing, mask_df = simulate_missingness(\n",
    "            df, show_missingness=show_missingness, random_state=random_seed\n",
    "        )\n",
    "    else:\n",
    "        df_complete, df_missing, mask_df = create_missings(\n",
    "            df, missingness=missingness_value, random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    continuous_cols, discrete_cols, categorical_cols = prep(df)\n",
    "\n",
    "    candidate_methods = {\n",
    "        \"KNN\": do_knn,\n",
    "        \"MICE\": do_mice,\n",
    "        \"MissForest\": do_mf,\n",
    "        \"MIDAS\": do_midas\n",
    "    }\n",
    "\n",
    "    best_hyperparams = {}\n",
    "\n",
    "    for method_name, imputation_func in candidate_methods.items():\n",
    "        print(f\"\\nOptimizing hyperparameters for {method_name}...\")\n",
    "        try:\n",
    "            best_trial, best_value = optimize_imputation_hyperparams(\n",
    "                imputation_func=imputation_func,\n",
    "                original_df=df_complete,\n",
    "                df_missing=df_missing,\n",
    "                mask_df=mask_df,\n",
    "                continuous_cols=continuous_cols,\n",
    "                discrete_cols=discrete_cols,\n",
    "                categorical_cols=categorical_cols,\n",
    "                timelimit=timelimit,\n",
    "                min_trials=min_trials,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            best_hyperparams[method_name] = best_trial.params\n",
    "            print(f'Best hyperparameters for {method_name}: {best_hyperparams[method_name]} with best agg error of {best_value}')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while optimizing {method_name}: {e}\")\n",
    "            best_hyperparams[method_name] = None\n",
    "\n",
    "    imputed_dfs = []\n",
    "    method_names = []\n",
    "\n",
    "    for method in ['KNN', 'MICE', 'MissForest', 'MIDAS']:\n",
    "        val = best_hyperparams.get(method)\n",
    "        if not val:\n",
    "            continue\n",
    "        try:\n",
    "            if method == 'KNN':\n",
    "                df_knn = do_knn(df_missing, continuous_cols=continuous_cols, \n",
    "                                discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                n_neighbors=val['n_neighbors'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_knn)\n",
    "                method_names.append('KNN')\n",
    "\n",
    "            elif method == 'MICE':\n",
    "                df_mice = do_mice(df_missing, continuous_cols=continuous_cols, \n",
    "                                  discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                                  iters=val['iters'], strat=val['strat'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mice)\n",
    "                method_names.append('MICE')\n",
    "\n",
    "            elif method == 'MissForest':\n",
    "                df_mf = do_mf(df_missing, continuous_cols=continuous_cols, \n",
    "                              discrete_cols=discrete_cols, categorical_cols=categorical_cols, \n",
    "                              iters=val['iters'], scale=val['scale'])\n",
    "                imputed_dfs.append(df_mf)\n",
    "                method_names.append('MissForest')\n",
    "\n",
    "            elif method == 'MIDAS':\n",
    "                df_midas_list, _ = do_midas(df_missing, continuous_cols=continuous_cols,\n",
    "                                            discrete_cols=discrete_cols,\n",
    "                                            categorical_cols=categorical_cols,\n",
    "                                            layer=val['layer'], vae=val['vae'], \n",
    "                                            samples=val['samples'])\n",
    "                imputed_dfs.extend(df_midas_list)\n",
    "                method_names.extend([f'MIDAS_{i+1}' for i in range(len(df_midas_list))])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to impute with {method}: {e}\")\n",
    "\n",
    "    best_method_per_col = {}\n",
    "    summary_list = []\n",
    "\n",
    "    for col in df_missing.columns:\n",
    "        if col in continuous_cols:\n",
    "            col_data_type = \"Continuous\"\n",
    "        elif col in discrete_cols:\n",
    "            col_data_type = \"Discrete\"\n",
    "        elif col in categorical_cols:\n",
    "            col_data_type = \"Categorical\"\n",
    "        else:\n",
    "            col_data_type = str(df_missing[col].dtype)\n",
    "\n",
    "        if mask_df[col].sum() == 0:\n",
    "            best_method_per_col[col] = None\n",
    "            summary_list.append({\n",
    "                'Column': col,\n",
    "                'Data Type': col_data_type,\n",
    "                'Best Method': None,\n",
    "                'Metric': np.nan,\n",
    "                'Error_SD': np.nan,\n",
    "                'Max_Error': np.nan,\n",
    "                'Min_Error': np.nan,\n",
    "                'Within_10pct': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        metrics = []\n",
    "        error_sd = np.nan\n",
    "        max_error = np.nan\n",
    "        min_error = np.nan\n",
    "        within_10pct = np.nan\n",
    "\n",
    "        if col in continuous_cols or col in discrete_cols:\n",
    "            for df_imp in imputed_dfs:\n",
    "                imp_vals = pd.to_numeric(df_imp[col][mask_df[col]], errors='coerce')\n",
    "                orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "                errors = np.abs(imp_vals - orig_vals)\n",
    "                mae = errors.mean() if not errors.empty else np.nan\n",
    "                metrics.append(mae)\n",
    "            best_idx = np.nanargmin(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "            best_imp_vals = pd.to_numeric(imputed_dfs[best_idx][col][mask_df[col]], errors='coerce')\n",
    "            best_orig_vals = pd.to_numeric(df_complete[col][mask_df[col]], errors='coerce')\n",
    "            errors = np.abs(best_imp_vals - best_orig_vals)\n",
    "            error_sd = errors.std() if not errors.empty else np.nan\n",
    "            max_error = errors.max() if not errors.empty else np.nan\n",
    "            min_error = errors.min() if not errors.empty else np.nan\n",
    "            condition = ((best_orig_vals != 0) & (errors <= 0.1 * best_orig_vals.abs())) | \\\n",
    "                        ((best_orig_vals == 0) & (errors == 0))\n",
    "            within_10pct = condition.mean() if not condition.empty else np.nan\n",
    "\n",
    "        elif col in categorical_cols or pd.api.types.is_string_dtype(df_complete[col]):\n",
    "            for df_imp in imputed_dfs:\n",
    "                correct = (df_imp[col][mask_df[col]] == df_complete[col][mask_df[col]])\n",
    "                acc = correct.mean() if not correct.empty else np.nan\n",
    "                metrics.append(acc)\n",
    "            best_idx = np.nanargmax(metrics)\n",
    "            best_metric = metrics[best_idx]\n",
    "\n",
    "        else:\n",
    "            best_idx = None\n",
    "            best_metric = np.nan\n",
    "\n",
    "        best_method = method_names[best_idx] if best_idx is not None else None\n",
    "        best_method_per_col[col] = best_idx\n",
    "\n",
    "        summary_list.append({\n",
    "            'Column': col,\n",
    "            'Data Type': col_data_type,\n",
    "            'Best Method': best_method,\n",
    "            'Metric': best_metric,\n",
    "            'Error_SD': error_sd,\n",
    "            'Max_Error': max_error,\n",
    "            'Min_Error': min_error,\n",
    "            'Within_10pct': within_10pct\n",
    "        })\n",
    "\n",
    "    summary_table = pd.DataFrame(summary_list)\n",
    "\n",
    "    best_imputed_df = df_complete.copy()\n",
    "    for col in df_complete.columns:\n",
    "        if mask_df[col].sum() > 0 and best_method_per_col[col] is not None:\n",
    "            method_idx = best_method_per_col[col]\n",
    "            best_imputed_df.loc[mask_df[col], col] = imputed_dfs[method_idx].loc[mask_df[col], col]\n",
    "\n",
    "    return best_imputed_df, summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = run_full_pipeline(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 03:59:43,460] A new study created in memory with name: no-name-b2804d32-bc31-45f3-8978-dcb86a13e862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing hyperparameters for KNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 03:59:43,728] Trial 0 finished with value: 130465.4207477649 and parameters: {'n_neighbors': 13, 'scale': False}. Best is trial 0 with value: 130465.4207477649.\n",
      "[I 2025-04-17 03:59:43,946] Trial 1 finished with value: 134917.0266492101 and parameters: {'n_neighbors': 4, 'scale': True}. Best is trial 0 with value: 130465.4207477649.\n",
      "[I 2025-04-17 03:59:44,158] Trial 2 finished with value: 128910.987206718 and parameters: {'n_neighbors': 5, 'scale': True}. Best is trial 2 with value: 128910.987206718.\n",
      "[I 2025-04-17 03:59:44,356] Trial 3 finished with value: 134917.0266492101 and parameters: {'n_neighbors': 4, 'scale': True}. Best is trial 2 with value: 128910.987206718.\n",
      "[I 2025-04-17 03:59:44,553] Trial 4 finished with value: 126270.22596605406 and parameters: {'n_neighbors': 14, 'scale': True}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:44,718] Trial 5 finished with value: 155476.810668925 and parameters: {'n_neighbors': 3, 'scale': False}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:44,900] Trial 6 finished with value: 126868.45825273624 and parameters: {'n_neighbors': 13, 'scale': True}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:45,073] Trial 7 finished with value: 126868.45825273624 and parameters: {'n_neighbors': 13, 'scale': True}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:45,266] Trial 8 finished with value: 134917.0266492101 and parameters: {'n_neighbors': 4, 'scale': True}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:45,450] Trial 9 finished with value: 134005.4336048941 and parameters: {'n_neighbors': 3, 'scale': True}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:45,637] Trial 10 finished with value: 128624.07678141852 and parameters: {'n_neighbors': 10, 'scale': False}. Best is trial 4 with value: 126270.22596605406.\n",
      "[I 2025-04-17 03:59:45,817] Trial 11 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 11 with value: 125837.15715446904.\n",
      "[I 2025-04-17 03:59:45,995] Trial 12 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 11 with value: 125837.15715446904.\n",
      "[I 2025-04-17 03:59:46,182] Trial 13 finished with value: 125837.15715446904 and parameters: {'n_neighbors': 15, 'scale': True}. Best is trial 11 with value: 125837.15715446904.\n",
      "[I 2025-04-17 03:59:46,356] Trial 14 finished with value: 128624.07678141852 and parameters: {'n_neighbors': 10, 'scale': False}. Best is trial 11 with value: 125837.15715446904.\n",
      "[I 2025-04-17 03:59:46,547] Trial 15 finished with value: 125437.40255018426 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 15 with value: 125437.40255018426.\n",
      "[I 2025-04-17 03:59:46,733] Trial 16 finished with value: 125437.40255018426 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 15 with value: 125437.40255018426.\n",
      "[I 2025-04-17 03:59:46,913] Trial 17 finished with value: 125437.40255018426 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 15 with value: 125437.40255018426.\n",
      "[I 2025-04-17 03:59:47,086] Trial 18 finished with value: 131090.14980959828 and parameters: {'n_neighbors': 7, 'scale': False}. Best is trial 15 with value: 125437.40255018426.\n",
      "[I 2025-04-17 03:59:47,270] Trial 19 finished with value: 125437.40255018426 and parameters: {'n_neighbors': 7, 'scale': True}. Best is trial 15 with value: 125437.40255018426.\n",
      "[I 2025-04-17 03:59:47,271] A new study created in memory with name: no-name-68433b6c-ff19-471e-a3f8-7de7daaad2f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  n_neighbors: 7\n",
      "  scale: True\n",
      "Best Objective Value (aggregated error): 125437.40255018426\n",
      "Best hyperparameters for KNN: {'n_neighbors': 7, 'scale': True} with best agg error of 125437.40255018426\n",
      "\n",
      "Optimizing hyperparameters for MICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:831: RuntimeWarning: overflow encountered in cast\n",
      "  bachelor_preds = bachelor_preds.astype(_PRE_LINK_DATATYPE)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py:776: RuntimeWarning: overflow encountered in cast\n",
      "  candidate_preds = candidate_preds.astype(_PRE_LINK_DATATYPE)  # type: ignore\n",
      "[W 2025-04-17 03:59:48,418] Trial 0 failed with parameters: {'iters': 12, 'strat': 'shap', 'scale': True} because of the following error: ValueError('data must be finite, check for nan or inf values').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_36436\\3942454051.py\", line 216, in objective\n",
      "    imputed_df = imputation_func(df_missing,\n",
      "  File \"C:\\Users\\Matin\\AppData\\Local\\Temp\\ipykernel_36436\\3092344904.py\", line 90, in do_mice\n",
      "    kernel.mice(iterations=iters, verbose=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 1186, in mice\n",
      "    imputation_values = self._mean_match_mice(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 971, in _mean_match_mice\n",
      "    imputation_values = self._mean_match_nearest_neighbors(\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\miceforest\\imputation_kernel.py\", line 602, in _mean_match_nearest_neighbors\n",
      "    kd_tree = KDTree(candidate_preds, leafsize=16, balanced_tree=False)\n",
      "  File \"c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\_kdtree.py\", line 360, in __init__\n",
      "    super().__init__(data, leafsize, compact_nodes, copy_data,\n",
      "  File \"_ckdtree.pyx\", line 561, in scipy.spatial._ckdtree.cKDTree.__init__\n",
      "ValueError: data must be finite, check for nan or inf values\n",
      "[W 2025-04-17 03:59:48,422] Trial 0 failed with value None.\n",
      "[I 2025-04-17 03:59:48,423] A new study created in memory with name: no-name-5ce495d3-d119-441b-92a1-e78a461bc774\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while optimizing MICE: data must be finite, check for nan or inf values\n",
      "\n",
      "Optimizing hyperparameters for MissForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 2/3 [00:08<00:04,  4.38s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 67%|   | 2/3 [00:14<00:07,  7.11s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.38it/s]\n",
      "[I 2025-04-17 04:00:03,863] Trial 0 finished with value: 133132.05827693842 and parameters: {'iters': 3, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 67%|   | 2/3 [00:10<00:04,  4.95s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 67%|   | 2/3 [00:14<00:07,  7.30s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.97it/s]\n",
      "[I 2025-04-17 04:00:19,520] Trial 1 finished with value: 133145.73115992727 and parameters: {'iters': 3, 'scale': True}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 13%|        | 2/15 [00:08<00:56,  4.36s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 13%|        | 2/15 [00:12<01:24,  6.47s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.11it/s]\n",
      "[I 2025-04-17 04:00:33,439] Trial 2 finished with value: 133132.05827693842 and parameters: {'iters': 15, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 33%|      | 2/6 [00:08<00:17,  4.30s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 33%|      | 2/6 [00:13<00:26,  6.73s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  3.12it/s]\n",
      "[I 2025-04-17 04:00:47,882] Trial 3 finished with value: 133145.73115992727 and parameters: {'iters': 6, 'scale': True}. Best is trial 0 with value: 133132.05827693842.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 14%|        | 2/14 [00:08<00:49,  4.11s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 14%|        | 2/14 [00:12<01:12,  6.02s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.94it/s]\n",
      "[I 2025-04-17 04:01:00,936] Trial 4 finished with value: 133132.05827693842 and parameters: {'iters': 14, 'scale': False}. Best is trial 0 with value: 133132.05827693842.\n",
      "[I 2025-04-17 04:01:00,939] A new study created in memory with name: no-name-8db89a98-d953-4836-9046-72fe6316fe89\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  iters: 3\n",
      "  scale: False\n",
      "Best Objective Value (aggregated error): 133132.05827693842\n",
      "Best hyperparameters for MissForest: {'iters': 3, 'scale': False} with best agg error of 133132.05827693842\n",
      "\n",
      "Optimizing hyperparameters for MIDAS...\n",
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 15.636027336120605\n",
      "Epoch: 1 , loss: 12.337962044609917\n",
      "Epoch: 2 , loss: 9.15252325269911\n",
      "Epoch: 3 , loss: 8.159549713134766\n",
      "Epoch: 4 , loss: 7.996186362372504\n",
      "Epoch: 5 , loss: 7.822097937266032\n",
      "Epoch: 6 , loss: 7.579149934980604\n",
      "Epoch: 7 , loss: 7.499464909235637\n",
      "Epoch: 8 , loss: 7.326467196146647\n",
      "Epoch: 9 , loss: 7.373421165678236\n",
      "Epoch: 10 , loss: 7.259845124350654\n",
      "Epoch: 11 , loss: 7.192229244444105\n",
      "Epoch: 12 , loss: 7.185925722122192\n",
      "Epoch: 13 , loss: 7.138575024074978\n",
      "Epoch: 14 , loss: 7.124434126747979\n",
      "Epoch: 15 , loss: 7.085193819469875\n",
      "Epoch: 16 , loss: 7.072419431474474\n",
      "Epoch: 17 , loss: 7.10293001598782\n",
      "Epoch: 18 , loss: 7.054453531901042\n",
      "Epoch: 19 , loss: 6.948907534281413\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:01:15,481] Trial 0 finished with value: 136012.90598441515 and parameters: {'layer': [512, 256], 'vae': False, 'samples': 16}. Best is trial 0 with value: 136012.90598441515.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 16.207295894622803\n",
      "Epoch: 1 , loss: 14.146910137600369\n",
      "Epoch: 2 , loss: 12.069238079918755\n",
      "Epoch: 3 , loss: 9.983858479393852\n",
      "Epoch: 4 , loss: 8.95934862560696\n",
      "Epoch: 5 , loss: 8.43393784099155\n",
      "Epoch: 6 , loss: 8.225883536868626\n",
      "Epoch: 7 , loss: 7.938134670257568\n",
      "Epoch: 8 , loss: 7.808457692464192\n",
      "Epoch: 9 , loss: 7.727700445387098\n",
      "Epoch: 10 , loss: 7.6380121443006725\n",
      "Epoch: 11 , loss: 7.504967716005114\n",
      "Epoch: 12 , loss: 7.495669523874919\n",
      "Epoch: 13 , loss: 7.410564766989814\n",
      "Epoch: 14 , loss: 7.345555861790975\n",
      "Epoch: 15 , loss: 7.267838133705987\n",
      "Epoch: 16 , loss: 7.2920101748572455\n",
      "Epoch: 17 , loss: 7.245056258307563\n",
      "Epoch: 18 , loss: 7.265906201468574\n",
      "Epoch: 19 , loss: 7.193224694993761\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:01:29,750] Trial 1 finished with value: 139271.67633861475 and parameters: {'layer': [256, 256], 'vae': False, 'samples': 14}. Best is trial 0 with value: 136012.90598441515.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 16.207295682695175\n",
      "Epoch: 1 , loss: 14.146891487969292\n",
      "Epoch: 2 , loss: 12.069215774536133\n",
      "Epoch: 3 , loss: 9.983822769588894\n",
      "Epoch: 4 , loss: 8.959352440304226\n",
      "Epoch: 5 , loss: 8.433939377466837\n",
      "Epoch: 6 , loss: 8.22588555018107\n",
      "Epoch: 7 , loss: 7.938143067889744\n",
      "Epoch: 8 , loss: 7.808442539638943\n",
      "Epoch: 9 , loss: 7.727703703774346\n",
      "Epoch: 10 , loss: 7.63799622323778\n",
      "Epoch: 11 , loss: 7.504962046941121\n",
      "Epoch: 12 , loss: 7.495663298500909\n",
      "Epoch: 13 , loss: 7.410571283764309\n",
      "Epoch: 14 , loss: 7.34552706612481\n",
      "Epoch: 15 , loss: 7.267837365468343\n",
      "Epoch: 16 , loss: 7.291982730229695\n",
      "Epoch: 17 , loss: 7.245056788126628\n",
      "Epoch: 18 , loss: 7.265892399681939\n",
      "Epoch: 19 , loss: 7.193230523003472\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:01:44,662] Trial 2 finished with value: 139274.98515212914 and parameters: {'layer': [256, 256], 'vae': False, 'samples': 15}. Best is trial 0 with value: 136012.90598441515.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 17.09950171576606\n",
      "Epoch: 1 , loss: 15.891795688205296\n",
      "Epoch: 2 , loss: 15.031748453776041\n",
      "Epoch: 3 , loss: 14.102981885274252\n",
      "Epoch: 4 , loss: 13.14046859741211\n",
      "Epoch: 5 , loss: 12.020598305596245\n",
      "Epoch: 6 , loss: 10.783609125349257\n",
      "Epoch: 7 , loss: 9.804703447553846\n",
      "Epoch: 8 , loss: 9.114121278127035\n",
      "Epoch: 9 , loss: 8.790538999769423\n",
      "Epoch: 10 , loss: 8.35461057557\n",
      "Epoch: 11 , loss: 8.268216689427694\n",
      "Epoch: 12 , loss: 8.033705208036634\n",
      "Epoch: 13 , loss: 8.00889637735155\n",
      "Epoch: 14 , loss: 7.863510343763563\n",
      "Epoch: 15 , loss: 7.817133559121026\n",
      "Epoch: 16 , loss: 7.667694383197361\n",
      "Epoch: 17 , loss: 7.738150993982951\n",
      "Epoch: 18 , loss: 7.59298578898112\n",
      "Epoch: 19 , loss: 7.583406156963772\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:01:58,916] Trial 3 finished with value: 153096.41896828968 and parameters: {'layer': [128, 128], 'vae': False, 'samples': 12}. Best is trial 0 with value: 136012.90598441515.\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 19.495845794677734\n",
      "Epoch: 1 , loss: 18.502730051676433\n",
      "Epoch: 2 , loss: 17.599118126763237\n",
      "Epoch: 3 , loss: 16.298101160261368\n",
      "Epoch: 4 , loss: 14.668783929612902\n",
      "Epoch: 5 , loss: 12.127192444271511\n",
      "Epoch: 6 , loss: 10.295042779710558\n",
      "Epoch: 7 , loss: 9.102697902255589\n",
      "Epoch: 8 , loss: 8.62790200445387\n",
      "Epoch: 9 , loss: 8.59179589483473\n",
      "Epoch: 10 , loss: 8.238357411490547\n",
      "Epoch: 11 , loss: 8.169635746214125\n",
      "Epoch: 12 , loss: 8.169391605589125\n",
      "Epoch: 13 , loss: 7.897606902652317\n",
      "Epoch: 14 , loss: 8.027411513858372\n",
      "Epoch: 15 , loss: 7.657174958123101\n",
      "Epoch: 16 , loss: 7.874516937467787\n",
      "Epoch: 17 , loss: 7.530957645840115\n",
      "Epoch: 18 , loss: 7.54287510448032\n",
      "Epoch: 19 , loss: 7.634644481870863\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 04:02:17,092] Trial 4 finished with value: 144608.4053679733 and parameters: {'layer': [512, 256], 'vae': True, 'samples': 11}. Best is trial 0 with value: 136012.90598441515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization completed!\n",
      "Best Trial Hyperparameters:\n",
      "  layer: [512, 256]\n",
      "  vae: False\n",
      "  samples: 16\n",
      "Best Objective Value (aggregated error): 136012.90598441515\n",
      "Best hyperparameters for MIDAS: {'layer': [512, 256], 'vae': False, 'samples': 16} with best agg error of 136012.90598441515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:333: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      " 67%|   | 2/3 [00:10<00:05,  5.23s/it]c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:303: UserWarning: NRMSE increased.\n",
      "  warnings.warn(\"NRMSE increased.\")\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:453: UserWarning: Stopping criterion triggered during fitting. Before last imputation matrix will be returned.\n",
      "  warnings.warn(\n",
      " 67%|   | 2/3 [00:15<00:07,  7.58s/it]\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:490: UserWarning: Label encoding is no longer performed by default. Users will have to perform categorical features encoding by themselves.\n",
      "  warnings.warn(\"Label encoding is no longer performed by default. \"\n",
      "c:\\Users\\Matin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\missforest\\missforest.py:494: UserWarning: In version 4.2.3, estimator fitting process is moved to `fit` method. `MissForest` will now imputes unseen missing values with fitted estimators with `transform` method. To retain the old behaviour, use `fit_transform` to fit the whole unseen data instead.\n",
      "  warnings.warn(f\"In version {VERSION}, estimator fitting process \"\n",
      "100%|| 2/2 [00:00<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [41, 2]\n",
      "\n",
      "Computation graph constructed\n",
      "\n",
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 15.636018011305067\n",
      "Epoch: 1 , loss: 12.337935977511936\n",
      "Epoch: 2 , loss: 9.15252192815145\n",
      "Epoch: 3 , loss: 8.159546030892265\n",
      "Epoch: 4 , loss: 7.996194495095147\n",
      "Epoch: 5 , loss: 7.822096639209324\n",
      "Epoch: 6 , loss: 7.579149934980604\n",
      "Epoch: 7 , loss: 7.49990635448032\n",
      "Epoch: 8 , loss: 7.326392465167576\n",
      "Epoch: 9 , loss: 7.373434411154853\n",
      "Epoch: 10 , loss: 7.259839322831896\n",
      "Epoch: 11 , loss: 7.19220945570204\n",
      "Epoch: 12 , loss: 7.1859373516506615\n",
      "Epoch: 13 , loss: 7.138576904932658\n",
      "Epoch: 14 , loss: 7.124428616629706\n",
      "Epoch: 15 , loss: 7.08519098493788\n",
      "Epoch: 16 , loss: 7.072405444251166\n",
      "Epoch: 17 , loss: 7.102920293807983\n",
      "Epoch: 18 , loss: 7.054459280437893\n",
      "Epoch: 19 , loss: 6.948911666870117\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n",
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "impsss, table = run_full_pipeline3(new_df,timelimit=60,random_seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Data Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Best Method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Metric",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Error_SD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Max_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Min_Error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Within_10pct",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2ed369bd-ea4c-473a-a1d7-5e0aa4050012",
       "rows": [
        [
         "0",
         "Dm2",
         "Categorical",
         "KNN",
         "0.8888888888888888",
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "Dm4",
         "Discrete",
         "MIDAS_16",
         "9.852941176470589",
         "9.439222980801091",
         "39.0",
         "0.0",
         "0.38235294117647056"
        ],
        [
         "2",
         "E11",
         "Discrete",
         "KNN",
         "7.206896551724138",
         "6.073239370649074",
         "25.0",
         "0.0",
         "0.6206896551724138"
        ],
        [
         "3",
         "E12",
         "Discrete",
         "MissForest",
         "6.217391304347826",
         "4.25287626456956",
         "14.0",
         "1.0",
         "0.6521739130434783"
        ],
        [
         "4",
         "E21",
         "Discrete",
         "MissForest",
         "1.6857142857142857",
         "1.9368171065440538",
         "8.0",
         "0.0",
         "0.7142857142857143"
        ],
        [
         "5",
         "E22",
         "Discrete",
         "MissForest",
         "4.464285714285714",
         "11.532505245985247",
         "62.0",
         "0.0",
         "0.6071428571428571"
        ],
        [
         "6",
         "E31",
         "Discrete",
         "KNN",
         "13.821428571428571",
         "13.280600085249862",
         "54.0",
         "2.0",
         "0.6428571428571429"
        ],
        [
         "7",
         "E32",
         "Discrete",
         "KNN",
         "7.583333333333333",
         "6.565603007884565",
         "31.0",
         "0.0",
         "0.8333333333333334"
        ],
        [
         "8",
         "E41",
         "Discrete",
         "KNN",
         "7.23404255319149",
         "5.418217121965357",
         "25.0",
         "0.0",
         "0.5957446808510638"
        ],
        [
         "9",
         "E42",
         "Discrete",
         "KNN",
         "7.230769230769231",
         "4.942126605482238",
         "17.0",
         "1.0",
         "0.5769230769230769"
        ],
        [
         "10",
         "E23",
         "Continuous",
         "KNN",
         "9.176600985221675",
         "7.025410173574699",
         "23.871428571428567",
         "0.2857142857142776",
         "0.4827586206896552"
        ],
        [
         "11",
         "E24",
         "Discrete",
         "KNN",
         "6.553333333333333",
         "5.573813981364158",
         "25.0",
         "0.0",
         "0.9666666666666667"
        ],
        [
         "12",
         "E25",
         "Discrete",
         "KNN",
         "8.302631578947368",
         "5.733141530308091",
         "20.0",
         "0.7999999999999972",
         "0.5789473684210527"
        ],
        [
         "13",
         "E26",
         "Discrete",
         "MissForest",
         "6.705882352941177",
         "5.636888308625741",
         "19.0",
         "1.0",
         "0.7647058823529411"
        ],
        [
         "14",
         "E27",
         "Discrete",
         "KNN",
         "1.8",
         "1.4100479758212652",
         "5.0",
         "0.0",
         "0.6"
        ],
        [
         "15",
         "FastingBloodSugar",
         "Discrete",
         "MissForest",
         "6.36",
         "5.964338465692011",
         "23.0",
         "1.0",
         "0.76"
        ],
        [
         "16",
         "Glucose2hpp",
         "Discrete",
         "MIDAS_8",
         "27.45945945945946",
         "20.82812547520325",
         "77.0",
         "2.0",
         "0.2972972972972973"
        ],
        [
         "17",
         "Cholestrol",
         "Discrete",
         "MIDAS_8",
         "27.11111111111111",
         "30.580704491222683",
         "123.0",
         "0.0",
         "0.5555555555555556"
        ],
        [
         "18",
         "Triglycerides",
         "Discrete",
         "KNN",
         "43.84",
         "63.79764885949951",
         "328.0",
         "0.0",
         "0.2"
        ],
        [
         "19",
         "HDL",
         "Discrete",
         "MissForest",
         "8.04",
         "8.182705339108006",
         "27.0",
         "0.0",
         "0.56"
        ],
        [
         "20",
         "LDL",
         "Discrete",
         "MIDAS_5",
         "21.914285714285715",
         "17.73890942428607",
         "77.0",
         "1.0",
         "0.2857142857142857"
        ],
        [
         "21",
         "Hb.A1C",
         "Continuous",
         "MissForest",
         "0.5410295211396741",
         "0.44937477226663897",
         "1.5695444207859106",
         "0.02372595233921082",
         "0.5833333333333334"
        ],
        [
         "22",
         "CreatininUrine",
         "Continuous",
         "MIDAS_6",
         "78.14655686438084",
         "64.56760004263455",
         "218.16478564739228",
         "0.08202120661732692",
         "0.2413793103448276"
        ],
        [
         "23",
         "PotassiumUrineRandom",
         "Continuous",
         "KNN",
         "55.83368663594469",
         "75.64131496856305",
         "379.7857142857143",
         "0.7685714285714198",
         "0.16129032258064516"
        ],
        [
         "24",
         "SodiumUrineRandom",
         "Continuous",
         "KNN",
         "41.71611428571428",
         "34.785326084467314",
         "116.65714285714287",
         "0.6285714285713766",
         "0.44"
        ],
        [
         "25",
         "W.B.C",
         "Discrete",
         "MIDAS_10",
         "1048.5294117647059",
         "876.2768081852217",
         "2839.0",
         "16.0",
         "0.4411764705882353"
        ],
        [
         "26",
         "R.B.C",
         "Continuous",
         "KNN",
         "0.289563492063492",
         "0.2944855314189402",
         "1.2442857142857147",
         "0.009999999999999787",
         "0.8611111111111112"
        ],
        [
         "27",
         "Hemoglobin",
         "Continuous",
         "KNN",
         "0.8891625615763549",
         "0.7023767975799217",
         "2.7714285714285687",
         "0.0",
         "0.7586206896551724"
        ],
        [
         "28",
         "Hematocrit",
         "Continuous",
         "KNN",
         "2.1913419913419916",
         "1.83720041079151",
         "6.557142857142857",
         "0.014285714285712459",
         "0.7878787878787878"
        ],
        [
         "29",
         "MCV",
         "Continuous",
         "MissForest",
         "2.5987782307140694",
         "2.2137908420011345",
         "9.246115726394251",
         "0.14657152760601377",
         "1.0"
        ],
        [
         "30",
         "MCH",
         "Continuous",
         "MissForest",
         "0.9610622304295499",
         "0.893854116798568",
         "2.9398620176563455",
         "0.0018223093363545217",
         "1.0"
        ],
        [
         "31",
         "MCHC",
         "Continuous",
         "MissForest",
         "0.8910619713352643",
         "0.867333352631959",
         "3.776113896348079",
         "0.015887868407133965",
         "0.9393939393939394"
        ],
        [
         "32",
         "Neutrophils",
         "Continuous",
         "KNN",
         "4.284",
         "3.4526543077330354",
         "13.199999999999996",
         "0.028571428571424917",
         "0.64"
        ],
        [
         "33",
         "Lymphocyte",
         "Continuous",
         "KNN",
         "6.1677551020408155",
         "3.6602080492058224",
         "15.814285714285703",
         "1.028571428571432",
         "0.34285714285714286"
        ],
        [
         "34",
         "Mixed",
         "Continuous",
         "MIDAS_14",
         "2.44396025883524",
         "2.200637098886645",
         "6.976214325428009",
         "0.08630923032760407",
         "0.3157894736842105"
        ],
        [
         "35",
         "Platelets",
         "Discrete",
         "KNN",
         "42.82142857142857",
         "36.94345385949699",
         "137.0",
         "1.0",
         "0.39285714285714285"
        ],
        [
         "36",
         "DBP",
         "Discrete",
         "KNN",
         "6.321428571428571",
         "4.02817458362848",
         "17.5",
         "1.0",
         "0.7857142857142857"
        ],
        [
         "37",
         "SBP",
         "Discrete",
         "KNN",
         "9.066666666666666",
         "7.7712260308775285",
         "28.5",
         "0.5",
         "0.7333333333333333"
        ],
        [
         "38",
         "gdi",
         "Discrete",
         "MIDAS_12",
         "1.103448275862069",
         "0.9763206289141645",
         "3.0",
         "0.0",
         "0.3448275862068966"
        ],
        [
         "39",
         "work_activity",
         "Discrete",
         "MIDAS_6",
         "2089097.3846153845",
         "1332885.58355553",
         "4421785.0",
         "61855.0",
         "0.19230769230769232"
        ],
        [
         "40",
         "transport",
         "Discrete",
         "MissForest",
         "1063492.9189189188",
         "1126686.2427863001",
         "2477200.0",
         "462.0",
         "0.0"
        ],
        [
         "41",
         "lesiretime",
         "Discrete",
         "KNN",
         "1871575.6",
         "1524394.1547324902",
         "5122854.0",
         "0.0",
         "0.28"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 42
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Best Method</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Error_SD</th>\n",
       "      <th>Max_Error</th>\n",
       "      <th>Min_Error</th>\n",
       "      <th>Within_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dm2</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>KNN</td>\n",
       "      <td>8.888889e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dm4</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_16</td>\n",
       "      <td>9.852941e+00</td>\n",
       "      <td>9.439223e+00</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E11</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.206897e+00</td>\n",
       "      <td>6.073239e+00</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E12</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>6.217391e+00</td>\n",
       "      <td>4.252876e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E21</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.685714e+00</td>\n",
       "      <td>1.936817e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E22</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>4.464286e+00</td>\n",
       "      <td>1.153251e+01</td>\n",
       "      <td>6.200000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E31</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.382143e+01</td>\n",
       "      <td>1.328060e+01</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E32</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.583333e+00</td>\n",
       "      <td>6.565603e+00</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E41</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.234043e+00</td>\n",
       "      <td>5.418217e+00</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.595745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E42</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>7.230769e+00</td>\n",
       "      <td>4.942127e+00</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E23</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>9.176601e+00</td>\n",
       "      <td>7.025410e+00</td>\n",
       "      <td>2.387143e+01</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.482759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E24</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.553333e+00</td>\n",
       "      <td>5.573814e+00</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E25</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>8.302632e+00</td>\n",
       "      <td>5.733142e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E26</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>6.705882e+00</td>\n",
       "      <td>5.636888e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E27</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.800000e+00</td>\n",
       "      <td>1.410048e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FastingBloodSugar</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>6.360000e+00</td>\n",
       "      <td>5.964338e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Glucose2hpp</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_8</td>\n",
       "      <td>2.745946e+01</td>\n",
       "      <td>2.082813e+01</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.297297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cholestrol</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_8</td>\n",
       "      <td>2.711111e+01</td>\n",
       "      <td>3.058070e+01</td>\n",
       "      <td>1.230000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Triglycerides</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.384000e+01</td>\n",
       "      <td>6.379765e+01</td>\n",
       "      <td>3.280000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>8.040000e+00</td>\n",
       "      <td>8.182705e+00</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LDL</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_5</td>\n",
       "      <td>2.191429e+01</td>\n",
       "      <td>1.773891e+01</td>\n",
       "      <td>7.700000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hb.A1C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>5.410295e-01</td>\n",
       "      <td>4.493748e-01</td>\n",
       "      <td>1.569544e+00</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CreatininUrine</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MIDAS_6</td>\n",
       "      <td>7.814656e+01</td>\n",
       "      <td>6.456760e+01</td>\n",
       "      <td>2.181648e+02</td>\n",
       "      <td>0.082021</td>\n",
       "      <td>0.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PotassiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>5.583369e+01</td>\n",
       "      <td>7.564131e+01</td>\n",
       "      <td>3.797857e+02</td>\n",
       "      <td>0.768571</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SodiumUrineRandom</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.171611e+01</td>\n",
       "      <td>3.478533e+01</td>\n",
       "      <td>1.166571e+02</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>W.B.C</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_10</td>\n",
       "      <td>1.048529e+03</td>\n",
       "      <td>8.762768e+02</td>\n",
       "      <td>2.839000e+03</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>R.B.C</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>2.895635e-01</td>\n",
       "      <td>2.944855e-01</td>\n",
       "      <td>1.244286e+00</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hemoglobin</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>8.891626e-01</td>\n",
       "      <td>7.023768e-01</td>\n",
       "      <td>2.771429e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hematocrit</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>2.191342e+00</td>\n",
       "      <td>1.837200e+00</td>\n",
       "      <td>6.557143e+00</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MCV</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>2.598778e+00</td>\n",
       "      <td>2.213791e+00</td>\n",
       "      <td>9.246116e+00</td>\n",
       "      <td>0.146572</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MCH</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>9.610622e-01</td>\n",
       "      <td>8.938541e-01</td>\n",
       "      <td>2.939862e+00</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MCHC</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>8.910620e-01</td>\n",
       "      <td>8.673334e-01</td>\n",
       "      <td>3.776114e+00</td>\n",
       "      <td>0.015888</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Neutrophils</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.284000e+00</td>\n",
       "      <td>3.452654e+00</td>\n",
       "      <td>1.320000e+01</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lymphocyte</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.167755e+00</td>\n",
       "      <td>3.660208e+00</td>\n",
       "      <td>1.581429e+01</td>\n",
       "      <td>1.028571</td>\n",
       "      <td>0.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mixed</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>MIDAS_14</td>\n",
       "      <td>2.443960e+00</td>\n",
       "      <td>2.200637e+00</td>\n",
       "      <td>6.976214e+00</td>\n",
       "      <td>0.086309</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>4.282143e+01</td>\n",
       "      <td>3.694345e+01</td>\n",
       "      <td>1.370000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>DBP</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.321429e+00</td>\n",
       "      <td>4.028175e+00</td>\n",
       "      <td>1.750000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SBP</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>9.066667e+00</td>\n",
       "      <td>7.771226e+00</td>\n",
       "      <td>2.850000e+01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gdi</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_12</td>\n",
       "      <td>1.103448e+00</td>\n",
       "      <td>9.763206e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>work_activity</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MIDAS_6</td>\n",
       "      <td>2.089097e+06</td>\n",
       "      <td>1.332886e+06</td>\n",
       "      <td>4.421785e+06</td>\n",
       "      <td>61855.000000</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>transport</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>MissForest</td>\n",
       "      <td>1.063493e+06</td>\n",
       "      <td>1.126686e+06</td>\n",
       "      <td>2.477200e+06</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>lesiretime</td>\n",
       "      <td>Discrete</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.871576e+06</td>\n",
       "      <td>1.524394e+06</td>\n",
       "      <td>5.122854e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column    Data Type Best Method        Metric      Error_SD  \\\n",
       "0                    Dm2  Categorical         KNN  8.888889e-01           NaN   \n",
       "1                    Dm4     Discrete    MIDAS_16  9.852941e+00  9.439223e+00   \n",
       "2                    E11     Discrete         KNN  7.206897e+00  6.073239e+00   \n",
       "3                    E12     Discrete  MissForest  6.217391e+00  4.252876e+00   \n",
       "4                    E21     Discrete  MissForest  1.685714e+00  1.936817e+00   \n",
       "5                    E22     Discrete  MissForest  4.464286e+00  1.153251e+01   \n",
       "6                    E31     Discrete         KNN  1.382143e+01  1.328060e+01   \n",
       "7                    E32     Discrete         KNN  7.583333e+00  6.565603e+00   \n",
       "8                    E41     Discrete         KNN  7.234043e+00  5.418217e+00   \n",
       "9                    E42     Discrete         KNN  7.230769e+00  4.942127e+00   \n",
       "10                   E23   Continuous         KNN  9.176601e+00  7.025410e+00   \n",
       "11                   E24     Discrete         KNN  6.553333e+00  5.573814e+00   \n",
       "12                   E25     Discrete         KNN  8.302632e+00  5.733142e+00   \n",
       "13                   E26     Discrete  MissForest  6.705882e+00  5.636888e+00   \n",
       "14                   E27     Discrete         KNN  1.800000e+00  1.410048e+00   \n",
       "15     FastingBloodSugar     Discrete  MissForest  6.360000e+00  5.964338e+00   \n",
       "16           Glucose2hpp     Discrete     MIDAS_8  2.745946e+01  2.082813e+01   \n",
       "17            Cholestrol     Discrete     MIDAS_8  2.711111e+01  3.058070e+01   \n",
       "18         Triglycerides     Discrete         KNN  4.384000e+01  6.379765e+01   \n",
       "19                   HDL     Discrete  MissForest  8.040000e+00  8.182705e+00   \n",
       "20                   LDL     Discrete     MIDAS_5  2.191429e+01  1.773891e+01   \n",
       "21                Hb.A1C   Continuous  MissForest  5.410295e-01  4.493748e-01   \n",
       "22        CreatininUrine   Continuous     MIDAS_6  7.814656e+01  6.456760e+01   \n",
       "23  PotassiumUrineRandom   Continuous         KNN  5.583369e+01  7.564131e+01   \n",
       "24     SodiumUrineRandom   Continuous         KNN  4.171611e+01  3.478533e+01   \n",
       "25                 W.B.C     Discrete    MIDAS_10  1.048529e+03  8.762768e+02   \n",
       "26                 R.B.C   Continuous         KNN  2.895635e-01  2.944855e-01   \n",
       "27            Hemoglobin   Continuous         KNN  8.891626e-01  7.023768e-01   \n",
       "28            Hematocrit   Continuous         KNN  2.191342e+00  1.837200e+00   \n",
       "29                   MCV   Continuous  MissForest  2.598778e+00  2.213791e+00   \n",
       "30                   MCH   Continuous  MissForest  9.610622e-01  8.938541e-01   \n",
       "31                  MCHC   Continuous  MissForest  8.910620e-01  8.673334e-01   \n",
       "32           Neutrophils   Continuous         KNN  4.284000e+00  3.452654e+00   \n",
       "33            Lymphocyte   Continuous         KNN  6.167755e+00  3.660208e+00   \n",
       "34                 Mixed   Continuous    MIDAS_14  2.443960e+00  2.200637e+00   \n",
       "35             Platelets     Discrete         KNN  4.282143e+01  3.694345e+01   \n",
       "36                   DBP     Discrete         KNN  6.321429e+00  4.028175e+00   \n",
       "37                   SBP     Discrete         KNN  9.066667e+00  7.771226e+00   \n",
       "38                   gdi     Discrete    MIDAS_12  1.103448e+00  9.763206e-01   \n",
       "39         work_activity     Discrete     MIDAS_6  2.089097e+06  1.332886e+06   \n",
       "40             transport     Discrete  MissForest  1.063493e+06  1.126686e+06   \n",
       "41            lesiretime     Discrete         KNN  1.871576e+06  1.524394e+06   \n",
       "\n",
       "       Max_Error     Min_Error  Within_10pct  \n",
       "0            NaN           NaN           NaN  \n",
       "1   3.900000e+01      0.000000      0.382353  \n",
       "2   2.500000e+01      0.000000      0.620690  \n",
       "3   1.400000e+01      1.000000      0.652174  \n",
       "4   8.000000e+00      0.000000      0.714286  \n",
       "5   6.200000e+01      0.000000      0.607143  \n",
       "6   5.400000e+01      2.000000      0.642857  \n",
       "7   3.100000e+01      0.000000      0.833333  \n",
       "8   2.500000e+01      0.000000      0.595745  \n",
       "9   1.700000e+01      1.000000      0.576923  \n",
       "10  2.387143e+01      0.285714      0.482759  \n",
       "11  2.500000e+01      0.000000      0.966667  \n",
       "12  2.000000e+01      0.800000      0.578947  \n",
       "13  1.900000e+01      1.000000      0.764706  \n",
       "14  5.000000e+00      0.000000      0.600000  \n",
       "15  2.300000e+01      1.000000      0.760000  \n",
       "16  7.700000e+01      2.000000      0.297297  \n",
       "17  1.230000e+02      0.000000      0.555556  \n",
       "18  3.280000e+02      0.000000      0.200000  \n",
       "19  2.700000e+01      0.000000      0.560000  \n",
       "20  7.700000e+01      1.000000      0.285714  \n",
       "21  1.569544e+00      0.023726      0.583333  \n",
       "22  2.181648e+02      0.082021      0.241379  \n",
       "23  3.797857e+02      0.768571      0.161290  \n",
       "24  1.166571e+02      0.628571      0.440000  \n",
       "25  2.839000e+03     16.000000      0.441176  \n",
       "26  1.244286e+00      0.010000      0.861111  \n",
       "27  2.771429e+00      0.000000      0.758621  \n",
       "28  6.557143e+00      0.014286      0.787879  \n",
       "29  9.246116e+00      0.146572      1.000000  \n",
       "30  2.939862e+00      0.001822      1.000000  \n",
       "31  3.776114e+00      0.015888      0.939394  \n",
       "32  1.320000e+01      0.028571      0.640000  \n",
       "33  1.581429e+01      1.028571      0.342857  \n",
       "34  6.976214e+00      0.086309      0.315789  \n",
       "35  1.370000e+02      1.000000      0.392857  \n",
       "36  1.750000e+01      1.000000      0.785714  \n",
       "37  2.850000e+01      0.500000      0.733333  \n",
       "38  3.000000e+00      0.000000      0.344828  \n",
       "39  4.421785e+06  61855.000000      0.192308  \n",
       "40  2.477200e+06    462.000000      0.000000  \n",
       "41  5.122854e+06      0.000000      0.280000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
